We extended the concept of Kronecker-factored approximate curvature (KFAC) to Gauss-Newton matrices of PINN losses that involve derivatives, rather than evaluations, of a neural network.
It greatly reduces the computational cost of approximate natural gradient methods and allows them to scale to much larger networks.
Our approach goes beyond the popular KFAC for traditional deep learning problems as it captures contributions from a PDE's differential operator that are crucial for optimization.
To establish KFAC for such losses, we use Taylor-mode automatic differentiation to describe the differential operator's computation graph as a forward network with shared weights which allows us to apply a recently-developed variant of KFAC for networks with weight sharing.
Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems, scale more favorably to higher-dimensional neural networks and PDEs, and consistently outperform first-order methods.

\textbf{Limitations \& future directions} While our implementation currently only supports MLPs and the Poisson and heat equations, the concepts we use to derive KFAC (Taylor-mode, weight sharing) apply to arbitrary architectures and PDEs, as described in~\Cref{sec:KFAC-general}.
Further, we are excited that our current algorithms show promising performance when compared to second-order methods with sophisticated heuristics.
In fact, the original KFAC optimizer itself~\cite{martens2015optimizing} relies heavily on such heuristics that have been hypothesized to be crucial for its performance~\cite{clarke2023adam}.
Our algorithms borrow components, but we did not explore all bells and whistles, e.g.\,adaptive damping and heuristics to distribute damping over the Kronecker factors.
We believe our current algorithm's performance can further be improved, e.g.\,by 1) updating the KFAC matrices less frequently, as is standard for traditional KFAC, 2) merging the two Kronecker approximations for boundary and interior Gramian into a single one, 3) removing matrix inversions~\cite{lin2023simplifying}, 4) using structured Kronecker factors~\cite{lin2023structured}, 5) computing the Kronecker factors in parallel with the gradient~\cite{dangel2020backpack}, 6) using single precision, 7) studying cheaper KFAC flavours based on the empirical Fisher~\cite{kunstner2019limitations} or input-based curvature~\cite{benzing2022gradient,petersen2023isaac}.