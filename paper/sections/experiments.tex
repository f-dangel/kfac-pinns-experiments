\begin{figure}[t]
  \centering
  \includegraphics{kfac_pinns_exp/exp17_groupplot_poisson2d/l2_error_over_time.pdf}
  \caption{Performance of different optimizers on the 2d Poisson equation~\eqref{eq:2D-Poisson} measured in relative $L_2$ error against wall clock time for architectures with different parameter dimensions $D$.}
  \label{fig:2D-Poisson}
\end{figure}

We implement KFAC, KFAC*, and ENGD with either the per-layer or full Gramian in PyTorch~\citep{paszke2019pytorch}.
As a matrix-free version of ENGD, we use the Hessian-free optimizer~\citep{martens2010deep} which uses truncated conjugate gradients (CG) with exact Gramian-vector products to precondition the gradient.
We chose this because there is a fully-featured implementation from~\citet{tatzel2022late} which offers many additional heuristics like adaptive damping, CG backtracking, and backtracking line search, allowing this algorithm to work well with little hyper-parameter tuning.
As baselines, we use SGD with tuned learning rate and momentum, Adam with tuned learning rate, and LBFGS with tuned learning rate and history size.
We tune hyper-parameters using Weights \& Biases~\citep{wandb} (see \Cref{sec:tuning-protocol} for the exact protocol).
For random/grid search, we run an initial round of approximately 50 runs with generous search spaces, then narrow them down and re-run for another 50 runs; for Bayesian search, we assign the same total compute to each optimizer.
We report runs with lowest $L_2$ error estimated on a held-out data set with the known solution to the studied PDE.
To be comparable, all runs are executed on a compute cluster with RTX 6000 GPUs (24\,GiB RAM) in double precision, and we use the same computation time budget for all optimizers on a fixed PINN problem.
All search spaces and best run hyper-parameters, as well as training curves over iteration count rather than time, are in \Cref{sec:experimental_details}.

\begin{figure}
  \centering
  \includegraphics{kfac_pinns_exp/exp30_heat4d_groupplot/l2_error_over_time.pdf}
  \caption{Performance of different optimizers on the (4+1)d heat equation~\eqref{eq:4D-heat} measured in relative $L^2$ error against wall clock time for architectures with different parameter dimensions $D$.}
  \label{fig:4D-heat}
\end{figure}

\paragraph{Pedagogical example: 2d Poisson equation}
We start with a low-dimensional Poisson equation from~\citet{muller2023achieving} to reproduce ENGD's performance (\Cref{fig:2D-Poisson}).
It is given by
\begin{align}\label{eq:2D-Poisson}
  \begin{split}
    -\Delta u(x,y)
    &=
      2\pi^2 \sin(\pi x) \sin(\pi y) \quad \text{for } (x,y)\in[0,1]^2\,
    \\
    u(x,y)
    &=
      0 \quad \text{for } (x,y) \in\partial[0,1]^2.
  \end{split}
\end{align}
We choose a fixed data set of same size as the original paper, then use random/grid search to evaluate the performance of all optimizers for different $\tanh$-activated MLPs, one shallow and two with five fully-connected layers of different width (all details in \Cref{sec:2d-poisson-appendix}).
We include ENGD whenever the network's parameter space is small enough to build up the Gramian.

For the shallow net (\Cref{fig:2D-Poisson}, left), we can reproduce the results of~\cite{muller2023achieving}, where exact ENGD achieves high accuracy.
In terms of computation time, our KFACs are competitive with full-ENGD for a long phase, outperforming the first-order and quasi-Newton baselines.
In contrast to ENGD, which runs out of memory for networks with more than $\num{10000}$ parameters, KFAC scales to larger networks (\Cref{fig:2D-Poisson}, center and right) and is competitive with other second-order optimizers like Hessian-free, which uses more sophisticated heuristics.
We make similar observations on a small (1+1)d heat equation with the same models, see \Cref{sec:1d-heat-equation,fig:heat1d-appendix}.

\paragraph{An evolutionary problem: (4+1)d heat equation}
To demonstrate that our methods can also be applied to other problems than the Poisson equation, we consider a four-dimensional heat equation
\begin{align}\label{eq:4D-heat}
  \begin{split}
    \partial_t u(t,\vx)-\kappa\Delta_\vx u(t,\vx)
    &=
      0 \quad \text{for } t\in[0,1], \vx\in [0,1]^{4}\,,
    \\
    u(0,\vx)
    &=
      \sum_{i=1}^{4} \sin(2 x_i) \quad \text{for }
      \vx\in [0,1]^{4}\,,
    \\
    u(t,\vx)
    &=
      \exp(-t) \sum_{i=1}^{4} \sin(2 x_i) \quad \text{for } t\in[0,1], \vx\in\partial[0,1]^{4}\,,
  \end{split}
\end{align}
with diffusivity constant $\kappa = \nicefrac{1}{4}$, similar to that studied in \cite{muller2023achieving} (see \Cref{sec:pinn-loss-heat-equation} for the heat equation's PINN loss).
We use the previous architectures with same hidden widths and evaluate optimizer performance with random/grid search (all details in \Cref{sec:4d-heat-app}), see \Cref{fig:4D-heat}.
To prevent over-fitting, we use mini-batches and sample a new batch each iteration.
We noticed that KFAC improves significantly when batches are sampled less frequently and hypothesize that it might need more iterations to make similar progress than one iteration of Hessian-free or ENGD on a batch.
Consequently we sample a new batch only every 100 iterations for KFAC.
To ensure that this does not lead to an unfair advantage for KFAC, we conduct an additional experiment for the MLP with $D=\num{116864}$ where we tune batch sizes, batch sampling frequencies, and all hyper-parameters with generous search spaces using Bayesian search (\Cref{sec:4d-heat-bayes-app}).
We find that this does not significantly boost performance of the other methods (compare \Cref{fig:4D-heat,fig:heat4d-bayes-appendix}).
Again, we observe that KFAC offers competitive performance compared to other second-order methods for networks with prohibitive size for ENGD and consistently outperforms SGD, Adam, and LBFGS.
We confirmed these observations with another 5d Poisson equation on the same architectures, see~\Cref{sec:poisson5d-appendix,fig:poisson5d-appendix}.

\begin{figure}
  \centering
  \includegraphics{kfac_pinns_exp/exp33_poisson_bayes_groupplot/l2_error_over_time.pdf}
  \caption{
    Optimizer performance on Poisson equations in high dimensions and different boundary conditions measured in relative $L_2$ error against wall clock time for networks with $D$ parameters.
  }
  \label{fig:10D-Poisson}
\end{figure}

\paragraph{High-dimensional Poisson equations}
To demonstrate scaling to high-dimensional PDEs and even larger neural networks, we consider three Poisson equations ($d=5,10,100$) with different boundary conditions used in~\cite{yu2018deep, muller2023achieving}, which admit the solutions
\begin{align}\label{eq:solutions_poisson}
  \begin{split}
    u_\star(\vx)
    &=
      \sum_{i=1}^5 \cos(\pi x_i) \quad \text{for } \vx\in [0,1]^{5}\,,
    \\
    u_\star(\vx)
    &=
      \sum_{k=1}^5 x_{2k-1}x_{2k}  \quad \text{for } \vx\in [0,1]^{10}\,,
    \\
    u_\star(\vx)
    &=
      \lVert \vx \rVert_2^2 \quad \text{for } \vx\in [0,1]^{100}\,.
  \end{split}
\end{align}
We use the same architectures as before, but with larger intermediate widths and parameters up to a million (\Cref{fig:10D-Poisson}).
Due to lacking references for training such high-dimensional problems, we select all hyper-parameters via Bayesian search, including batch sizes and batch sampling frequencies (details in \Cref{sec:high-dimensional-poissons-app}).
We see a similar picture as before with KFAC consistently outperforming first-order methods and LBFGS, offering competitive performance with Hessian-free.
To account for the possibility that the Bayesian search did not properly identify good hyper-parameters, we conduct a random/grid search experiment for the 10d Poisson equation (\Cref{fig:10D-Poisson}, middle), using similar batch sizes and same batch sampling frequencies as for the $(4+1)$d heat equation (details in \Cref{sec:poisson10d-appendix}).
In this experiment, KFAC also achieved similar performance than Hessian-free and outperformed SGD, Adam, and LBFGS (\Cref{fig:poisson_10d-appendix}).

\paragraph{(9+1)d Fokker-Planck equation} To show the applicability to nonlinear PDEs, we consider a Fokker-Planck equation in logarithmic space. PINN formulations of the
Fokker-Planck equation have been considered in \cite{hu2024score, sun2024dynamical}. Concretely, we are solving a nine-dimensional equation
of the form
\begin{align}
  \partial_t q(t, \vx)
  -
  \frac{d}{2}
  -
  \frac12 \nabla q(t,\vx)\cdot \vx
  -
  \lVert \nabla q(t,\vx) \rVert^2
  -
  \Delta q(t, \vx)
  =
  0,
  \quad
  q(0)
  =
  \log(p^*(0)),
\end{align}
\begin{wrapfigure}[16]{r}{0.5\textwidth}
  \centering
  \vspace{-2ex}
  \includegraphics{kfac_pinns_exp/exp43_log_fokker_planck9d_isotropic_gaussian_random/l2_error_over_time.pdf}
  \caption{
    Performance of different optimizers on a (9+1)d logarithmic Fokker-Planck equation in relative $L_2$ error against wall clock time.
  }
  \label{fig:10D-logFP}
\end{wrapfigure}
with $d=9$, $t\in[0,1]$ and $\vx \in \mathbb R^9$, where in practice $\mathbb R^9$ is replaced by $[-5,5]^9$.
The solution is $q^* = \log(p^*)$ and $p^*$ is given by $p^*(t,\vx)\sim \mathcal{N}(0, \exp(-t)\mI + (1-\exp(-t))2\mI)$.
We model the solution with a medium sized tanh-activated MLP with $D=\num{118145}$ parameters,
batch sizes are $N_{\Omega} = \num{3000}$, $N_{\partial\Omega} = \num{1000}$, and we assign each run a computation time budget of $\num{6000}\,\text{s}$.
As in previous experiments, the batches are re-sampled every iteration for all optimizers except for KFAC and KFAC*, which use the same batch for ten steps (details in \Cref{sec:fokker10d-appendix}).
\Cref{fig:10D-logFP} reports the $L^2$ error over training time.
Again, KFAC is among the best performing optimizers offering competitive performance to Hessian-free and clearly outperforming all first-order methods.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
