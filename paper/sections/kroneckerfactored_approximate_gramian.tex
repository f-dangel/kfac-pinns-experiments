This subsection describes the Kronecker approximation we propose for the per-layer Gramian.
It should also justify that the approximation is similar to that in other works.

\toodoo{F.D.
  Warning: The below write-up is a first attempt to come up with a Kronecker approximation.
  I still need to find more intuitive explanations, ideally with ideas from other KFAC papers.}

We will rely on a similar intuition than \citet{eschenhagen2023kroneckerfactored} in the presence of weight sharing.
However, we will use a slightly different motivation to obtain the Kronecker structure in the presence of weight sharing.

\paragraph{Some intuition:} Consider a weight matrix $\mW \in \sR^{D_1 \times
  D_2}$ inside a neural network that produces a loss $\ell$. We want to compute
the Gramian $\mF(\mW) = \grad{\mW}\ell (\grad{\mW}\ell)^{\top}$. Consider the following scenarios:
\begin{enumerate}
\item \textbf{(No sharing, no transpose)} The matrix is used to form the matrix vector product $\vz = \mW \vx \in \sR^{D_2}$ with an input vector $\vx \in \sR^{D_1}$.
  The Gramian is then
  \begin{align*}
    \mF(\mW) = \vx \vx^{\top} \otimes \grad{\vz}\ell (\grad{\vz}\ell)^{\top}\,.
  \end{align*}
  The input to the matrix multiply forms the first Kronecker factor, the gradient w.r.t.\,the matrix multiply's output forms the second Kronecker factor.

\item \textbf{(No sharing, transpose)} The matrix is used to form the transpose matrix vector product $\vz = \mW^{\top} \vx \in \sR^{D_1}$.
  The Gramian is then
  \begin{align*}
    \mF(\mW) = \grad{\vz}\ell (\grad{\vz}\ell)^{\top} \otimes \vx \vx^{\top}\,.
  \end{align*}
  The gradient w.r.t.\,the transpose matrix multiply's output forms the first Kronecker factor, the input to the matrix multiply forms the second Kronecker factor.
  I.e.
  the Gramian w.r.t.\,the transpose matrix is just the Gramian of the non-transposed matrix, but with swapped Kronecker factors.

\item \textbf{(Sharing, no transpose)} The matrix is used to form the matrix matrix product $\mZ = \mW \mX \in \sR^{D_2 \times S}$ with an input matrix $\mX \in \sR^{D_1 \times S}$ consisting of $S$ vector-valued columns which share the same weights.
  The Gramian is then
  \begin{align*}
    \mF(\mW) =
    \left(
    \mX
    \otimes
    \mI_{D_2}
    \right)
    \grad{\mZ}\ell (\grad{\mZ}\ell)^{\top}
    \left(
    \mX^{\top}
    \otimes
    \mI_{D_2}
    \right)
  \end{align*}
  Note that this does not simplify into a single Kronecker product.
  We need to apply another approximation.
  One way to obtain a single Kronecker factor is to seek an approximation for the central term such that $\grad{\mZ}\ell (\grad{\mZ}\ell)^{\top} \approx \mI \otimes \mU$ with $\mU \in \sR^{D_1 \times D_1}$.
  Let's look for the matrix that minimizes the reconstruction error
  $\left\lVert \grad{\mZ}\ell (\grad{\mZ}\ell)^{\top} - \mI \otimes \mU
  \right\rVert_{\text{F}}^2$. The solution to this is given by the average
  diagonal blocks of $\grad{\mZ}\ell (\grad{\mZ}\ell)^{\top}$. We can write it
  as
  \begin{align}
    \mU
    =
    \frac{1}{S}
    \flatten^{-1}
    \left(
    \grad{\mZ}\ell
    \right)
    \left[
    \flatten^{-1}
    \left(
    \grad{\mZ}\ell
    \right)
    \right]^{\top}
  \end{align}
  where $\flatten^{-1} \left(\grad{\mZ}\ell \right) \in \sR^{D_1 \times S}$.
  With this approximation, we can express the Gramian as a single Kronecker product:
  \begin{align*}
    \begin{split}
      \mF(\mW)
      &\approx
        \left(
        \mX
        \otimes
        \mI_{D_2}
        \right)
        \left(
        \mI
        \otimes
        \mU
        \right)
        \left(
        \mX^{\top}
        \otimes
        \mI_{D_2}
        \right)
      \\
      &=
        \mX \mX^{\top}
        \otimes
        \mU
      \\
      &=
        \mX \mX^{\top}
        \otimes
        \frac{1}{S}
        \flatten^{-1}
        \left(
        \grad{\mZ}\ell
        \right)
        \left[
        \flatten^{-1}
        \left(
        \grad{\mZ}\ell
        \right)
        \right]^{\top}
    \end{split}
  \end{align*}
  Again, note that the input to the matrix multiply forms the first Kronecker factor, the gradient w.r.t.\,the matrix multiply's output forms the second Kronecker factor.
  However, we needed additional approximations to obtain a single Kronecker product.

\item \textbf{(Sharing, transpose)} The matrix is used to form the transpose matrix matrix product $\mZ = \mW^{\top} \mX \in \sR^{D_1 \times S}$ with $\mX \in \sR^{D_2 \times S}$.
  The Gramian is then
  \begin{align*}
    \begin{split}
      \mF(\mW)
      &=
        \left(
        \mI_{D_1}
        \otimes
        \mX
        \right)
        \mK^{\top}
        \grad{\mZ}\ell (\grad{\mZ}\ell)^{\top}
        \mK
        \left(
        \mI_{D_1}
        \otimes
        \mX^{\top}
        \right)\,.
    \end{split}
  \end{align*}
  Again, this does not simplify into a single Kronecker product.
  So we are forced to make another approximation.
  Let's first take a closer look at $\mK^{\top} \grad{\mZ}\ell$.
  The application of $\mK^{\top}$ simply changes the flattening scheme of the vector.
  With the definition $\grad{\mZ^{\top}}\ell := \grad{\flatten(\mZ^{\top})} \ell$, we have that $\mK^{\top} \grad{\mZ}\ell = \grad{\mZ^{\top}}\ell$.
  Just like in the (sharing, no transpose) case from above, we will now look for an approximation of $\grad{\mZ^{\top}}\ell (\grad{\mZ^{\top}}\ell)^{\top} \approx \mU \otimes \mI$ with $\mU \in \sR^{D_1\times D_1}$.
  Again, we choose $\mU$ to minimize the reconstruction $\left\lVert \grad{\mZ^{\top}}\ell (\grad{\mZ^{\top}}\ell)^{\top} - \mU \otimes \mI \right\rVert_{\text{F}}^2$.
  We can transform this by applying $\mK$ from the left and from the right before evaluating the squared Frobenius norm (this only rearranges the elements and the Frobenius norm does not depend on the order).
  So we have $\left\lVert \mK \left( \grad{\mZ^{\top}}\ell (\grad{\mZ^{\top}}\ell)^{\top} - \mU \otimes \mI \right) \mK \right\rVert_{\text{F}}^2 = \left\lVert \grad{\mZ}\ell (\grad{\mZ}\ell)^{\top} - \mI \otimes \mU \right\rVert_{\text{F}}^2 $, which is the same objective from the (sharing, no transpose) case.
  Hence, its solution is $\mU = \nicefrac{1}{S} \flatten^{-1}
  \left(\grad{\mZ}\ell \right) \left[\flatten^{-1} \left(\grad{\mZ}\ell \right)
  \right]^{\top}$. With that, the single Kronecker product approximation of the
  Fisher is
  \begin{align*}
    \begin{split}
      \mF^{(i)}
      &\approx
        \left(
        \mI_{D_1}
        \otimes
        \mX
        \right)
        \left(
        \mU
        \otimes
        \mI_{S}
        \right)
        \left(
        \mI_{D_1}
        \otimes
        \mX^{\top}
        \right)
      \\
      &=
        \mU
        \otimes
        \mX \mX^{\top}
      \\
      &=
        \frac{1}{S}
        \flatten^{-1}
        \left(
        \grad{\mZ}\ell
        \right)
        \left[
        \flatten^{-1}
        \left(
        \grad{\mZ}\ell
        \right)
        \right]^{\top}
        \otimes
        \mX \mX^{\top}
    \end{split}
  \end{align*}
  The input to the transpose matrix multiply forms the first Kronecker factor, the gradient w.r.t.\,the transpose matrix multiply's output forms the second Kronecker factor.

\end{enumerate}

\paragraph{Single datum case} As a first step, let's write down only the diagonal terms of the Gramian, i.e.\,the terms caused by the same children and try to simplify each term into a Kronecker product of same dimension:
\begin{align}
  \begin{split}
    \mF^{(i)}
    &\approx
      \underbrace{
      \left(
      {\vz^{(i-1)}}^\top\otimes \mI
      \right)^{\top}
      \grad{\vz^{(i)}}\Delta u
      \left(
      \left(
      {\vz^{(i-1)}}^\top\otimes \mI
      \right)^{\top}
      \grad{\vz^{(i)}}\Delta u
      \right)^{\top}
      }_{(1, 1)}
    \\
    &\phantom{=}+
      \underbrace{
      \left(
      \mI \otimes \grad{\vz^{(i)}}u
      \right)^{\top}
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \left(
      \left(
      \mI \otimes \grad{\vz^{(i)}}u
      \right)^{\top}
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
      }_{(2, 2)}
    \\
    &\phantom{=}+
      \underbrace{
      2
      \left(
      \mI \otimes
      \left[
      \left( \gradsquared{\vz^{(i)}}u \right) \mW^{(i)}
      \right]
      \right)
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \left(
      2
      \left(
      \mI \otimes
      \left[
      \left( \gradsquared{\vz^{(i)}}u \right) \mW^{(i)}
      \right]
      \right)
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
      }_{(3, 3)}
      \,.
      \shortintertext{Without approximations, we can re-write this as}
    &=
      \vz^{(i-1)} {\vz^{(i-1)}}^\top
      \otimes
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      \left(
      \grad{\vz^{(i)}}u
      \right)
      \left(
      \grad{\vz^{(i)}}u
      \right)^{\top}
      \otimes
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      4
      \left(
      \mI \otimes
      \left[
      \left( \gradsquared{\vz^{(i)}}u \right) \mW^{(i)}
      \right]
      \right)
      \left[
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
      \right]
      \left(
      \mI \otimes
      \left[
      {\mW^{(i)}}^{\top}
      \left( \gradsquared{\vz^{(i)}}u \right)
      \right]
      \right)
      \,.
      \intertext{The leading two terms have the same Kronecker structure, $h^{(i-1)} \times h^{(i-1)} \otimes h^{(i)} \times h^{(i)}$.
      The third term does not simplify further, because the non-identity term in the Jacobians (outer terms) is a matrix, not a vector.
      One way to obtain the same Kronecker structure is to approximate the central term $ \left(\grad{\gradsquared{\vz^{(i-1)}}u}\Delta u \right) \left(\grad{\gradsquared{\vz^{(i-1)}}u}\Delta u \right)^{\top} \approx \mU \otimes \mI$ where $\mU \in \sR^{h^{(i-1)}\times h^{(i-1)}}$ (remember that $\grad{\gradsquared{\vz^{(i-1)}}}u \in \sR^{{h^{(i-1)}}^2}$.
      In the following, let $g := \grad{\gradsquared{\vz^{(i-1)}}}u$ for brevity.
      The `optimal' $\mU$ that minimizes the Frobenius norm $\left\lVert\mU \otimes \mI - \vg \vg^{\top} \right\rVert_{\text{F}}^2$ is the averaged block diagonal $\mU = \nicefrac{1}{h^{(i-1)}} \mG \mG^{\top}$ where $\mG = \flatten^{-1}(\vg) \in \sR^{h^{(i-1)} \times h^{(i-1)}}$.
      With this additional approximation, we can also express the third term as a Kronecker product:}
    &\approx
      \vz^{(i-1)} {\vz^{(i-1)}}^\top
      \otimes
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      \left(
      \grad{\vz^{(i)}}u
      \right)
      \left(
      \grad{\vz^{(i)}}u
      \right)^{\top}
      \otimes
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      \frac{4}{h^{(i-1)}}
      \left(
      \flatten^{-1}
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)
      \right)
      \left(
      \flatten^{-1}
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)
      \right)^{\top}
      \otimes
      \left( \gradsquared{\vz^{(i)}}u \right)
      \mW^{(i)}
      {\mW^{(i)}}^{\top}
      \left( \gradsquared{\vz^{(i)}}u \right)
      \,.
      \shortintertext{Let's make the final approximation from three Kronecker products into a single Kronecker product.
      In KFAC-style, we use a Kronecker-of-sums to approximate a sum-of-Kroneckers:}
    &\approx
      \mA^{(i)} \otimes \mB^{(i)}
  \end{split}
\end{align}
with
\begin{subequations}
  \label{eq:gram-kronecker-approximations-unbatched}
  \begin{align}
    \begin{split}
      \mA^{(i)}
      &=
        \vz^{(i-1)} {\vz^{(i-1)}}^\top
        +
        \left(
        \grad{\vz^{(i)}}u
        \right)
        \left(
        \grad{\vz^{(i)}}u
        \right)^{\top}
      \\
      &\phantom{=}+
        \frac{4}{h^{(i-1)}}
        \left(
        \flatten^{-1}
        \left(
        \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
        \right)
        \right)
        \left(
        \flatten^{-1}
        \left(
        \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
        \right)
        \right)^{\top}\,,
    \end{split}
    \\
    \begin{split}
      \mB^{(i)}
      &=
        \left(
        \grad{\vz^{(i)}}\Delta u
        \right)
        \left(
        \grad{\vz^{(i)}}\Delta u
        \right)^{\top}
        +
        \left(
        \grad{\grad{\vz^{(i-1)}}u}\Delta u
        \right)
        \left(
        \grad{\grad{\vz^{(i-1)}}u}\Delta u
        \right)^{\top}
      \\
      &\phantom{=}+
        \left( \gradsquared{\vz^{(i)}}u \right)
        \mW^{(i)}
        {\mW^{(i)}}^{\top}
        \left( \gradsquared{\vz^{(i)}}u \right)\,.
    \end{split}
  \end{align}
\end{subequations}

\paragraph{With batching} In the presence of multiple data points, we need to approximate the Gramian $\mF^{(i)} \approx \nicefrac{1}{N} \sum_{n=1}^N \mA_n^{(i)} \otimes \mB_n^{(i)}$ further (the subscripts $_n$ denote the computation on datum $n$).
We can just do that in the same way as the original KFAC paper, which gives us
\begin{align}\label{eq:gram-kronecker-approximations-batched}
  \mF^{(i)}
  \approx
  \left(
  \frac{1}{N}\sum_{n=1}^N \mA_n^{(i)}
  \right)
  \otimes
  \left(
  \sum_{n=1}^N \mB_n^{(i)}
  \right)
\end{align}
\Cref{eq:gram-kronecker-approximations-unbatched,eq:gram-kronecker-approximations-batched} are our proposed approximation for the Gramian.

\toodoo{--- F.D.
  Below is an alternative write-up which starts from the perspective of weight sharing.
  I haven't quite figured out the exact connection yet.}

We will Kronecker-approximate the Gramian using the recently introduced generalization of KFAC to weight-sharing layers in~\citet{eschenhagen2023kroneckerfactored}.

\paragraph{One datum, no weight sharing} Let's start with maximum likelihood estimation with a single data point $(\vx, \vy)$.
Consider a linear layer inside a neural network which maps some vector-valued hidden feature of $\vx$, $\va \in \sR^{D_{\text{in}}}$ to a vector-valued output $\vz \in \sR^{D_{\text{out}}}$ via $\vz = \mW \va$.
$\vz$ is then further processed and used to compute the negative log-likelihood loss $\ell(\vx, \vy, \mW) = - \log p(\vy \mid \vx, \mW)$.
For this single-usage layer, the weigh matrix's Fisher is exactly Kroneckerfactored, $\mF(\mW) = \va \va^{\top} \otimes \E_{\hat{\vy} \sim p(\dot \mid \vx, \mW)}\left[ \vg \vg^{\top} \right]$ where $\vg = \grad{\vz} \ell(\vx, \hat{\vy}, \mW)$.
In practise, we will use one sample from the model's likelihood to estimate the second expectation.
This yields $\mF(\mW) \approx \va \va^{\top} \otimes \vg \vg^{\top}$

\paragraph{One datum, weight sharing} Now consider a layer which is used
multiple times in the computation graph. This means the layer will not process a
single vector $\va$, but a sequence of vectors $\left\{ \va_1, \dots, \va_S
\right\}$ where $S$ denotes weight sharing number. We can column-stack these
vectors into a matrix $\mA \in \sR^{D_{\text{in}}\times S}$, likewise for the
linear layer's outputs $\mZ \in \sR^{D_{\text{out}}\times S}$ and activation
gradients $\mG \in \sR^{D_{\text{out}} \times S}$.

As described in \citet{eschenhagen2023kroneckerfactored}, there are two possible Kronecker approximations for this setup.

The first one is the \emph{expand} approximation, which takes the covariance over the shared vectors, $\mF(\mW) \approx \Cov_s[\va] \otimes S \Cov_s[\vg] = \nicefrac{1}{S} \sum_{s=1}^S \va_s \va_s^{\top} \otimes \sum_{s=1}^S \vg_s \vg_s^{\top} \eqqcolon \mF^{(\text{expand})}(\mW)$ where $\vg_s = \grad{\vz_s} \ell(\vx, \hat{\vy}, \mW)$.
We can express this in matrix notation as $\mF^{(\text{expand})}(\mW) = \nicefrac{1}{S} \mA \mA^{\top} \otimes \mG \mG^{\top}$.

A more aggressive, though cheaper, approximation is the \emph{reduce} approximation, which uses the mean outer product, $\mF(\mW) \approx \E_s[\va] \E_s[\va]^{\top} \otimes S \E_s[\vg] S \E_s[\vg]^{\top} = \left(\nicefrac{1}{S} \sum_{s=1}^S \va_s \right) \left( \nicefrac{1}{S} \sum_{s=1}^S \va_s^{\top}\right) \otimes \left(\sum_{s=1}^S \vg_s \right) \left(\sum_{s=1}^S \vg_s^{\top} \right) \eqqcolon \mF^{(\text{reduce})}(\mW)$.
We can express this in matrix form as $\mF^{(\text{reduce})}(\mW) = \left( \nicefrac{1}{S} \mA \vone \right) \left( \nicefrac{1}{S} \mA \vone \right)^{\top} \otimes \left(\mG \vone \right) \left( \mG \vone \right)^{\top}$.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
