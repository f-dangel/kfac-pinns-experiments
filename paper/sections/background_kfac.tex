\subsection{Kronecker-factored Approximate Curvature (KFAC)}

%\toodoo{F.D.  Get to the point more quickly. Make notation  consistent.}

We review the idea of Kronecker-factored Approximate Curvature (KFAC) which was introduced by~\citet{?, martens2015optimizing, ?} as an approximation of the per-layer Fisher information by a Kronecker product to speed up the computation of the natural gradient direction. 
%We review the general principle here. 
%This approach was introduced in the context of maximum likelihood estimation with a neural network and we reivew its basic principles here. 
%Later, we will expand this to Gramian matrices with PDE terms. 
\toodoo{We could also follow \cite{eschenhagen2023kroneckerfactored} for the notation} 


\paragraph{Block-diagonal approximation}
In the first step the Fisher-information matrix $\mF$ can be approximated by a block diagonal matrix with blocks corresponding to the Fisher-information matrices of the individual layers of the network, i.e., $\mF(\vtheta) \approx \operatorname{diag}(\mF(\vtheta^{(1)}), \dots, \mF(\vtheta^{(L)}))$. 
Note that a block diagonal linear system can be solved by solving the subsystems corresponding to the individual blocks, hence reducing the computational complexity. 

\paragraph{Kronecker-factored approximation of the blocks}
We now consider the individual blocks $\mF(\vtheta^{(l)})$, for which we examine $\jac_{\vtheta^{(l)}} u_{\vtheta}(x_n)$ for a fixed data point. 
The parameters $\vtheta^{(l)} = \mW^{(l)}$ of the $l$-th layer appears in the computational graph by $\vz^{(l)} = \mW^{(l)}\vz^{(l-1)}$ and note that by the vec-trick, we have $\jac_{\vtheta^{(l)}} \vz^{(l)} = \vz^{(l-1)} \otimes I$. 
By the chain rule, we have
\begin{align}
    \jac_{\vtheta^{(l)}} u_{\vtheta}(\vx_n) & = \jac_{\vtheta^{(l)}} \vz_n^{(l)} \jac_{\vz^{(l)}}  \vz^{(L)}_n = %(\vz^{(l-1)}_n\otimes I) \nabla_{\vz^{(l)}}  \vz^{(L)}_n = 
    \vz^{(l-1)}_n\otimes  \jac_{\vz^{(l)}}  \vz^{(L)}_n. 
\end{align}
%and hence 
%\begin{align}
%    \nabla_{\vtheta^{(l)}} u_{\vtheta}(x_n)\nabla_{\vtheta^{(l)}} u_{\vtheta}(x_n)^\top = (\vz^{(l-1)}_n\otimes \vz^{(l-1)}_n) \nabla_{\vz^{(l)}}  \vz^{(L)}_n\nabla_{\vz^{(l)}}  \vz^{(L)}_n^\top.  
%\end{align}
Summing over the data points and using  $\sum_n \mA_n \otimes \mB_n \approx (\sum_n \mA_n) \otimes (\sum_n \mB_n)$ we obtain 
\begin{equation}
    \mF(\vtheta^{(l)}) %= \sum_{n=1}^N \nabla_{\vtheta^{(l)}} u_{\vtheta}(x_n)\nabla_{\vtheta^{(l)}} u_{\vtheta}(x_n)^\top 
    \approx \left(\sum_{n=1}^N \vz^{(l-1)}_n {\vz^{(l-1)}_n}^\top \right)\otimes \left(\sum_{n=1}^N\nabla_{\vz^{(l)}}  \vu_n\nabla_{\vz^{(l)}}  \vu_n^\top\right),
\end{equation}
see~\citep{eschenhagen2023kroneckerfactored}. 
Solving a Kronecker-factored linear system is only as expensive as solving two systems of the sizes of the two factors therefore greatly reducing the computational cost. \todo{correct?} 
%the inverse of a Kronecker-factored matrix is given by the Kronecker product of the individual inverses hence reducing the computational complexity of the corresponding linear system. 

\paragraph{Weight sharing}
\todo{add}

\clearpage

Assume we have drawn a data set $\smash{\sD = \left\{ (\vx_n, \vy_n) \right\}_{n=1}^N}$ with $\smash{(\vx_n, \vy_n) \stackrel{\text{i.i.d}}{\sim} p_{\text{data}}(\vx, \vy)}$.
We want to approximate the data-generating process through $p_{\vtheta}(\vx, \vy)$ by modelling a likelihood $p_{\vtheta}(\vy \mid \vx)$ for the labels with a neural network, that is we use $p_{\vtheta}(\vx, \vy) = p_{\text{data}}(\vx) p_{\vtheta}(\vy | \vx)$ and maximize $KL(p_{\text{data}} || p_{\vtheta})$.
Since $p_{\text{data}}$ is not accessible, one replaces $p_{\text{data}}(\vx)$ and $p_{\text{data}}(\vy \mid \vx)$ with their empirical distributions implied by $\sD$.
This yields the objective \cite[see][Section 4]{martens2020new}
\begin{align*}
  \frac{1}{N} \sum_{n=1}^N -\log p_{\vtheta}(\vy_n \mid \vx_n)
\end{align*}
which corresponds to the empirical risk $\frac{1}{N} \sum_{n=1}^N \ell(\vx_n, \vy_n, \vtheta)$ with a negative log-likelihood loss function, such as square or softmax cross-entropy loss.
The likelihood modelled by the neural network is of the form $p_{\vtheta}(\vy_n
\mid \vx_n) = r(\vy_n \mid f_{\vtheta}(\vx))$. The Fisher of our modelled
probability $p_{\vtheta}(\vx, \vy)$ is
\begin{align*}
  \mF(\vtheta)
  &=
    \E_{(\vx, \vy) \sim p_{\vtheta}(\vx,\vy)}
    \left[
    \grad{\vtheta} \log p_{\vtheta}(\vx, \vy)
    (\grad{\vtheta} \log p_{\vtheta}(\vx, \vy))^{\top}
    \right]
  \\
  &=
    \E_{p_{\text{data}}(\vx)}
    \E_{p_{\vtheta}(\vy \mid \vx)}
    \underbrace{
    \left[
    \grad{\vtheta} \log p_{\vtheta}(\vy \mid \vx)
    (\grad{\vtheta} \log p_{\vtheta}(\vy \mid \vx))^{\top}
    \right]
    }_{\coloneqq \mF_{\vy \mid \vx}(\vtheta)}\,.
  \\
  &\approx
    \frac{1}{N} \sum_{n=1}^N
    \E_{p_{\vtheta}(\vy \mid \vx_n)}
    \left[
    \grad{\vtheta} \log p_{\vtheta}(\vy \mid \vx_n)
    (\grad{\vtheta} \log p_{\vtheta}(\vy \mid \vx_n))^{\top}
    \right]
\end{align*}

\paragraph{One datum, no weight sharing:}
Let's start with maximum likelihood estimation with a single data point $(\vx, \vy)$.
Consider a linear layer inside a neural network which maps some vector-valued hidden feature of $\vx$, $\va \in \sR^{D_{\text{in}}}$ to a vector-valued output $\vz \in \sR^{D_{\text{out}}}$ via $\vz = \mW \va$.
$\vz$ is then further processed and used to compute the negative log-likelihood loss $\ell(\vx, \vy, \mW) = - \log p(\vy \mid \vx, \mW)$.
For this single-usage layer, the weigh matrix's Fisher is exactly Kroneckerfactored, $\mF(\mW) = \va \va^{\top} \otimes \E_{\hat{\vy} \sim p(\vy \mid \vx, \mW)}\left[ \vg \vg^{\top} \right]$ where $\vg = \grad{\vz} \ell(\vx, \hat{\vy}, \mW)$.
By applying the chain rule at the layer's output, the Kronecker structure emerges from the output-parameter Jacobian $\jac_{\mW}\vz = \va^{\top} \otimes \mI$.
In practise, we will use one sample from the model's likelihood to estimate the expectation, $\mF(\mW) \approx \vz \vz^{\top} \otimes \vg \vg^{\top}$.

% explain how batch axes are treated
\paragraph{Multiple data, no weight sharing} In the presence of multiple data points, the sum over per-datum Kronecker products is further approximated as a Kronecker product of sums over data points:
\begin{align*}
  \mF(\mW)
  &=
    \frac{1}{N}
    \sum_{n=1}^N
    \va_n \va_n^{\top} \otimes \E_{\hat{\vy}_n \sim p(\vy_n \mid \vx_n, \mW)}\left[ \vg_n \vg_n^{\top} \right]
  \\
  &\approx
    \left(
    \frac{1}{N}
    \sum_{n=1}^N
    \va_n \va_n^{\top}
    \right)
    \otimes
    \left(
    \sum_{n=1}^N
    \E_{\hat{\vy}_n \sim p(\vy_n \mid \vx_n, \mW)}\left[ \vg_n \vg_n^{\top} \right]
    \right)
  \\
  &\approx
    \left(
    \frac{1}{N}
    \sum_{n=1}^N
    \va_n \va_n^{\top}
    \right)
    \otimes
    \left(
    \sum_{n=1}^N
    \vg_n \vg_n^{\top}
    \right)
\end{align*}

% expand approximation treats the shared axis like a batch axis
\paragraph{One datum, weight sharing} Now consider a layer whose weight is applied onto \emph{multiple} vectors.
This concept is known as weight sharing.
This could be a linear layer with matrix-valued inputs like in attention, a convolution layer whose kernel is shared between patches of the input, or weights that are used multiple times throughout the computation graph (e.g.\, weight tying).
This means the layer will not process a single vector $\va$, but a sequence of vectors $\left\{ \va_1, \dots, \va_S \right\}$ where $S$ denotes weight sharing number.
We can column-stack these vectors into a matrix $\mA \in \sR^{D_{\text{in}}\times S}$, likewise for the linear layer's outputs $\vz = \mW \mA \in \sR^{D_{\text{out}}\times S}$ and activation gradients $\mG \in \sR^{D_{\text{out}} \times S}$.
The output-weight Jacobian of a weight-sharing layer is $\jac_{\mW} \mZ = \mA^{\top} \otimes \mI$ \cite[see e.g.][]{dangel2020modular} and the Fisher does not simplify into a Kronecker product without further approximations.
As described in \citet{eschenhagen2023kroneckerfactored}, there are two possible Kronecker approximations for this setup.
We will focus on the \emph{expand} approximation, which yields the Kronecker approximation for convolutional layers proposed by~\citet{grosse2016kroneckerfactored}.
It treats the shared axis like a batch axis,
$\mF(\mW) \approx \nicefrac{1}{S} \sum_{s=1}^S \va_s \va_s^{\top} \otimes \sum_{s=1}^S \vg_s \vg_s^{\top}$ where $\vg_s = \grad{\vz_s} \ell(\vx, \hat{\vy}, \mW)$.
We can express this in matrix notation as $\mF(\mW) \approx \nicefrac{1}{S} \mA \mA^{\top} \otimes \mG \mG^{\top}$.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
