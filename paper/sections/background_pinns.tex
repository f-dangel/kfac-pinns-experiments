For simplicity, we present our approach for multi-layer perceptrons (MLPs) consisting of fully-connected and element-wise activation layers.
However, the generality of Taylor-mode automatic differentiation and KFAC for linear layers with weight sharing allow our KFAC to be applied to such layers (e.g.\,fully-connected, convolution, attention) in arbitrary neural network architectures.

\paragraph{Flattening \& Derivatives}
We vectorize matrices using the \emph{first-index-varies-fastest} convention, i.e.\,column-stacking (row index varies first, column index varies second) and denote the corresponding flattening operation by $\flatten$.
This allows to reduce derivatives of matrix- or tensor-valued objects back to the vector case by flattening a functions input and output domain before differentiation.
The Jacobian of a vector-to-vector function $\va \mapsto \vb(\va)$ has entries $[\jac_{\va}\vb]_{i,j} = \nicefrac{\partial \evb_i}{\partial \eva_j}$.
For a matrix-to-matrix function $\mA \mapsto \mB(\mA)$, the Jacobian is $\jac_{\mA} \mB = \jac_{\flatten \mA }\flatten\mB$.
A useful property of $\flatten$ is $\flatten(\mA\mX\mB) = (\mB^\top\otimes \mA)\flatten{\mX}$ for matrices $\mA, \mX, \mB$ which implies $\jac_\mX(\mA\mX\mB) = \mB^\top\otimes \mA$.

\paragraph{Sequential neural nets} Consider a \emph{sequential neural network} $u_{\vtheta} = f_{\vtheta^{(L)}} \circ f_{\vtheta^{(L-1)}} \circ \ldots \circ f_{\vtheta^{(1)}} $ of depth $L\in\mathbb N$. It consists of layers $f_{\vtheta^{(l)}}\colon \sR^{h^{(l-1)}}\to\sR^{h^{(l)}}$, $\vz^{(l-1)}\mapsto \vz^{(l)} = f_{\vtheta^{(l)}}(\vz^{(l-1)})$ with trainable parameters $\vtheta^{(l)} \in \sR^{p^{(l)}}$ that transform an input $\vz^{(0)} \coloneqq \vx \in \mathbb R^{d \coloneqq h^{(0)}}$ into a prediction $u_\vtheta(\vx) = \vz^{(L)} \in \sR^{h^{(L)}}$ via intermediate representations $\vz^{(l)} \in \sR^{h^{(l)}}$.
In the context of PINNs, we use networks with scalar outputs ($h^{(L)}=1$) and denote the concatenation of all parameters by $\vtheta = (\vtheta^{(1)\top}, \dots, \vtheta^{(L)\top})^{\top} \in \sR^P$.
A common choice is to alternate fully-connected and activation layers.
Linear layers map $\vz^{(l-1)} \mapsto \vz^{(l)} = \mW^{(l)} \vz^{(l-1)}$ using a weight matrix $\mW^{(l)} = \flatten^{-1}\vtheta^{(l)}  \in \sR^{h^{(l)} \times h^{(l-1)}}$ (bias terms can be added as an additional column and by appending a $1$ to the layer input).
Activation layers map $\vz^{(l-1)}\mapsto \vz^{(l)} \sigma(\vz^{(l-1)})$ element-wise for a (typically smooth) $\sigma\colon\mathbb R\to\mathbb R$.

\subsection{Energy Natural Gradients for Physics-Informed Neural Networks}
Let us consider a domain $\Omega\subseteq\mathbb R^d$ and the partial differential equation
\begin{align*}\tag{PE}\label{eq:PE}
  -\mathcal{L} u & = f \quad \text{in }\Omega \\
  u & = g \quad \text{on }\partial\Omega
\end{align*}
with square-integrable right hand side $f\in L^2(\Omega)$ and twice continuously differentiable boundary condition $g\in C^2(\Omega)\cap C(\overline{\Omega})$.
$\mathcal{L}$ denotes a differential operator, e.g.\,the Laplacian $\mathcal{L} u = \Delta u = \sum_{i=1}^d \partial_{\evx_i}^2 u$.
We parametrize $u$ using a neural network and train its parameters $\vtheta$ to minimize the loss
\begin{align}\label{eq:pinn-loss}
  L(\vtheta)
  &=
    \underbrace{\frac{1}{2N_\Omega} \sum_{n=1}^{N_\Omega} (\mathcal{L} u_\vtheta(\vx_i) + f(\vx_n))^2}_{\eqqcolon L_\Omega(\vtheta)} + \underbrace{\frac{1}{2N_{\partial\Omega}}\sum_{n=1}^{N_{\partial\Omega}} ( u_\vtheta(\vx^\text{b}_n) - g(\vx^\text{b}_n))^2}_{\eqqcolon L_{\partial\Omega}(\vtheta)},
\end{align}
with points $\{\vx_n \in \Omega \}_{n=1}^{N_\Omega}$ from the domain's interior, and points $\{\vx^\text{b}_n \in \partial\Omega \}_{n=1}^{N_{\partial\Omega}}$ on its boundary.\footnote{The second regression loss can also include other constraints like measurement data.}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
