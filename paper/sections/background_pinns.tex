For simplicity, we present our approach for multi-layer perceptrons (MLPs) consisting of fully-connected and elementwise non-linear activation layers.
The generality of Taylor-mode automatic differentiation and KFAC for linear layers with weight sharing allow our KFAC to be applied to such layers (e.g.\,fully-connected, convolution, attention) in arbitrary neural network architectures.

\paragraph{Sequential neural nets} Consider a \emph{sequential neural network} $u_{\vtheta} = f_{\vtheta^{(L)}} \circ f_{\vtheta^{(L-1)}} \circ \ldots \circ f_{\vtheta^{(1)}} $ with depth $L\in\mathbb N$ that consists of layers $f_{\vtheta^{(l)}}\colon \sR^{h^{(l-1)}}\to\sR^{h^{(l)}}$, $\vz^{(l-1)}\mapsto \vz^{(l)} = f_{\vtheta^{(l)}}(\vz^{(l-1)})$ with trainable parameters $\vtheta^{(l)} \in \sR^{p^{(l)}}$ that transform an input $\vz^{(0)}= \vx \in \mathbb R^d$ into a prediction $\vu = \vz^{(L)}\in \sR^C$ with $C=1$ for most PINN settings via intermediate representations $\vz^{(l)} \in \sR^{h^{(l)}}$.
We denote the concatenation of all parameters by $\vtheta = (\vtheta^{(1)\top}, \dots, \vtheta^{(L)\top})^{\top} \in \sR^P$.
A common choice is to alternate fully-connected and activation layers.
The weights of a linear layer are trainable and we denote them by $\vtheta^{(l)} = \flatten\mW^{(l)}$, where the layer maps $\vz^{(l-1)} \mapsto \vz^{(l)} = \mW^{(l)} \vz^{(l-1)}$.
Bias terms can be added as an additional column of the weight matrix and by appending a $1$ to the layer input.
Activation layers map $\vz^{(l-1)}\mapsto \vz^{(l)} \sigma(\vz^{(l-1)})$ component-wise for a (typically smooth) function $\sigma\colon\mathbb R\to\mathbb R$.

%We denote the parameters of a linear layer by $\vtheta^{(l)} = \mW^{(l)}$ where the layer is given by $\vz^{(l)} = \mW^{(l)} \vz^{(l-1)}$.

\subsection{Energy Natural Gradients for Physics-Informed Neural Networks}
%\begin{itemize}
%    \item PINNs are one of the most promising approaches to neural network based PDE solvers
%    \item
%\end{itemize}
Let us consider a domain $\Omega\subseteq\mathbb R^d$ and the partial differential equation %and how to write the Gramian/Fisher as the product of a Jacobian and its transpose, see equation \eqref{eq:Jacobian_Fischer}.
%Is this what you were after Felix?
%I think it differs from the standard setting in the sense that suddenly the Laplacian shows up.
%It can also be used to derive a matrix-free version of the energy natural gradient descent, like done in Hessian-free optimization.
%$\bullet$ We consider the Poisson equation
\begin{align*}\tag{PE}\label{eq:PE}
  -\mathcal{L} u & = f \quad \text{in }\Omega \\
  u & = g \quad \text{on }\partial\Omega
\end{align*}
where $f\in L^2(\Omega)$ and $g\in C^2(\Omega)\cap C(\overline{\Omega})$ is a twice continuously differentiable function.
Here, $\mathcal{L}$ denotes a differential operator, e.g., the \emph{Laplacian} $\mathcal{L} u = \Delta u = \sum_i \partial_{x_i}^2 u$.
%Let us consider the function space energy
%  \[ E(u) = \frac{1}{2} \int_\Omega (\Delta u + f)^2 \mathrm dx + \frac12 \int_{\partial\Omega} (u-g)^2 \mathrm ds, \]
%which is designed such that $u$ is a solution of~\eqref{eq:PE} if and only if $E(u)=0$.
%Hence, we discretize the integral and use the following loss function t
In \emph{Physics-informed neural networks (PINNs)}, we parametrize $u$ using a neural network and train its parameters $\vtheta$ to minimize the loss
  %\[ L(\theta) \coloneqq E(u_\theta) = \frac{1}{2} \int_\Omega (\Delta u_\theta + f)^2 \mathrm dx + \frac12 \int_{\partial\Omega} (u_\theta-g)^2 \mathrm ds. \]
%Indeed, one can show that $L$ controls the error, i.e., $\lVert u_\theta - u^\star \rVert_{H^{1/2}(\Omega)} \le c_{\operatorname{reg}} \sqrt{L(\theta)}$, where $c_{\operatorname{reg}}$ denotes the regularity constant of the PDE, see~\cite{?}.
%In practice, we have to discretize the integrals in the PINN loss, and obtain the empirical loss function %with discretized integrals, is
\begin{align}\label{eq:pinn-loss}
  L(\vtheta)
  &=
    %\frac{1}{2N_\Omega}
    %L_\Omega(\vtheta) + %\frac{1}{2N_{\partial\Omega}}
    %L_{\partial\Omega}(\vtheta)
  %\\&
  %=
    \underbrace{\frac{1}{2N_\Omega} \sum_{i=1}^{N_\Omega} (\mathcal{L} u_\vtheta(\vx_i) + f(\vx_i))^2}_{\eqqcolon L_\Omega(\vtheta)} + \underbrace{\frac{1}{2N_{\partial\Omega}}\sum_{i=1}^{N_{\partial\Omega}} ( u_\vtheta(\vx^b_i) - g(\vx^b_i))^2}_{\eqqcolon L_{\partial\Omega}(\vtheta)},
\end{align}
where $(\vx_n)_{n=1}^{N_\Omega}$ and  $(\vx^b_n)_{n=1}^{N_{\partial\Omega}}$ are points in the interior and on the boundary of $\Omega$ respectively.\footnote{Adding additional information, including measurement data can be included via a regression loss that can be treated analogously to the boundary loss.}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
