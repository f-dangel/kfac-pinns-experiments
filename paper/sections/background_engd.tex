\subsection{Energy natural gradients}

%\begin{itemize}
%    \item recently, energy NGs have been proposed
%    \item one can show that they mimic Newtons method in function space
%    \item yield very good accuracy
%    \item
%\end{itemize}

Natural gradients have been introduced by~\citet{amari1998natural} and have shown great success in reinforcement learning, and other problems...
The general idea is to replace the vanilla GD update rule by a preconditioned version
    \[ \theta_{k+1} = \theta_k - \eta_k G(\theta_k)^{-1} \nabla L(\theta_k), \]
where $G(\theta)\in\mathbb R^{p\times p}$, $G(\theta)_{ij} \coloneqq g_{u_\theta}(\partial_{\theta_i} u_\theta, \partial_{\theta_j} u_\theta)$ is a matrix capturing the function space geometry of the problem and its parametrization.
Classically, $G$ is chosen as the Fisher information matrix,
%\begin{equation}
%    F_I(\theta)_{ij} = \sum_{x} \frac{\partial_{\theta_i}p_\theta(x)\partial_{\theta_j}p_\theta(x)}{p_\theta(x)} = \sum_{x} \partial_{\theta_i} \log p_\theta(x) \partial_{\theta_j} \log p_\theta(x),
%\end{equation}
in which case the Riemannian metric $g$ is given by the Fisher-Rao metric~\cite[text]{keylist}.
For a supervised learning problem with training data $(x_1, y_1), \dots, (x_N, y_N)$ the (empirical) Fisher-information matrix commonly used, has entries the entries 
\begin{equation}
  F(\theta)_{ij} = \sum_{n=1}^N \partial_{\theta_i} u_\theta(x_n)\partial_{\theta_j} u_\theta(x_n),
\end{equation}
see~\cite{amari2000natural,martens2020new}. 

In the PINN setting however, the models $u_\theta$ are functions rather than probability measures %$p_\theta$
and the loss involves PDE terms.
%In order to adjust the definition of
To capture the geometric properties of this specific problem we consider the following Fisher / Gramian matrix %to this problemThe energy natural gradient is for this example to use the Fischer/Gramian of the form
\begin{equation}
  F(\theta) = F_\Omega(\theta) + F_{\partial\Omega}(\theta) = \frac1{{N_\Omega}} \sum_{k=1}^{N_\Omega} \partial_{\theta_i} \Delta u_\theta(x_k) \partial_{\theta_j} \Delta u_\theta(x_k) + \frac1{{N_{\partial\Omega}}} \sum_{k=1}^{N_{\partial\Omega}} \partial_{\theta_i} u_\theta(x_k^b) \partial_{\theta_j} u_\theta (x_k^b).
\end{equation}
%where
%\begin{equation}\label{eq:FisherInterior}
%  F_\Omega(\theta)_{ij} = \frac1{{N_\Omega}} \sum_{k=1}^{N_\Omega} \partial_{\theta_i} \Delta u_\theta(x_k) \partial_{\theta_j} \Delta u_\theta(x_k)
  % = \frac1{{N_\Omega}} \sum_{i=1}^{N_\Omega} (\partial_{\theta_i} f_\theta) (\partial_{\theta_j} f_\theta ),
%\end{equation}
% where $f_\theta = \Delta u_\theta$.
%and
%\begin{equation}
%  F_{\partial\Omega}(\theta)_{ij} = \frac1{{N_{\partial\Omega}}} \sum_{k=1}^{N_{\partial\Omega}} \partial_{\theta_i} u_\theta(x_k^b) \partial_{\theta_j} u_\theta (x_k^b).
  % = \frac1{{N_\Omega}} \sum_{i=1}^{N_\Omega} (\partial_{\theta_i} f_\theta) (\partial_{\theta_j} f_\theta ),
%\end{equation}
It can be shown that the energy natural gradient method mimics Newton's method up to a projection onto the tangent space of the model and a discretization error that vanishes quadratically in the step size~\cite{muller2023achieving, }
Further, the Gramian matrix admits an interpretation as the Gau√ü-Newton matrix of the residual function, see~\cite{} and Appendix....

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
