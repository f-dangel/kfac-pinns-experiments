\subsection{Energy natural gradients}

%\begin{itemize}
%    \item recently, energy NGs have been proposed
%    \item one can show that they mimic Newtons method in function space
%    \item yield very good accuracy
%    \item
%\end{itemize}

Natural gradients have been introduced by~\citet{amari1998natural} and have shown great success in reinforcement learning, and other problems...
The general idea is to replace the vanilla GD update rule by a preconditioned version
    \[ \vtheta_{k+1} = \vtheta_k - \eta_k \mG(\vtheta_k)^{-1} \nabla L(\vtheta_k), \]
where $\mG(\vtheta)\in\mathbb R^{p\times p}$, $\mG(\vtheta)_{ij} \coloneqq g_{u_\vtheta}(\partial_{\vtheta_i} u_\vtheta, \partial_{\vtheta_j} u_\vtheta)$ is a matrix capturing the function space geometry of the problem and its parametrization and $g$ denotes a suitable Riemannian metric. 
In least squares regression with data $(\vx_1, \vy_1), \dots, (\vx_N, \vy_N)$ the Gramian $\mG(\vtheta)$ is chosen as the (empirical) Fisher information matrix given by~\citep{martens2020new, eschenhagen2023kroneckerfactored} 
%\begin{equation}
%    F_I(\vtheta)_{ij} = \sum_{x} \frac{\partial_{\vtheta_i}p_\vtheta(x)\partial_{\vtheta_j}p_\vtheta(x)}{p_\vtheta(x)} = \sum_{x} \partial_{\vtheta_i} \log p_\vtheta(x) \partial_{\vtheta_j} \log p_\vtheta(x),
%\end{equation}
%in which case the Riemannian metric $g$ is given by the Fisher-Rao metric~\citet{}.
%For a supervised learning problem with training data $(\vx_1, \vy_1), \dots, (\vx_N, \vy_N)$ the (empirical) Fisher-information matrix commonly used, has entries the entries 
\begin{equation}
  %\mF(\vtheta)_{ij} = \sum_{n=1}^N \partial_{\vtheta_i} u_\vtheta(\vx_n)\partial_{\vtheta_j} u_\vtheta(\vx_n) \quad \text{maybe use?} 
  \mF(\vtheta) = \frac1N\sum_{n=1}^N \jac_{\vtheta} u_{\vtheta}(\vx_n)^\top \jac_{\vtheta} u_{\vtheta}(\vx_n)
  .
\end{equation}
To account for the PDE terms in the loss function in PINNs, 
%the models $u_\vtheta$ are functions rather than probability measures %$p_\vtheta$
%and the loss involves PDE terms.
%In order to adjust the definition of
%To capture the geometric properties of this specific problem we consider the following Fisher / Gramian matrix %to this problemThe energy natural gradient is for this example to use the Fischer/Gramian of the form
the Gramian matrix 
\begin{equation}
  \mG(\vtheta) = %\mG_\Omega(\vtheta) + \mG_{\partial\Omega}(\vtheta) =
  \underbrace{\frac1{{N_\Omega}} \sum_{n=1}^{N_\Omega} \jac_{\vtheta} \Delta u_\vtheta(\vx_n)^\top \jac_{\vtheta} \Delta u_\vtheta(\vx_n)}_{\eqqcolon \mG_\Omega(\vtheta)} + \underbrace{\frac1{{N_{\partial\Omega}}} \sum_{n=1}^{N_{\partial\Omega}} \jac_{\vtheta} u_\vtheta(\vx_n^b)^\top \jac_{\vtheta} u_\vtheta (\vx_n^b)}_{\eqqcolon \mG_{\partial\Omega}(\vtheta)}
  %\frac1{{N_\Omega}} \sum_{k=1}^{N_\Omega} \partial_{\vtheta_i} \Delta u_\vtheta(x_k) \partial_{\vtheta_j} \Delta u_\vtheta(x_k) + \frac1{{N_{\partial\Omega}}} \sum_{k=1}^{N_{\partial\Omega}} \partial_{\vtheta_i} u_\vtheta(x_k^b) \partial_{\vtheta_j} u_\vtheta (x_k^b)
\end{equation}
%where
%\begin{equation}\label{eq:FisherInterior}
%  F_\Omega(\vtheta)_{ij} = \frac1{{N_\Omega}} \sum_{k=1}^{N_\Omega} \partial_{\vtheta_i} \Delta u_\vtheta(x_k) \partial_{\vtheta_j} \Delta u_\vtheta(x_k)
  % = \frac1{{N_\Omega}} \sum_{i=1}^{N_\Omega} (\partial_{\vtheta_i} f_\vtheta) (\partial_{\vtheta_j} f_\vtheta ),
%\end{equation}
% where $f_\vtheta = \Delta u_\vtheta$.
%and
%\begin{equation}
%  F_{\partial\Omega}(\vtheta)_{ij} = \frac1{{N_{\partial\Omega}}} \sum_{k=1}^{N_{\partial\Omega}} \partial_{\vtheta_i} u_\vtheta(x_k^b) \partial_{\vtheta_j} u_\vtheta (x_k^b).
  % = \frac1{{N_\Omega}} \sum_{i=1}^{N_\Omega} (\partial_{\vtheta_i} f_\vtheta) (\partial_{\vtheta_j} f_\vtheta ),
%\end{equation}
has been suggested under the name \emph{energy natural gradient} (ENG), see~\cite{muller2023achieving}. 
The energy natural gradient method mimics Newton's method up to a projection onto the tangent space of the model and a discretization error that vanishes quadratically in the step size 
and improves the accuracy of PINNs by several orders of magnitude when compared to GD, Adam, or BFGS~\citep{muller2023achieving, ?}. 
The Gramian matrix $\mG(\theta)$ can be interpreted as the Gau√ü-Newton matrix of the residual function, see~\cite{} and Appendix....

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
