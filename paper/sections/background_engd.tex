First-order optimizers like gradient descent and Adam struggle at producing satisfactory solutions when used to train PINNs~\citep{cuomo2022scientific}.
Instead, function-space-inspired second-order methods have lately shown promising results~\citep{muller2024optimization}.
We focus on \emph{energy natural gradients (ENGD)} which, when applied to PINN objectives like \Cref{eq:pinn-loss}, correspond to the Gauss-Newton method~\citep{muller2023achieving}.
ENGD mimics Newton's method up to a projection onto the tangent space of the model and a discretization error that vanishes quadratically in the step size, thus providing locally optimal updates in the residual%~\citep{muller2023achieving}
. Alternatively, the Gauss-Newton method can be motivated from the standpoint of operator-preconditioning, where the Gauss-Newton matrix leads to optimal conditioning of the problem~\citep{de2023operator}.

%To account for the PDE terms in the loss function in PINNs,
%the models $u_\vtheta$ are functions rather than probability measures %$p_\vtheta$
%and the loss involves PDE terms.
%In order to adjust the definition of
%To capture the geometric properties of this specific problem we consider the following Fisher / Gramian matrix %to this problemThe energy natural gradient is for this example to use the Fischer/Gramian of the form
Natural gradient methods perform parameter updates via a preconditioned gradient descent scheme $\vtheta_{t+1} = \vtheta_t - \alpha_t \mG(\vtheta_t)^+\nabla L(\vtheta_t)$, where $\mG(\vtheta)^+$ denotes the pseudo-inverse of a suitable \emph{Gramian matrix} $\mG(\vtheta)$.
In the context of ENGD for the PINN-loss~\eqref{eq:pinn-loss}, the Gramian is given by
\begin{equation}\label{eq:gramian}
  \mG(\vtheta) = %\mG_\Omega(\vtheta) + \mG_{\partial\Omega}(\vtheta) =
  \underbrace{\frac1{{N_\Omega}} \sum_{n=1}^{N_\Omega} \jac_{\vtheta} \mathcal{L} u_\vtheta(\vx_n)^\top \jac_{\vtheta} \mathcal{L} u_\vtheta(\vx_n)}_{\eqqcolon \mG_\Omega(\vtheta)} + \underbrace{\frac1{{N_{\partial\Omega}}} \sum_{n=1}^{N_{\partial\Omega}} \jac_{\vtheta} u_\vtheta(\vx_n^b)^\top \jac_{\vtheta} u_\vtheta (\vx_n^b)}_{\eqqcolon \mG_{\partial\Omega}(\vtheta)}.
  %\frac1{{N_\Omega}} \sum_{k=1}^{N_\Omega} \partial_{\vtheta_i} \Delta u_\vtheta(x_k) \partial_{\vtheta_j} \Delta u_\vtheta(x_k) + \frac1{{N_{\partial\Omega}}} \sum_{k=1}^{N_{\partial\Omega}} \partial_{\vtheta_i} u_\vtheta(x_k^b) \partial_{\vtheta_j} u_\vtheta (x_k^b)
\end{equation}
%where
%\begin{equation}\label{eq:FisherInterior}
%  F_\Omega(\vtheta)_{ij} = \frac1{{N_\Omega}} \sum_{k=1}^{N_\Omega} \partial_{\vtheta_i} \Delta u_\vtheta(x_k) \partial_{\vtheta_j} \Delta u_\vtheta(x_k)
  % = \frac1{{N_\Omega}} \sum_{i=1}^{N_\Omega} (\partial_{\vtheta_i} f_\vtheta) (\partial_{\vtheta_j} f_\vtheta ),
%\end{equation}
% where $f_\vtheta = \Delta u_\vtheta$.
%and
%\begin{equation}
%  F_{\partial\Omega}(\vtheta)_{ij} = \frac1{{N_{\partial\Omega}}} \sum_{k=1}^{N_{\partial\Omega}} \partial_{\vtheta_i} u_\vtheta(x_k^b) \partial_{\vtheta_j} u_\vtheta (x_k^b).
  % = \frac1{{N_\Omega}} \sum_{i=1}^{N_\Omega} (\partial_{\vtheta_i} f_\vtheta) (\partial_{\vtheta_j} f_\vtheta ),
%\end{equation}
%has been suggested under the name \emph{energy natural gradient} (ENG), see~\cite{muller2023achieving}.


%The energy natural gradient method mimics Newton's method up to a projection onto the tangent space of the model and a discretization error that vanishes quadratically in the step size
%and improves the accuracy of PINNs by several orders of magnitude when compared to GD, Adam, or BFGS~\citep{muller2023achieving, ?}.
%The Gramian matrix $\mG(\theta)$ can be interpreted as the Gauss-Newton matrix of the residual function, see~\cite{} and Appendix....

%\paragraph{Interpretation as Gau\ss-Newton method}
%Consider the combined residual
Note that $\mG(\vtheta)$ is the Gauss-Newton matrix of $r(\vtheta) = ((2N_\Omega)^{-1/2}r_\Omega(\vtheta), (2N_{\partial\Omega})^{-1/2} r_{\partial\Omega}(\vtheta))^\top$,
%\begin{equation*}
%    r\colon\mathbb R^p\to\mathbb R^{N_\Omega+N_{\partial\Omega}}, \quad
%    \vtheta
%    \mapsto
%    \begin{pmatrix}
%        (2N_\Omega)^{-1/2}r_\Omega(\vtheta) \\
%        (2N_{\partial\Omega})^{-1/2}r_{\partial\Omega}(\vtheta)
%    \end{pmatrix},
%\end{equation*}
where $r_{\Omega}(\vtheta)_n = \Delta u_\vtheta(\vx_n) + f(\vx_n)$ and $r_{\Omega}(\vtheta)_n = u_\vtheta(\vx_n^b) - g(\vx_n^b)$, respectively.
%Then %(with the right choice of the inner product)
%$L(\vtheta) = \frac12 \lVert r(\vtheta) \rVert_2^2$ and hence the Gau\ss-Newton matrix is given by
%\begin{equation*}
%    \jac_\vtheta r(\vtheta)^\top \jac_\vtheta r(\vtheta) = \jac_\vtheta r_\Omega(\vtheta)^T \jac_\vtheta r_\Omega(\vtheta) + \jac_\vtheta r_{\partial\Omega}(\theta)^T \jac_\vtheta r_{\partial\Omega}(\vtheta) = \mG(\vtheta).
%\end{equation*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
