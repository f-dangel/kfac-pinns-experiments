ENGD's Gramian is a sum of PDE and boundary Gramians, $\mG(\vtheta)= \mG_\Omega(\vtheta) + \mG_{\partial\Omega}(\vtheta)$.
We will approximate each Gramian separately with a block diagonal matrix with Kronecker-factored blocks, $\mG_{\bullet}(\vtheta) \approx \diag(\mG^{(1)}_{\bullet}(\vtheta), \dots, \mG^{(L)}_{\bullet}(\vtheta))$ for $\bullet \in \{\Omega, \partial\Omega\}$ with $\mG^{(l)}_{\bullet}(\vtheta) \approx \mA^{(l)}_{\bullet} \otimes \mB^{(l)}_{\bullet}$.
For the boundary Gramian $\mG_{\partial\Omega}(\vtheta)$, we can re-use the established KFAC from~\Cref{eq:kfac-linear} as its loss corresponds to regression over the network's output.
The interior Gramian $\mG_\Omega(\vtheta)$, however, involves PDE terms in the form of network derivatives and therefore \emph{cannot} be approximated with the existing KFAC.
It requires a new approximation that we develop here for the running example of the Poisson equation and more general PDEs (\Cref{eq:KFAC-PINN,eq:KFAC-PINNs-general}).
To do so, we need to make the dependency between the weights and the differential operator $\mathcal{L}u$ explicit.
We use Taylor-mode automatic differentiation to express this computation of higher-order derivatives as forward passes of a larger net with shared weights, for which we then propose a Kronecker-factored approximation, building on KFAC's recently-proposed generalization to linear layers with weight sharing~\cite{eschenhagen2023kroneckerfactored}.

\subsection{Higher-order Forward Mode Automatic Differentiation as Weight Sharing}\label{sec:taylor-mode-AD}

Here, we review higher-order forward mode, also known as \emph{Taylor-mode}, automatic differentiation~\citep{griewank1996algorithm, griewank2008evaluating, bettencourt2019taylor}.
Many PDEs only incorporate first- and second-order partial derivatives and we focus our discussion on second-order Taylor mode for MLPs to keep the presentation light.
However, one can treat higher-order PDEs and arbitrary network architectures completely analogously.

Taylor-mode propagates directional (higher-order) derivatives.
We now recap the forward propagation rules for MLPs consisting of fully-connected and element-wise activation layers.
Our goal is to evaluate first-and second-order partial derivatives of the form $\partial_{\evx_i}u, \partial^2_{\evx_i, \evx_j}u$ for $i,j = 1, \dots, d$.
At the first layer, set $\vz^{(0)} = \vx\in\mathbb R^d, \partial_{x_i}\vz^{(0)} = \ve_i\in\mathbb R^d$, i.e., the $i$-th basis vector and $\partial^2_{x_i,x_j}\vz^{(0)} = \vzero \in\mathbb R^d$.

For a linear layer $f_{\vtheta^{(l)}}(\vz^{(l-1)}) = \mW^{(l)} \vz^{(l-1)}$, applying the chain rule yields the propagation rule
\begin{subequations}\label{eq:forward_pass}
  \begin{align}
    \vz^{(l)}
    &=
      \mW^{(l)} \vz^{(l-1)} \quad \in \sR^{h^{(l)}}\,,
    \\
    \partial_{x_i} \vz^{(l)}
    &=
      \mW^{(l)} \partial_{x_i} \vz^{(l-1)}  \quad \in \sR^{h^{(l)}}\,,
    \\
    \label{subeq:secondOrderForward-LinearLayer}
    \partial^2_{x_i,x_j} \vz^{(l)}
    &=
      \mW^{(l)} \partial^2_{x_i,x_j} \vz^{(l-1)}  \quad \in \sR^{h^{(l)}}\,.
  \end{align}
\end{subequations}
The propagation rule through a nonlinear element-wise activation layer $\vz^{(l-1)}\mapsto \sigma(\vz^{(l-1)})$ is
\begin{subequations}\label{eq:taylor-forward-activation}
  \begin{align}
    \vz^{(l)}
    &=
      \sigma(\vz^{(l-1)})\quad \in \sR^{h^{(l)}}\,,
    \\
    \partial_{x_i} \vz^{(l)}
    &=
      \sigma'(\vz^{(l-1)}) \odot \partial_{x_i} \vz^{(l-1)}\quad \in \sR^{h^{(l)}}\,,
    \\
    \label{subeq:secondOrderForward-nonlinearLayer}
    \partial^2_{x_i,x_j} \vz^{(l)}
    &=
      \partial_{x_i} \vz^{(l-1)} \odot \sigma''(\vz^{(l-1)}) \odot \partial_{x_j} \vz^{(l-1)}
      +
      \sigma'(\vz^{(l-1)}) \odot \partial^2_{x_i,x_j} \vz^{(l-1)}\quad \in \sR^{h^{(l)}}\,.
  \end{align}
\end{subequations}

\paragraph{Forward Laplacian} For differential operators of special structure, we can fuse the Taylor-mode forward propagation of individual directional derivatives in \Cref{eq:forward_pass,eq:taylor-forward-activation} and obtain a more efficient computation.
E.g., to compute not the full Hessian but only the Laplacian, we can simplify the forward pass, which yields the \emph{forward Laplacian} framework of~\citet{li2023forward}.
To the best of our knowledge, this connection has not been pointed out in the literature.
Concretely, by summing~\eqref{subeq:secondOrderForward-LinearLayer} and~\eqref{subeq:secondOrderForward-nonlinearLayer} over $i=j$, we obtain the Laplacian forward pass for linear and activation layers
\begin{subequations}\label{eq:forward-laplacian-main}
  \begin{align}
    \label{eq:forward_Laplacian_linear}
    \Delta_\vx\vz^{(l)}
    &=
      \mW^{(l)}\Delta_\vx\vz^{(l-1)}
      \quad \in \sR^{h^{(l)}}\,,
    \\
    \label{eq:forward_Laplacian_nonlinear}
    \Delta_\vx\vz^{(l)}
    &=
      \sigma'(\vz^{(l-1)})\odot\Delta_\vx\vz^{(l-1)}
      +
      \sum_{i=1}^d \sigma''(\vz^{(l-1)})\odot (\partial_{x_i}{\vz^{(l-1)}})^{\odot 2}
      \quad \in \sR^{h^{(l)}}\,.
  \end{align}
\end{subequations}
This reduces computational cost, but is restricted to PDEs that involve second-order derivatives only via the Laplacian, or a partial Laplacian over a sub-set of input coordinates (e.g.\,heat equation, \Cref{sec:experiments}).
For a more general second-order linear PDE operator $\sum_{i,j=1}^d c_{i,j} \partial^2_{\evx_i,\evx_j}$, the forward pass for a linear layer is $\mathcal{L} \vz^{(l)} = \mW^{(l)}\mathcal{L} \vz^{(l-1)}$, generalizing~\eqref{eq:forward_Laplacian_linear}, and similarly for~\Cref{eq:forward_Laplacian_nonlinear}.
This is similar to \cite{li2024dof}, which transforms the input space using an eigen-decomposition of $c_{i,j}$ to a basis where it is diagonal with entries $\{0, \pm 1\}$, reducing the computation to two standard forward Laplacians.

Importantly, the computation of higher-order derivatives for linear layers boils down to a forward pass through the layer with weight sharing over the different partial derivatives (\Cref{eq:forward_pass}), and weight sharing can potentially be reduced depending on the differential operator's structure (\Cref{eq:forward_Laplacian_linear}).
Therefore, we can use the concept of KFAC in the presence of weight sharing to derive a principled Kronecker approximation for Gramians containing differential operator terms.

\subsection{KFAC for Gauss-Newton Matrices with the Laplace Operator}\label{sec:KFAC-Laplace}
Let's consider the interior Gramian block for a linear layer and the Poisson equation,
\begin{align}
  \mG^{(l)}_\Omega(\vtheta)
  =
  \frac{1}{N_\Omega} \sum_{n=1}^{N_\Omega}
  \left(
  \jac_{\mW^{(l)}} \Delta_\vx u_n  \right)^\top
  \jac_{\mW^{(l)}} \Delta_\vx u_n\,.
\end{align}
Because we made the Laplacian computation explicit through Taylor-mode autodiff (\Cref{sec:taylor-mode-AD}, specifically \Cref{eq:forward_Laplacian_linear}), we can stack all output vectors that share the layer's weight into a matrix
$\mZ_n^{(l)} \in \sR^{h^{(l)} \times S}$ with $S = d+2$ and columns $\mZ_{n, 1}^{(l)} = \vz_n^{(l)}, \mZ_{n, 2}^{(l)} = \partial_{x_1}\vz_n^{(l)}, \dots, \mZ_{n, 1+d}^{(l)} = \partial_{x_d}\vz_n^{(l)}$ and $\mZ_{n, 2+d}^{(l)} = \Delta_\vx\vz_n^{(l)}$ (likewise $\mZ_n^{(l-1)} \in \sR^{h^{(l-1)} \times S}$ for the layer inputs), then apply the chain rule
\begin{align*}
  \jac_{\mW^{(l)}} \Delta_\vx u_n
  &=
    (\jac_{\mZ_n^{(l)}}\Delta_\vx u_n) \jac_{\mW^{(l)}} \mZ_n^{(l)}
    =
    \sum_{s=1}^S
    {
    \underbrace{\mZ^{(l-1)}_{n,s}}_{\in \sR^{h^{(l-1)}}}
    }^{\top}
    \otimes
    \underbrace{\jac_{\mZ^{(l)}_{n,s}} \Delta_{\vx}u_n}_{\eqqcolon \vg_{n,s}^{(l)} \in \sR^{h^{(l)}}}\,,
\end{align*}
which has similar structure than the Jacobian in \Cref{sec:kfac-background}, but with an additional sum over the shared vectors. With that, we can now express the exact interior Gramian as
\begin{equation}\label{eq:laplace_gramian_block_exact}
  \mG^{(l)}_\Omega(\vtheta)
  =
  \frac{1}{N}
  \sum_{n=1}^N
  \sum_{s=1}^S
  \sum_{s'=1}^S
  \mZ^{(l-1)}_{n,s} \mZ^{(l-1)\top}_{n,s'} \otimes \vg_{n,s}^{(l)} \vg_{n,s'}^{(l)\top}.
\end{equation}
Next, we want to approximate \Cref{eq:laplace_gramian_block_exact} with a Kronecker product.
To avoid introducing a new convention, we rely on the KFAC approximation for linear layers with weight sharing developed by \citet{eschenhagen2023kroneckerfactored}---specifically, the approximation called KFAC-expand.
This drops all terms with $s\neq s'$, then applies the expectation approximation from \Cref{sec:kfac-background} over the batch and shared axes:
\begin{tcolorbox}[colframe=kfac, title={KFAC for ENGD with the Laplace operator},bottom=0mm,top=-2mm,middle=0mm]
  \begin{align}\label{eq:KFAC-PINN}
    \mG^{(l)}_\Omega(\vtheta)
    \approx
    \left( \frac{1}{N S} \sum_{n,s=1}^{N,S} \mZ^{(l-1)}_{n,s}{\mZ^{(l-1)}_{n,s}}^\top \right )
    \otimes
    \left(
    \frac{1}{N}
    \sum_{n,s=1}^{N,S} \vg^{(l)}_{n,s}{\vg^{(l)}_{n,s}}^\top
    \right)
    \coloneqq
    \mA_{\Omega}^{(l)} \otimes \mB_{\Omega}^{(l)}
  \end{align}
\end{tcolorbox}

\subsection{KFAC for Generalized Gauss-Newton Matrices Involving General PDE Terms} \label{sec:KFAC-general}
To generalize the previous section, let's consider the general $M$-dimensional PDE system of order $k$,
\begin{equation}\label{eq:general-pde-system}
  \Psi(u, \partial_{\vx} u, \dots, \partial^k_{\vx} u) = \vzero \in \sR^M.
\end{equation}
An order $\alpha \in \{0, \dots, k\}$ has $S_{\alpha} = \binom{d + k - 1}{d - 1}$ independent partial derivatives and the total number of independent derivatives is $S \coloneqq \sum_{\alpha=0}^k S_{\alpha} = \binom{d + k}{k}$.
$\Psi$ is a smooth mapping from all partial derivatives to $\sR^M$, $\Psi\colon \mathbb \sR^S\to\mathbb \sR^M$.
To construct a PINN loss for \Cref{eq:general-pde-system}, we feed the residual $\vr_{\Omega, n}(\vtheta) \coloneqq \Psi(u_{\vtheta}(\vx_n), \partial_{\vx} u_{\vtheta}(\vx_n), \dots, \partial^k_{\vx} u_{\vtheta}(\vx_n)) \in \sR^M$ where $\partial^{\alpha}_{\vx} u_{\vtheta}(\vx_n) \in \sR^{d \times S_{\alpha}}$, into a smooth convex criterion function $\ell\colon \sR^M \to \sR$,
\begin{equation}
  L_{\Omega}(\vtheta)\coloneqq \frac1N\sum_{n=1}^N \ell(\vr_{\Omega,n}(\vtheta))\,.
\end{equation}
The generalized Gauss-Newton (GGN) matrix~\cite{schraudolph2002fast} is the Hessian of $L_{\Omega}(\vtheta)$ when the residual is linearized w.r.t.\,$\vtheta$ before differentiation. It is positive semi-definite and has the form
\begin{align}
  \mG_{\Omega}(\vtheta)
  \coloneqq
  \frac{1}{N}
  \sum_{n=1}^N
  \left(\jac_\vtheta \vr_{\Omega,n}(\vtheta)  \right)^\top
  \mLambda(\vr_{\Omega,n})
  \left(\jac_\vtheta \vr_{\Omega, n}(\vtheta) \right)\,,
\end{align}
where $\mLambda(\vr) \coloneqq \nabla^2_{\vr} \ell(\vr) \in \sR^{M\times M} \succ 0$ the criterion's Hessian, e.g.\,$\ell(\vr) = \nicefrac{1}{2} \lVert \vr \rVert_2^2$ and $\mLambda(\vr) = \mI_M$.

Generalizing the second-order Taylor-mode from \Cref{sec:taylor-mode-AD} to higher orders for the linear layer, we find
\begin{align}
  \partial_\vx^\alpha \vz^{(l)} = \mW^{(l)}\partial_\vx^\alpha \vz^{(l-1)}
  \qquad \in \sR^{h^{(l)} \times S_{\alpha}}
\end{align}
for arbitrary $\alpha$.
Hence, we can derive a forward propagation for the required derivatives in which linear layers process at most $S$ vectors\footnote{Depending on the linear operator, one may reduce weight sharing, as demonstrated for the Laplacian in \Cref{sec:taylor-mode-AD}.}, i.e.
the linear layer's weight is shared over the matrices $\partial^{0}_\vx \vz^{(l-1)} \coloneqq \vz^{(l-1)}, \partial_\vx^1 \vz^{(l-1)}, \dots, \partial_\vx^k \vz^{(l-1)}$. Stacking them into a matrix $\mZ^{(l-1)}_n = (\vz^{(l-1)}, \partial_\vx^1 \vz^{(l-1)}, \dots, \partial_\vx^k \vz^{(l-1)} ) \in \sR^{h^{(l-1)} \times S}$, we can use the chain rule to obtain
\begin{align}
  \begin{split}
    \mG^{(l)}_{\Omega}(\vtheta)
    &=
      \frac{1}{N}
      \sum_{n=1}^N
      \left(\jac_{\mW^{(l)}} \mZ^{(l)}_n \right)^{\top}
      % \underbrace{
      \left(
      \jac_{\mZ^{(l)}_n} \vr_{\Omega,n}
      \right)^{\top}
      \mLambda(\vr_{\Omega,n})
      \left(
      \jac_{\mZ^{(l)}_n} \vr_{\Omega,n}
      \right)
      % }_{\eqqcolon \mLambda^{(l)}_n \in \sR^{S h^{(l)} \times S h^{(l)}}}
      \left(\jac_{\mW^{(l)}} \mZ^{(l)}_n \right)
    \\
    &=
      \frac{1}{N}
      \sum_{n,s,s'=1}^{N,S,S}
      \left(\jac_{\mW^{(l)}} \mZ^{(l)}_{n,s} \right)^{\top}
      \left(
      \jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n}
      \right)^{\top}
      \mLambda(\vr_{\Omega,n})
      \left(
      \jac_{\mZ^{(l)}_{n,s'}} \vr_{\Omega,n}
      \right)
      \left(\jac_{\mW^{(l)}} \mZ^{(l)}_{n,s'} \right)
    \\
    &=
      \frac{1}{N}
      \sum_{n,s,s'=1}^{N,S,S}
      \mZ^{(l-1)}_{n,s}
      \mZ^{(l-1)\top}_{n,s'}
      \otimes
      \left(\jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n} \right)^{\top}
      \mLambda(\vr_{\Omega,n})
      \left(
      \jac_{\mZ^{(l)}_{n,s'}} \vr_{\Omega,n}
      \right)
  \end{split}
\end{align}
where $\mZ_{n,s}^{(l-1)} \in \sR^{h^{(l-1)}}$ denotes the $s$-th column of $\mZ_n^{(l-1)}$.
Following the same steps as in \Cref{sec:KFAC-Laplace}, we apply the KFAC-expand approximation from \cite{eschenhagen2023kroneckerfactored} to obtain the generalization of ~\Cref{eq:KFAC-PINN}:
\begin{tcolorbox}[colframe=kfac, title={KFAC with general PDE terms},bottom=0mm,top=-2mm,middle=0mm]
  \begin{align}\label{eq:KFAC-PINNs-general}
    \begin{split}
      \hspace{-4ex}
      \mG_{\Omega}^{(l)}(\vtheta)
      &\approx
        \left(
        \!\!
        \frac{1}{N S}
        \!\!
        \sum_{n,s=1}^{N,S}
        \!\!
        \mZ^{(l-1)}_{n,s} \mZ^{(l-1)\top}_{n,s'}
        \!\!
        \right)
        \otimes
        \left(
        \!\!
        \frac{1}{N}
        \!\!
        \sum_{n,s=1}^{N,S}
        \!\!
        \left(\jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n} \right)^{\top}
        \!\!\!\mLambda(\vr_{\Omega,n})
        \left(
        \jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n}
        \right)
        \!\!
        \right)
        \!\!
      \\
      &\coloneqq
        \mA_{\Omega}^{(l)} \otimes \mB_{\Omega}^{(l)}
    \end{split}
  \end{align}
\end{tcolorbox}
To bring this expression even closer to \Cref{eq:KFAC-PINN}, we can re-write the second Kronecker factor using an outer product decomposition $\mLambda(\vr_{\Omega,n}) = \sum_{m=1}^M \vl_{n,m} \vl_{n,m}$ with $\vl_{n,m} \in \sR^M$, then introduce $\vg^{(l)}_{n,s,m} \coloneqq (\jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n})^{\top} \vl_{n,m} \in \sR^{h^{(l)}}$ and write the second term as $\nicefrac{1}{N} \sum_{n,s,m=1}^{N,S,M} \vg^{(l)}_{n,s,m} \vg^{(l)\top}_{n,s,m}$.
We refer the interested reader to a more detailed derivation in \Cref{app:derivations}.

\textbf{KFAC for variational problems}
Our proposed KFAC approximation is not limited to PINNs and can be used for variational problems of the form
\begin{align}
  \min_u \int_\Omega \ell(u, \partial u, \dots, \partial^k u) \mathrm{d}\vx\,,
\end{align}
where $\ell\colon\mathbb R^K\to\mathbb R$ is a convex function.
We can perceive this as a special case of the setting above with $\Psi = \operatorname{id}$ and hence the KFAC approximation~\eqref{eq:KFAC-PINNs-general} remains meaningful.
In particular, it can be used for the \emph{deep Ritz method} and other variational approaches to solve PDEs~\citep{yu2018deep}.

\begin{comment}
  \subsection{Computational complexity}
  \todo[inline]{what can we say? probably better to move to the respective approximations}
  \begin{itemize}
  \item GN: $O(p^3+Np^2)$
  \item BFGS: $O(p^2+?)$
  \item L-BFGS: $O(mp+?)$?
  \item KFAC: $O(?)$
  \item
  \end{itemize}
\end{comment}

\subsection{Algorithmic Details}

We provide the additional components that form our KFAC-based optimization algorithms, which are influenced by ENGD and the original KFAC algorithm.
See \Cref{app:pseudo} for pseudo-code.

\paragraph{Exponential moving averages and damping} %Given a combination of interior and condition loss, $\gL_{\Omega},\gL_{\partial\Omega}$ we
First, we approximate the interior and boundary Gramians at iteration $t$  according to~\eqref{eq:KFAC-PINN} and~\eqref{eq:KFAC-PINNs-general} and the classic KFAC approximation, respectively, giving
$\mG_{\Omega,t}^{(l)} \approx \mA_{\Omega,t}^{(l)}\otimes \mB_{\Omega,t}^{(l)}$ and $\mG_{\partial \Omega, t}^{(l)}\approx \mA_{\Omega,t}^{(l)}\otimes \mB_{\Omega,t}^{(l)}$ .
Here, we replace $\mA^{(l)}_{\bullet,t}$ with an exponential moving average $\hat{\mA}^{(l)}_{\bullet,t} = \beta \hat{\mA}^{(l)}_{\bullet,t-1} + (1 - \beta) \mA^{(l)}_{\bullet,t}$
and identically for $\hat{\mB}^{(l)}_{\bullet, t}$ for some $\beta\in[0,1)$.
Further, we add constant damping of strength $\lambda>0$ to all Kronecker factors, obtaining $\tilde{\mA}_{\bullet,t} = \hat{\mA}^{(l)}_{\bullet,t} + \lambda \mI$ and $\tilde{\mB}_{\bullet,t} = \hat{\mB}^{(l)}_{\bullet,t} + \lambda \mI$
leading to the
block-diagonal Kronecker-factored approximation
\begin{align*}
  \mG_{\bullet, t}
  &\approx
    \blockdiag
    \left(
    \tilde{\mA}^{(1)}_{\bullet,t} \otimes \tilde{\mB}_{\bullet,t}^{(1)},
    \dots,
    \tilde{\mA}^{(L)}_{\bullet,t} \otimes \tilde{\mB}_{\bullet,t}^{(L)}
    \right)
    \qquad \bullet \in \{ \Omega, \partial\Omega\}\,.
\end{align*}
    %     over all previous Kronecker factors, i.e.\, as defined in ?? and identically for $\hat{\mB}^{(l)}_{\bullet, t}$.

\paragraph{Gradient pre-conditioning% and damping
}
Given the current mini-batch gradient $\vg^{(l)}$ for layer $l$, we obtain the update direction $\vDelta_t^{(l)} = -(\tilde{\mA}_{\Omega} \otimes \tilde{\mB}_{\Omega} + \tilde{\mA}_{\partial\Omega} \otimes \tilde{\mB}_{\partial\Omega})^{-1} \vg^{(l)}$ by
pre-condition with our Kronecker-factored approximate Gramian.
    %     First, we add constant damping of strength $\lambda>0$ to all Kronecker factors, obtaining $\tilde{\mA}_{\bullet,t} = \hat{\mA}^{(l)}_{\bullet,t} + \lambda \mI$ and $\tilde{\mB}_{\bullet,t} = \hat{\mB}^{(l)}_{\bullet,t} + \lambda \mI$.
    %     Then, we multiply the inverse of the sum of Kronecker products onto the gradient to obtain the update direction
This can be done without building up the full matrix utilizing a generalized eigendecomposition of the individual Kronecker factors, where we use the procedure described in \cite[Appendix I]{martens2015optimizing}.

    %     \paragraph{Combining PDE and boundary approximations}
    %     The boundary Gramian matrix $\mG_{\partial\Omega}(\mW^{(l)})$ is a classic Gauss-Newton matrix and thus we can use a classic KFAC approximation $\hat{\mG}_{\partial\Omega}(\mW^{(l)})$.
    %     To invert the sum $\hat{\mG}_\Omega(\mW^{(l)}) + \hat{\mG}_{\partial\Omega}(\mW^{(l)})$ of the two Kronecker approximations we use the procedure described in \cite[Appendix I]{martens2015optimizing}.

\paragraph{Learning rate and momentum}
We consider two different updates $ \vtheta_{t+1} = \vtheta_t + \vdelta_{t}$ at iteration $t$ from the pre-conditioned gradient $\vDelta_t$, which we call as `KFAC' and `KFAC*'.
KFAC uses momentum %buffer of all previous updates,
$\hat{\vdelta}_t = \mu \vdelta_{t-1} + \vDelta_t$ %(similar to momentum in SGD).
    %     The
where the parameter $\mu$ can be chosen by the practitioner.
Like in ENGD, we use a logarithmic grid line search along
    %     $\hat{\vdelta}_t$, i.e.\,
    %     choosing
leading to the update
$\vdelta_t = \alpha_{\star} \hat{\vdelta}_t$ where
$\alpha_{\star} = \argmin_{\alpha} \gL(\vtheta_t + \alpha \hat{\vdelta}_t)$ where $\alpha \in \{2^{-30}, \dots, 2^0\}$.
KFAC* uses the learning rate and momentum heuristic proposed for the original KFAC optimizer~\citep{martens2015optimizing}.
It parameterizes the iteration's update as $\vdelta_{t+1}(\alpha, \mu) = \alpha \vDelta_t + \mu \vdelta_t$, then obtains the optimal parameters by minimizing the quadratic model $m(\vdelta_{t+1}) = \gL(\vtheta_t) + \vdelta_{t+1}^{\top} \nabla_{\vtheta_t}\gL(\vtheta_t) + \nicefrac{1}{2}\vdelta_{t+1}^{\top} (\mG(\vtheta_t) + \lambda \mI) \vdelta_{t+1}$ with the true damped Gramian.
The optimal learning rate and momentum $\argmin_{\alpha, \mu} m(\vdelta_t)$ are given by~(see \citep[][Section 7]{martens2015optimizing} for details)
\begin{align}
  \begin{pmatrix}
    \alpha^{\star} \\ \mu^{\star}
  \end{pmatrix}
  =
  -
  \begin{pmatrix}
    \vDelta_t^{\top} \mG \vDelta_t + \lambda \left\lVert \vDelta_t \right\rVert^2
    & \vDelta_t \mG \vdelta_t + \lambda \vDelta^{\top}_t \vdelta_t
    \\
    \vDelta_t \mG \vdelta_t + \lambda \vDelta^{\top}_t \vdelta_t
    &
      \vdelta_t^{\top} \mG \vDelta_t + \lambda \left\lVert \vdelta_t \right\rVert^2
  \end{pmatrix}^{-1}
  \begin{pmatrix}
    \vDelta_t \nabla_{\vtheta_t} \gL
    \\
    \vdelta_t \nabla_{\vtheta_t} \gL
  \end{pmatrix}\,.
\end{align}
The computational cost is dominated by the two Gramian-vector products $\mG \vDelta_t$ and $\mG \vdelta_t$ which can be performed with automatic differentiation~\citep{pearlmutter1994fast,schraudolph2002fast}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
