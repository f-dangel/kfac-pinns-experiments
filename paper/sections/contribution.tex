ENGD's Gramian is a sum of PDE and boundary Gramians, $\mG(\vtheta)= \mG_\Omega(\vtheta) + \mG_{\partial\Omega}(\vtheta)$.
We will approximate each Gramian separately with a block diagonal matrix with Kronecker-factored blocks, $\mG_{\bullet}(\vtheta) \approx \diag(\mG^{(1)}_{\bullet}(\vtheta), \dots, \mG^{(L)}_{\bullet}(\vtheta))$ for $\bullet \in \{\Omega, \partial\Omega\}$ with $\mG^{(l)}_{\bullet}(\vtheta) \approx \mA^{(l)}_{\bullet} \otimes \mB^{(l)}_{\bullet}$.
For the boundary Gramian $\mG_{\partial\Omega}(\vtheta)$, we can re-use the established KFAC from~\Cref{eq:kfac-linear} as its loss corresponds to regression over the network's output.
The interior Gramian $\mG_\Omega(\vtheta)$, however, involves PDE terms in the form of network derivatives and therefore \emph{cannot} be approximated with the existing KFAC.
It requires a new approximation that we develop here for the running example of the Poisson equation and more general PDEs (\Cref{eq:KFAC-PINN,eq:KFAC-PINNs-general}).
To do so, we need to make the dependency between the weights and the differential operator $\mathcal{L}u$ explicit.
We use Taylor-mode automatic differentiation to express this computation of higher-order derivatives as forward passes of a larger net with shared weights, for which we then propose a Kronecker-factored approximation, building on KFAC's recently-proposed generalization to linear layers with weight sharing~\cite{eschenhagen2023kroneckerfactored}.

\subsection{Higher-order Forward Mode Automatic Differentiation as Weight Sharing}\label{sec:taylor-mode-AD}

Here, we review higher-order forward mode, also known as \emph{Taylor-mode}, automatic differentiation~\citep[][tutorial in \Cref{app:taylor-mode-tutorial}]{griewank1996algorithm, griewank2008evaluating, bettencourt2019taylor}.
Many PDEs only incorporate first- and second-order partial derivatives and we focus our discussion on second-order Taylor-mode for MLPs to keep the presentation light.
However, one can treat higher-order PDEs and arbitrary network architectures completely analogously.

Taylor-mode propagates directional (higher-order) derivatives.
We now recap the forward propagation rules for MLPs consisting of fully-connected and element-wise activation layers.
Our goal is to evaluate first-and second-order partial derivatives of the form $\partial_{\evx_i}u, \partial^2_{\evx_i, \evx_j}u$ for $i,j = 1, \dots, d$.
At the first layer, set $\vz^{(0)} = \vx\in\mathbb R^d, \partial_{x_i}\vz^{(0)} = \ve_i\in\mathbb R^d$, i.e., the $i$-th basis vector and $\partial^2_{x_i,x_j}\vz^{(0)} = \vzero \in\mathbb R^d$.

For a linear layer $f_{\vtheta^{(l)}}(\vz^{(l-1)}) = \mW^{(l)} \vz^{(l-1)}$, applying the chain rule yields the propagation rule
\begin{subequations}\label{eq:forward_pass}
  \begin{align}
    \vz^{(l)}
    &=
      \mW^{(l)} \vz^{(l-1)} \quad \in \sR^{h^{(l)}}\,,
    \\
    \partial_{x_i} \vz^{(l)}
    &=
      \mW^{(l)} \partial_{x_i} \vz^{(l-1)}  \quad \in \sR^{h^{(l)}}\,,
    \\
    \label{subeq:secondOrderForward-LinearLayer}
    \partial^2_{x_i,x_j} \vz^{(l)}
    &=
      \mW^{(l)} \partial^2_{x_i,x_j} \vz^{(l-1)}  \quad \in \sR^{h^{(l)}}\,.
  \end{align}
\end{subequations}
The propagation rule through a nonlinear element-wise activation layer $\vz^{(l-1)}\mapsto \sigma(\vz^{(l-1)})$ is
\begin{subequations}\label{eq:taylor-forward-activation}
  \begin{align}
    \vz^{(l)}
    &=
      \sigma(\vz^{(l-1)})\quad \in \sR^{h^{(l)}}\,,
    \\
    \partial_{x_i} \vz^{(l)}
    &=
      \sigma'(\vz^{(l-1)}) \odot \partial_{x_i} \vz^{(l-1)}\quad \in \sR^{h^{(l)}}\,,
    \\
    \label{subeq:secondOrderForward-nonlinearLayer}
    \partial^2_{x_i,x_j} \vz^{(l)}
    &=
      \partial_{x_i} \vz^{(l-1)} \odot \sigma''(\vz^{(l-1)}) \odot \partial_{x_j} \vz^{(l-1)}
      +
      \sigma'(\vz^{(l-1)}) \odot \partial^2_{x_i,x_j} \vz^{(l-1)}\quad \in \sR^{h^{(l)}}\,.
  \end{align}
\end{subequations}

\paragraph{Forward Laplacian} For differential operators of special structure, we can fuse the Taylor-mode forward propagation of individual directional derivatives in \Cref{eq:forward_pass,eq:taylor-forward-activation} and obtain a more efficient computation.
E.g., to compute not the full Hessian but only the Laplacian, we can simplify the forward pass, which yields the \emph{forward Laplacian} framework of~\citet{li2023forward}.
To the best of our knowledge, this connection has not been pointed out in the literature.
Concretely, by summing~\eqref{subeq:secondOrderForward-LinearLayer} and~\eqref{subeq:secondOrderForward-nonlinearLayer} over $i=j$, we obtain the Laplacian forward pass for linear and activation layers
\begin{subequations}\label{eq:forward-laplacian-main}
  \begin{align}
    \label{eq:forward_Laplacian_linear}
    \Delta_\vx\vz^{(l)}
    &=
      \mW^{(l)}\Delta_\vx\vz^{(l-1)}
      \quad \in \sR^{h^{(l)}}\,,
    \\
    \label{eq:forward_Laplacian_nonlinear}
    \Delta_\vx\vz^{(l)}
    &=
      \sigma'(\vz^{(l-1)})\odot\Delta_\vx\vz^{(l-1)}
      +
      \textstyle
      \sum_{i=1}^d \sigma''(\vz^{(l-1)})\odot (\partial_{x_i}{\vz^{(l-1)}})^{\odot 2}
      \quad \in \sR^{h^{(l)}}\,.
  \end{align}
\end{subequations}
This reduces computational cost, but is restricted to PDEs that involve second-order derivatives only via the Laplacian, or a partial Laplacian over a sub-set of input coordinates (e.g.\,heat equation, \Cref{sec:experiments}).
For a more general second-order linear PDE operator $\gL = \sum_{i,j=1}^d c_{i,j} \partial^2_{\evx_i,\evx_j}$, the forward pass for a linear layer is $\mathcal{L} \vz^{(l)} = \mW^{(l)}\mathcal{L} \vz^{(l-1)} \in \sR^{h^{(l)}}$, generalizing~\eqref{eq:forward_Laplacian_linear}, and similarly for~\Cref{eq:forward_Laplacian_nonlinear}
\begin{align*}
  \mathcal{L}\vz^{(l)}
  &=
    \sigma'(\vz^{(l-1)})\odot\mathcal{L}\vz^{(l-1)}
    +
    \textstyle
    \sum_{i,j=1}^d c_{i,j} \sigma''(\vz^{(l-1)})\odot \partial_{x_i}{\vz^{(l-1)}}\odot \partial_{x_j}{\vz^{(l-1)}}
    \quad \in \sR^{h^{(l)}}\,,
\end{align*}
see \Cref{sec:generalized-forward-laplacian} for details. This is different from \cite{li2024dof}, which transforms the input space such that the coefficients are diagonal with entries $\{0, \pm 1\}$, reducing the computation to two forward Laplacians.

Importantly, the computation of higher-order derivatives for linear layers boils down to a forward pass through the layer with weight sharing over the different partial derivatives (\Cref{eq:forward_pass}), and weight sharing can potentially be reduced depending on the differential operator's structure (\Cref{eq:forward_Laplacian_linear}).
Therefore, we can use the concept of KFAC in the presence of weight sharing to derive a principled Kronecker approximation for Gramians containing differential operator terms.

\subsection{KFAC for Gauss-Newton Matrices with the Laplace Operator}\label{sec:KFAC-Laplace}
Let's consider the Poisson equation's interior Gramian block for a linear layer (suppressing $\Omega$ in $N_{\Omega}$)
\begin{align*}
  \mG^{(l)}_\Omega(\vtheta)
  =
  \nicefrac{1}{N}
  \textstyle
  \sum_{n=1}^{N}
  \left(
  \jac_{\mW^{(l)}} \Delta_\vx u_n  \right)^\top
  \jac_{\mW^{(l)}} \Delta_\vx u_n\,.
\end{align*}
Because we made the Laplacian computation explicit through Taylor-mode autodiff (\Cref{sec:taylor-mode-AD}, specifically \Cref{eq:forward_Laplacian_linear}), we can stack all output vectors that share the layer's weight into a matrix
$\mZ_n^{(l)} \in \sR^{h^{(l)} \times S}$ with $S = d+2$ and columns $\mZ_{n, 1}^{(l)} = \vz_n^{(l)}, \mZ_{n, 2}^{(l)} = \partial_{x_1}\vz_n^{(l)}, \dots, \mZ_{n, 1+d}^{(l)} = \partial_{x_d}\vz_n^{(l)}$, and $\mZ_{n, 2+d}^{(l)} = \Delta_\vx\vz_n^{(l)}$ (likewise $\mZ_n^{(l-1)} \in \sR^{h^{(l-1)} \times S}$ for the layer inputs), then apply the chain rule
\begin{align*}
  \jac_{\mW^{(l)}} \Delta_\vx u_n
  &=
    (\jac_{\mZ_n^{(l)}}\Delta_\vx u_n) \jac_{\mW^{(l)}} \mZ_n^{(l)}
    =
    \textstyle
    \sum_{s=1}^S
    {
    \underbrace{\mZ^{(l-1)}_{n,s}}_{\in \sR^{h^{(l-1)}}}
    }^{\top}
    \otimes
    \underbrace{\jac_{\mZ^{(l)}_{n,s}} \Delta_{\vx}u_n}_{\eqqcolon \vg_{n,s}^{(l)} \in \sR^{h^{(l)}}}\,,
\end{align*}
which has a structure similar to the Jacobian in \Cref{sec:kfac-background}, but with an additional sum over the $S$ shared vectors. With that, we can now express the exact interior Gramian for a layer as
\begin{equation}\label{eq:laplace_gramian_block_exact}
  \mG^{(l)}_\Omega(\vtheta)
  =
  \nicefrac{1}{N}
  \textstyle
  \sum_{n=1}^N
  \sum_{s=1}^S
  \sum_{s'=1}^S
  \mZ^{(l-1)}_{n,s} \mZ^{(l-1)\top}_{n,s'} \otimes \vg_{n,s}^{(l)} \vg_{n,s'}^{(l)\top}.
\end{equation}
Next, we want to approximate \Cref{eq:laplace_gramian_block_exact} with a Kronecker product.
To avoid introducing a new convention, we rely on the KFAC approximation for linear layers with weight sharing developed by \citet{eschenhagen2023kroneckerfactored}---specifically, the approximation called \emph{KFAC-expand}.
This drops all terms with $s\neq s'$, then applies the expectation approximation from \Cref{sec:kfac-background} over the batch and shared axes:
\begin{tcolorbox}[colframe=kfac, title={KFAC for the Gauss-Newton matrix of a Laplace operator},bottom=0mm,top=-2mm,middle=0mm]
  \begin{align}\label{eq:KFAC-PINN}
    \mG^{(l)}_\Omega(\vtheta)
    \approx
    \left( \frac{1}{N S} \sum_{n,s=1}^{N,S} \mZ^{(l-1)}_{n,s}{\mZ^{(l-1)}_{n,s}}^\top \right )
    \otimes
    \left(
    \frac{1}{N}
    \sum_{n,s=1}^{N,S} \vg^{(l)}_{n,s}{\vg^{(l)}_{n,s}}^\top
    \right)
    \eqqcolon
    \mA_{\Omega}^{(l)} \otimes \mB_{\Omega}^{(l)}
  \end{align}
\end{tcolorbox}

\subsection{KFAC for Generalized Gauss-Newton Matrices Involving General PDE Terms} \label{sec:KFAC-general}
To generalize the previous section, let's consider the general $M$-dimensional PDE system of order $k$,
\begin{equation}\label{eq:general-pde-system}
  \Psi(u, D_{\vx} u, \dots, D^k_{\vx} u) = \vzero \in \sR^M,
\end{equation}
where $D^m_\vx u$ collects all partial derivatives of order $m$.
For $m\in \{0, \dots, k\}$ there are $S_m = \binom{d + m - 1}{d - 1}$ independent partial derivatives and the total number of independent derivatives is $S \coloneqq \sum_{m=0}^k S_{m} = \binom{d + k}{k}$.
$\Psi$ is a smooth mapping from all partial derivatives to $\sR^M$, $\Psi\colon \mathbb \sR^S\to\mathbb \sR^M$.
To construct a PINN loss for \Cref{eq:general-pde-system}, we feed the residual $\vr_{\Omega, n}(\vtheta) \coloneqq \Psi(u_{\vtheta}(\vx_n), D_{\vx} u_{\vtheta}(\vx_n), \dots, D^k_{\vx} u_{\vtheta}(\vx_n)) \in \sR^M$ where $D^{m}_{\vx} u_{\vtheta}(\vx_n) \in \sR^{d \times S_{m}}$ into a smooth convex criterion function $\ell\colon \sR^M \to \sR$,
\begin{equation}
  L_{\Omega}(\vtheta)\coloneqq \nicefrac{1}{N}
  \textstyle
  \sum_{n=1}^N \ell(\vr_{\Omega,n}(\vtheta))\,.
\end{equation}
The generalized Gauss-Newton (GGN) matrix~\cite{schraudolph2002fast} is the Hessian of $L_{\Omega}(\vtheta)$ when the residual is linearized w.r.t.\,$\vtheta$ before differentiation. It is positive semi-definite and has the form
\begin{align}
  \mG_{\Omega}(\vtheta)
  \coloneqq
  \nicefrac{1}{N}
  \textstyle
  \sum_{n=1}^N
  \left(\jac_\vtheta \vr_{\Omega,n}(\vtheta)  \right)^\top
  \mLambda(\vr_{\Omega,n})
  \left(\jac_\vtheta \vr_{\Omega, n}(\vtheta) \right)\,,
\end{align}
with $\mLambda(\vr) \coloneqq \nabla^2_{\vr} \ell(\vr) \in \sR^{M\times M} \succ 0$ the criterion's Hessian, e.g.\,$\ell(\vr) = \nicefrac{1}{2} \lVert \vr \rVert_2^2$ and $\mLambda(\vr) = \mI_M$.

Generalizing the second-order Taylor-mode from \Cref{sec:taylor-mode-AD} to higher orders for the linear layer, we find
\begin{align}
  D_\vx^m \vz^{(l)} = \mW^{(l)}D_\vx^m \vz^{(l-1)}
  \qquad \in \sR^{h^{(l)} \times S_{m}}
\end{align}
for any $m$.
Hence, we can derive a forward propagation for the required derivatives where a linear layer processes at most $S$ vectors\footnote{Depending on the linear operator, one may reduce weight sharing, as demonstrated for the Laplacian in \Cref{sec:taylor-mode-AD}.}, i.e.\,the linear layer's weight is shared over the matrices $D^{0}_\vx \vz^{(l-1)} \coloneqq \vz^{(l-1)}, D_\vx^1 \vz^{(l-1)}, \dots, D_\vx^k \vz^{(l-1)}$. Stacking them into a matrix $\mZ^{(l-1)}_n = (\vz^{(l-1)}, D_\vx^1 \vz^{(l-1)}, \dots, D_\vx^k \vz^{(l-1)} ) \in \sR^{h^{(l-1)} \times S}$ (and $\mZ^{(l)}_n$ for the outputs), the chain rule yields
\begin{align*}
  \begin{split}
    \mG^{(l)}_{\Omega}(\vtheta)
    &=
      \nicefrac{1}{N}
      \textstyle
      \sum_{n=1}^N
      \left(\jac_{\mW^{(l)}} \mZ^{(l)}_n \right)^{\top}
      \left(
      \jac_{\mZ^{(l)}_n} \vr_{\Omega,n}
      \right)^{\top}
      \mLambda(\vr_{\Omega,n})
      \left(
      \jac_{\mZ^{(l)}_n} \vr_{\Omega,n}
      \right)
      \left(\jac_{\mW^{(l)}} \mZ^{(l)}_n \right)
    \\
    &=
      \nicefrac{1}{N}
      \textstyle
      \sum_{n,s,s'=1}^{N,S,S}
      \left(\jac_{\mW^{(l)}} \mZ^{(l)}_{n,s} \right)^{\top}
      \left(
      \jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n}
      \right)^{\top}
      \mLambda(\vr_{\Omega,n})
      \left(
      \jac_{\mZ^{(l)}_{n,s'}} \vr_{\Omega,n}
      \right)
      \left(\jac_{\mW^{(l)}} \mZ^{(l)}_{n,s'} \right)
    \\
    &=
      \nicefrac{1}{N}
      \textstyle
      \sum_{n,s,s'=1}^{N,S,S}
      \mZ^{(l-1)}_{n,s}
      \mZ^{(l-1)\top}_{n,s'}
      \otimes
      \left(\jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n} \right)^{\top}
      \mLambda(\vr_{\Omega,n})
      \left(
      \jac_{\mZ^{(l)}_{n,s'}} \vr_{\Omega,n}
      \right)
  \end{split}
\end{align*}
where $\mZ_{n,s}^{(l-1)} \in \sR^{h^{(l-1)}}$ denotes the $s$-th column of $\mZ_n^{(l-1)}$.
Following the same steps as in \Cref{sec:KFAC-Laplace}, we apply the KFAC-expand approximation from \cite{eschenhagen2023kroneckerfactored} to obtain the generalization of ~\Cref{eq:KFAC-PINN}:
\begin{tcolorbox}[colframe=kfac, title={KFAC for the GGN matrix of a general PDE operator},bottom=0mm,top=-2mm,middle=0mm]
  \begin{align}\label{eq:KFAC-PINNs-general}
    \begin{split}
      \hspace{-4ex}
      \mG_{\Omega}^{(l)}(\vtheta)
      &\approx
        \left(
        \!\!
        \frac{1}{N S}
        \!\!
        \sum_{n,s=1}^{N,S}
        \!\!
        \mZ^{(l-1)}_{n,s} \mZ^{(l-1)\top}_{n,s'}
        \!\!
        \right)
        \otimes
        \left(
        \!\!
        \frac{1}{N}
        \!\!
        \sum_{n,s=1}^{N,S}
        \!\!
        \left(\jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n} \right)^{\top}
        \!\!\!\mLambda(\vr_{\Omega,n})
        \left(
        \jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n}
        \right)
        \!\!
        \right)
        \!\!
      \\
      &\eqqcolon
        \mA_{\Omega}^{(l)} \otimes \mB_{\Omega}^{(l)}
    \end{split}
  \end{align}
\end{tcolorbox}
To bring this expression even closer to \Cref{eq:KFAC-PINN}, we can re-write the second Kronecker factor using an outer product decomposition $\mLambda(\vr_{\Omega,n}) = \sum_{m=1}^M \vl_{n,m} \vl_{n,m}$ with $\vl_{n,m} \in \sR^M$, then introduce $\vg^{(l)}_{n,s,m} \coloneqq (\jac_{\mZ^{(l)}_{n,s}} \vr_{\Omega,n})^{\top} \vl_{n,m} \in \sR^{h^{(l)}}$ and write the second term as $\nicefrac{1}{N} \sum_{n,s,m=1}^{N,S,M} \vg^{(l)}_{n,s,m} \vg^{(l)\top}_{n,s,m}$, similar to the Kronecker-factored low-rank (KFLR) approach of~\citet{botev2017practical}.

\textbf{KFAC for variational problems}
Our proposed KFAC approximation is not limited to PINNs and can be used for variational problems of the form
\begin{align}
  \min_u \int_\Omega \ell(u, \partial_{\vx} u, \dots, \partial^k_{\vx} u) \mathrm{d}\vx\,,
\end{align}
where $\ell\colon\mathbb R^K\to\mathbb R$ is a convex function.
We can perceive this as a special case of the setting above with $\Psi = \operatorname{id}$ and hence the KFAC approximation~\eqref{eq:KFAC-PINNs-general} remains meaningful.
In particular, it can be used for the \emph{deep Ritz method} and other variational approaches to solve PDEs~\citep{yu2018deep}.

\subsection{Algorithmic Details}

To design an optimizer based on our KFAC approximation, we re-use techniques from the original KFAC~\cite{martens2015optimizing} \& ENGD~\cite{muller2023achieving} algorithms.
\Cref{app:pseudo} shows pseudo-code for our method on the Poisson equation.

At iteration $t$, we approximate the per-layer interior and boundary Gramians using our derived Kronecker approximation (\Cref{eq:KFAC-PINN,eq:KFAC-PINNs-general}),
$\mG_{\Omega,t}^{(l)} \approx \mA_{\Omega,t}^{(l)}\otimes \mB_{\Omega,t}^{(l)}$ and $\mG_{\partial \Omega, t}^{(l)}\approx \mA_{\Omega,t}^{(l)}\otimes \mB_{\Omega,t}^{(l)}$.

\textbf{Exponential moving average and damping}
For preconditioning, we accumulate the Kronecker factors $\mA^{(l)}_{\bullet,t}, \mB^{(l)}_{\bullet,t}$ over time using an exponential moving average $\hat{\mA}^{(l)}_{\bullet,t} = \beta \hat{\mA}^{(l)}_{\bullet,t-1} + (1 - \beta) \mA^{(l)}_{\bullet,t}$ of factor $\beta\in[0,1)$ (identically for $\hat{\mB}^{(l)}_{\bullet, t}$), similar to the original KFAC.
Moreover, we apply the same constant damping of strength $\lambda>0$ to all Kronecker factors, $\tilde{\mA}^{(l)}_{\bullet,t} = \hat{\mA}^{(l)}_{\bullet,t} + \lambda \mI$ and $\tilde{\mB}^{(l)}_{\bullet,t} = \hat{\mB}^{(l)}_{\bullet,t} + \lambda \mI$ such that the curvature approximation used for preconditioning at step $t$ is
\begin{align*}
  \mG_{\bullet, t}
  \approx
    \diag
    \left(
    \tilde{\mA}^{(1)}_{\Omega,t} \otimes \tilde{\mB}_{\Omega,t}^{(1)},
    \dots,
    \tilde{\mA}^{(L)}_{\Omega,t} \otimes \tilde{\mB}_{\Omega,t}^{(L)}
    \right)
  +
    \diag
    \left(
    \tilde{\mA}^{(1)}_{\partial\Omega,t} \otimes \tilde{\mB}_{\partial\Omega,t}^{(1)},
    \dots,
    \tilde{\mA}^{(L)}_{\partial\Omega,t} \otimes \tilde{\mB}_{\partial\Omega,t}^{(L)}
    \right)\,.
\end{align*}

\textbf{Gradient preconditioning}
Given layer $l$'s mini-batch gradient $\vg_t^{(l)} = \nicefrac{\partial L(\vtheta_t)}{\partial\vtheta^{(l)}_t} \in \sR^{p^{(l)}}$, we obtain an update direction $\vDelta_t^{(l)} = -(\tilde{\mA}^{(l)}_{\Omega,t} \otimes \tilde{\mB}^{(l)}_{\Omega,t} + \tilde{\mA}^{(l)}_{\partial\Omega,t} \otimes \tilde{\mB}^{(l)}_{\partial\Omega,t})^{-1} \vg_t^{(l)} \in \sR^{p^{(l)}}$ using the trick of \cite[Appendix I]{martens2015optimizing} to invert the Kronecker sum which requires eigen-decomposing all Kronecker factors.

\textbf{Learning rate and momentum} From the preconditioned gradient $\vDelta_t \in \sR^P$, we consider two different updates $ \vtheta_{t+1} = \vtheta_t + \vdelta_{t}$ we call \emph{KFAC} and \emph{KFAC*}.
KFAC uses momentum over previous updates, $\hat{\vdelta}_t = \mu \vdelta_{t-1} + \vDelta_t$, and $\mu$ is chosen by the practitioner.
Like ENGD, it uses a logarithmic grid line search, selecting $\vdelta_t = \alpha_{\star} \hat{\vdelta}_t$ with $\alpha_{\star} = \argmin_{\alpha} L(\vtheta_t + \alpha \hat{\vdelta}_t)$ where $\alpha \in \{2^{-30}, \dots, 2^0\}$.
KFAC* uses the automatic learning rate and momentum heuristic of the original KFAC optimizer.
It parametrizes the iteration's update as $\vdelta_{t+1}(\alpha, \mu) = \alpha \vDelta_t + \mu \vdelta_t$, then obtains the optimal parameters by minimizing the quadratic model $m(\vdelta_{t+1}) = L(\vtheta_t) + \vdelta_{t+1}^{\top} \vg_t + \nicefrac{1}{2}\vdelta_{t+1}^{\top} (\mG(\vtheta_t) + \lambda \mI) \vdelta_{t+1}$ with the exact damped Gramian.
The optimal learning rate and momentum $\argmin_{\alpha, \mu} m(\vdelta_{t+1})$ are
\begin{align*}
  \begin{pmatrix}
    \alpha_{\star} \\ \mu_{\star}
  \end{pmatrix}
  =
  -
  \begin{pmatrix}
    \vDelta_t^{\top} \mG(\vtheta_t) \vDelta_t + \lambda \left\lVert \vDelta_t \right\rVert^2
    & \vDelta_t^{\top} \mG(\vtheta_t) \vdelta_t + \lambda \vDelta^{\top}_t \vdelta_t
    \\
    \vDelta_t^{\top} \mG(\vtheta_t) \vdelta_t + \lambda \vDelta^{\top}_t \vdelta_t
    &
      \vdelta_t^{\top} \mG(\vtheta_t) \vDelta_t + \lambda \left\lVert \vdelta_t \right\rVert^2
  \end{pmatrix}^{-1}
  \begin{pmatrix}
    \vDelta_t^{\top} \vg_t
    \\
    \vdelta_t^{\top} \vg_t
  \end{pmatrix}
\end{align*}
(see \citep[][Section 7]{martens2015optimizing} for details).
The computational cost is dominated by the two Gramian-vector products with $\vDelta_t$ and $\vdelta_t$.
By using the Gramian's outer product structure~\cite{dangel2022vivit,papyan2019measurements}, we perform them with autodiff~\citep{pearlmutter1994fast,schraudolph2002fast} using one Jacobian-vector product each, as recommended in \cite{martens2015optimizing}.


\paragraph{Computational complexity}

\paragraph{Computation of Laplacians }
We use the forward Laplacian framework in our implementation, which we found to be significantly faster and more memory efficient than using \textmd{functorch} to compute a batched Hessian trace, see \Cref{app:?}. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
