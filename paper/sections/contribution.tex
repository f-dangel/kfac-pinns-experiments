
The overall Gramian matrix of ENGD is given by the sum $\mG(\vtheta)= \mG_\Omega(\vtheta) + \mG_{\partial\Omega}(\vtheta)$ of two Gramian matrices stemming from the PDE and the boundary term, respectively.
A Kronecker factored approximation of the boundary Gramian $\mG_{\partial\Omega}(\vtheta)$ can be obtained just like the case of the  Fisher $\mF(\vtheta)$.
The interior Gramian $\mG_\Omega(\vtheta)$, which involves PDE terms, requires a different approximation that we provide in this section in \eqref{eq:KFAC-PINN} and \eqref{eq:KFAC-PINNs-general} for the Laplace equation and more general PDEs, respectively.
For this, we use forward mode automatic differentiation to express the computation of higher-order derivatives as forward passes of a larger network with shared weights, for which we propose a Kronecker-factored approximation generalizing the expand approximation by~\citet{eschenhagen2023kroneckerfactored}.
The approximations of $\mG_{\Omega}(\vtheta)$ and $\mG_{\partial\Omega}(\vtheta)$ can be %combined into a single approximation or
inverted using an explicit formula based on the individual inversions~\citep{martens2015optimizing}.

\subsection{Higher-order Forward Mode Automatic Differentiation as Weight Sharing}
\label{sec:taylor-mode-AD}

Here, we review higher-order forward mode differentiation, also known as {Taylor-mode automatic differentiation}~\citep{griewank1996algorithm, griewank2008evaluating, bettencourt2019taylor}.
Many PDEs only incorporate first and second partial derivatives and we focus our discussion here on second-order automatic differentiation for the sake of simplicity of the presentation.
However, a completely analogous approach can be taken for higher-order PDEs.

How to compute the function values through a forward pass is well known.
%The computation of the gradient through forward mode differentiation uses the chain rule $\jac (f\circ g) = \jac f (g) \jac g$.
%If we want to compute the Hessian in a forward-mode style, we use the \emph{Hessian chain rule}, which is given by
%\begin{align}
       %\nabla^2 (f\circ g) & = (\jac g)^\top\cdot \nabla^2 f(g) \cdot \jac g + \sum_{k} \partial_{k} f(g) \cdot \nabla^2 g_k \\
       %\partial_{\vx_i} \partial_{\vx_j} (f \circ g)(\vx) & = \partial_{\vx_i} g(\vx)^\top \nabla^2f(g(\vx))\partial_{\vx_j} g(\vx) + \jac f(g(\vx)) \partial_{\vx_i} \partial_{\vx_j} g(\vx).
%\end{align}
If $f_{\vtheta^{(l)}}(\vz) = \mW^{(l)} \vz %+ \vb^{(l)}
$ is a linear layer, then setting $\vz^{(0)} = \vx\in\mathbb R^d, \partial_{\vx_i}\vz^{(0)} = \ve_i\in\mathbb R^d$, i.e., the $i$-th basis vector and $\partial_{\vx_i}\partial_{\vx_j}\vz^{(0)} = 0\in\mathbb R^d$, applying the chain rule gives the second-order forward pass is given by
\begin{subequations}\label{eq:forward_pass}
    \begin{align}
    \vz^{(l)} & = \mW^{(l)} \vz^{(l-1)} %+ \vb^{(l)}
    \\
    \partial_{\vx_i} \vz^{(l)} & = \mW^{(l)} \partial_{\vx_i} \vz^{(l-1)} \\
    \partial_{\vx_i}\partial_{\vx_j} \vz^{(l)} & = \mW^{(l)} \partial_{\vx_i}\partial_{\vx_j} \vz^{(l-1)} \label{subeq:secondOrderForward-LinearLayer}.
\end{align}
\end{subequations}
The second-order forward pass through a nonlinear layer $\vz\mapsto \sigma(\vz)$ is given by
\begin{subequations}
    \begin{align}
    \vz^{(l)} & = \sigma(\vz^{(l-1)}) \\
    \partial_{\vx_i} \vz^{(l)} & = \sigma'(\vz^{(l-1)}) \odot \partial_{\vx_i} \vz^{(l-1)} \\
    \partial_{\vx_i}\partial_{\vx_j} \vz^{(l)} & = \partial_{\vx_i} \vz^{(l-1)} \odot \sigma''(\vz^{(l-1)}) \odot \partial_{\vx_j} \vz^{(l-1)} + \sigma'(\vz^{(l-1)}) \odot \partial_{\vx_i} \partial_{\vx_j} \vz^{(l-1)} \label{subeq:secondOrderForward-nonlinearLayer}.
\end{align}
\end{subequations}

\paragraph{Forward Laplacian}
To compute not the full Hessian but only the Laplacian, we can simplify the forward pass only to propagate the Laplacian which is known as \emph{forward Laplacian}~\citep{li2023forward}.
Summing~\eqref{subeq:secondOrderForward-LinearLayer} and~\eqref{subeq:secondOrderForward-nonlinearLayer} over $i=j$ we obtain the Laplacian forward pass for linear layers by
%We obtain the Laplacian forward pass by summing~\eqref{subeq:secondOrderForward-LinearLayer} and~\eqref{subeq:secondOrderForward-nonlinearLayer} over $i=j$ which yields
%\begin{subequations}
%    \begin{align}
%        \Delta \vz^{(l)} & = \mW^{(l)} \Delta \vz^{(l-1)} \quad \text{and } \\
%        \Delta \vz^{(l)} & = \sum_i \sigma''(\vz^{(l-1)}) \odot (\partial_{\vx_i} \vz^{(l-1)})^{\odot 2} + \sigma'(\vz^{(l-1)}) \odot \Delta \vz^{(l-1)}
%    \end{align}
%\end{subequations}
\begin{comment}
    \begin{align}
  %\nonumber
  \mZ^{(l)}\coloneqq %\underbrace{
  \begin{pmatrix}
    \vz^{(l)}
    \\
    \partial_{\vx_1} \vz^{(l)}
    \\
    \vdots
    \\
    \partial_{\vx_d} \vz^{(l)}
    \\
    %\partial_i\partial_j
    \Delta_\vx\vz^{(l)}
  \end{pmatrix}
  %}_{\eqqcolon %\mT^f
  %\in \sR^{D_{\text{out}} \times (D+2)}
  %\mZ^{(l)}}
  & =
    %\begin{pmatrix}
      %\mW^{(l)} %& \vb^{(l)}
    %\end{pmatrix}
    %\underbrace{
    %\begin{pmatrix}
    %  \vz^{(l-1)}
    %  &
    %  \partial_{i} \vz^{(l-1)}
    %  &
    %    \partial_i\partial_j \vz^{(l-1)}
      %\\1 & 0 & 0
    %\end{pmatrix}
    %}_{\eqqcolon \mT^h \in \sR^{(D_{\text{in}} +1) \times (D+2)}}\,,
    %\shortintertext{or, in compact form,}
    %\mT^f
  %&=
    %\tilde{\mW}
    %\mT^h\,.
    \begin{pmatrix}
    \mW^{(l)}\vz^{(l-1)}
    \\
    \mW^{(l)}\partial_{\vx_1} \vz^{(l-1)}
    \\
    \vdots
    \\
    \mW^{(l)}\partial_{\vx_d} \vz^{(l-1)}
    \\
    %\partial_i\partial_j
    \mW^{(l)}\Delta_\vx\vz^{(l-1)}
  \end{pmatrix}
     %= \tilde{\mW}^{(l)}\mZ^{(l-1)},
     %\quad \mZ^{(l)} = F(\mZ^{(l-1)})
    \label{eq:forward-laplacian-linear-layer-compact}
\end{align}
\end{comment}
\begin{align}\label{eq:forward_Laplacian_linear}
    \Delta_\vx\vz^{(l)} = \mW^{(l)}\Delta_\vx\vz^{(l-1)}
\end{align}
and for nonlinear layers by
\begin{comment}
    \begin{align}
    \mZ^{(l)}
    =
     \begin{pmatrix}
         \sigma(\vz^{(l-1)}) \\
         \sigma'(\vz^{(l-1)})\odot \partial_{x_1}\vz^{(l-1)} \\
         \vdots \\
         \sigma'(\vz^{(l-1)})\odot \partial_{x_d}\vz^{(l-1)} \\
         \sigma'(\vz^{(l-1)})\odot\Delta_x\vz^{(l-1)} + \sum_{i=1}^d \sigma''(\vz^{(l-1)})\odot \partial_{x_i}{\vz^{(l-1)}}^{\odot 2}
     \end{pmatrix}
     \label{eq:forward-laplacian-nonlinear-layer-compact}
\end{align}
\end{comment}
\begin{align}\label{eq:forward_Laplacian_nonlinear}
    \Delta_\vx\vz^{(l)} = \sigma'(\vz^{(l-1)})\odot\Delta_x\vz^{(l-1)} + \sum_{i=1}^d \sigma''(\vz^{(l-1)})\odot \partial_{x_i}{\vz^{(l-1)}}^{\odot 2}.
\end{align}
This greatly reduces the computational complexity of the forward pass, but is restricted to PDEs that involve second-order derivatives only via the Laplacian. %, i.e., for PDEs of the form $\Psi(u, \partial u, \Delta u)=0$.
%In matrix form, higher-order forward mode automatic differentiation is given by
%or $\mT^f=\tilde{\mW}\mT^h$. \todo{do we need this notation?}
%for linear and nonlinear layers, respectively, where $\tilde{\mW}^{(l)} = \operatorname{diag}(\mW^{(l)}, \dots, \mW^{(l)})$ is a block diagonal matrix.
We can interpret this formula as a forward pass through a larger neural network with weight-sharing.
%in the spirit of \cite{eschenhagen2023kroneckerfactored}
\begin{comment}
    by introducing the vector $\mZ^{(l)}\coloneqq  \begin{pmatrix}
    \vz^{(l)}
    &
    \partial_{\vx_1} \vz^{(l)}
    &
    \dots
    &
    \partial_{\vx_d} \vz^{(l)}
    &
    %\partial_i\partial_j
    \Delta_\vx\vz^{(l)}
\end{pmatrix}\in \mathbb R^{(d+2)h^{(l)}}$.
     with
$\mZ^{(0)}=  \begin{pmatrix}
    \vx
    &
    \ve_1
    &
    \dots
    &
    \ve_d
    &
    0
\end{pmatrix}\in \mathbb R^{(d+2)d}$.
Rearranging $\mZ^{(l)}$ as a matrix of shape $(d+2)\times h^{(l)}$ and yields $\mZ^{(l)} = \mZ^{(l-1)}{\mW^{(l)}}^\top$ for linear layers.
\end{comment}

\paragraph{Forward propagation for linear PDEs}
A similar reduction of the complexity of the forward pass similar to the forward Laplacian can be achieved for other linear PDEs.
Indeed, for a second order linear PDE operator $\mathcal L = \sum_{\lvert \alpha \rvert=2}c_\alpha \partial^\alpha_\vx$, we obtain the forward pass $\mathcal L \vz^{(l)} = \mW^{(l)}\mathcal L \vz^{(l-1)}$ generalizing~\eqref{eq:forward_Laplacian_linear} for linear layers.
Similarly, a generalization of~\eqref{eq:forward_Laplacian_nonlinear} can be given.
%\todo{asdf}


\subsection{A Kronecker-factored approximation for ENGD with the Laplace operator}\label{sec:KFAC-Laplace}
Just like in the case of the classic KFAC algorithm, we only consider the diagonal blocks of the Gauss-Newton matrix, which for the Poisson equation, are given by
\begin{align}
    \mG_\Omega(\mW^{(l)}) = \frac1{{N_\Omega}} \sum_{n=1}^{N_\Omega} \jac_{\mW^{(l)}} \Delta_\vx \vu_n^\top \jac_{\mW^{(l)}} \Delta_\vx \vu_n
\end{align}
Just like in the case of the block Fisher matrices $\mF(\vtheta^{(l)})$ we use the chain rule and compute
\begin{align*}
    \jac_{\mW^{(l)}} \Delta_\vx \vu_n & = \jac_{\mZ^{(l)}}\Delta_\vx \vu_n \jac_{\mW^{(l)}} \mZ^{(l)} %=
    %\jac_{\mW^{(l)}} \mZ^{(l)} \jac_{\mZ^{(l)}}\mZ^{(L)}\jac_{\mZ^{(L)}}\Delta_\vx \vu_n.
    %\\ & =
    %\operatorname{diag}\left({\vz^{(l-1)}}^\top \otimes \mI, \dots, {\Delta_\vx\vz^{(l-1)}}^\top \otimes \mI\right) \jac_{\mZ^{(l)}}\Delta_\vx \vu_n
    %\begin{pmatrix}
    %    \nabla_{\vz^{(l)}} \Delta_{\vx}\vu_n \\
    %    \nabla_{\partial_{\vx_1}\vz^{(l)}}\Delta_{\vx}\vu_n \\
    %    \vdots \\
    %    \nabla_{\partial_{\vx_d}\vz^{(l)}} \Delta_{\vx}\vu_n  \\
    %    \nabla_{\Delta_{\vx}\vz^{(l)}} \Delta_{\vx}\vu_n \\
    %\end{pmatrix}^\top
    %\begin{pmatrix}
    %    {\vz^{(l-1)}_n}^\top \otimes \mI \\
    %    {\partial_{\vx_1}\vz^{(l-1)}_n}^\top \otimes \mI \\
    %    \vdots \\
    %    {\partial_{\vx_d}\vz^{(l-1)}_n}^\top \otimes \mI \\
    %    {\Delta_{\vx}\vz^{(l-1)}_n}^\top \otimes \mI \\
    %\end{pmatrix}
    %\\ &
    %=
    %\sum_{s=1}^S (\nabla_{\mZ^{(l)}_s} \Delta_{\vx}\vu_n)^\top  ({\mZ^{(l-1)}_{n,s}}^\top \otimes \mI)?
    %\\ &
    = \left(\sum_{s=1}^S \mZ^{(l-1)}_{n,s} \otimes \nabla_{\mZ^{(l)}_s} \Delta_{\vx}\vu_n\right)^\top,
    %{\vz^{(l-1)}_n}^\top \otimes \jac_{\vz^{(l)}} \Delta_{\vx}\vu_n
    %+
    %\sum_{i=1}^d {\partial_{\vx_i}\vz^{(l-1)}_n}^\top \otimes \jac_{\partial_{\vx_i}\vz^{(l)}}\Delta_{\vx}\vu_n
    %\\ & \quad
    %+
    %{\Delta_{\vx}\vz^{(l-1)}_n}^\top \otimes \jac_{\Delta_{\vx}\vz^{(l)}} \Delta_{\vx}\vu_n.
    %\operatorname{diag}\left({\vz^{(l-1)}}^\top \otimes \jac_{\vz^{(l-1)}} \Delta_{\vx} \vu_n, \dots, {\Delta_\vx\vz^{(l-1)}}^\top \otimes \jac_{\Delta_\vx\vz^{(l-1)}} \Delta_{\vx} \vu_n\right)? not quite
\end{align*}
where $\mZ_{n, 1}^{(l)} = \vz_n^{(l)}, \mZ_{n, 2}^{(l)} = \partial_{x_1}\vz_n^{(l)}, \dots, \mZ_{n, 1+d}^{(l)} = \partial_{x_d}\vz_n^{(l)}$ and $\mZ_{n, 2+d}^{(l)} = \Delta_x\vz_n^{(l)}$ and $S=d+2$.
Using the notation
$\vg_{n, s}^{(l)} = \nabla_{\mZ_{s}^{(l)}} \Delta_\vx\vu_n$ we obtain
\begin{equation}\label{eq:laplace_gramian_block_exact}
    \mG_\Omega(\mW^{(l)})
    =
    \frac1N\sum_{n=1}^N
    \left[\sum_{s=1}^S \left( \mZ^{(l-1)}_{n,s}\otimes \vg_{n,s}^{(l)} \right)
    \cdot
    \sum_{s=1}^S \left( \mZ
    ^{(l-1)}_{n,s}\otimes \vg_{n,s}^{(l)} \right)^\top\right]
\end{equation}
Now we can use any Kronecker-factored approximation for feedforward networks with weight sharing, where we choose to work with the following.

%
%\paragraph{Kronecker approximation of the PDE Gramian}


%\paragraph{The expand approximation}
Approximating the inner matrix product of sums in equation~\eqref{eq:laplace_gramian_block_exact} by the sum of matrix products, simplifying, and approximating the sum of Kroneckers by the Kronecker of the sum we obtain
\begin{tcolorbox}[colframe=kfac, title={KFAC for ENGD with the Laplace operator},bottom=0mm,top=0mm,middle=0mm]
\begin{align}\label{eq:KFAC-PINN}
    \hat{\mG}_\Omega%^{\textup{exp}}
    (\mW^{(l)})
    \coloneqq \frac{S}{N^2 }
    \left[\sum_{n=1}^N \sum_{s=1}^S \mZ^{(l-1)}_{n,s}{\mZ^{(l-1)}_{n,s}}^\top \right]
    \otimes
    \left[\sum_{n=1}^N\sum_{s=1}^S \vg^{(l)}_{n,s}{\vg^{(l)}_{n,s}}^\top   \right].
\end{align}
\end{tcolorbox}

%\todo{@Felix. Here it would be natural to comment on forward and empirical, right? To my understanding we do not really use the reduce approximation, correct?}

%\paragraph{The reduce approximation}
%Alternatively, approximating the sum of Kronecker products (running over $s$) by the Kronecker product of the sum, simplifying and again approximating for the outer sum (running over $n$) we obtain
%\begin{align}
%    \hat{\mG}_\Omega^{\textup{red}}(\mW^{(l)})
%    \coloneqq
%    \frac{1}{N^2 S^2} \left[\sum_{n=1}^N \left( \sum_{s=1}^S \mZ^{(l-1)}_{n,s} \right) \left(\sum_{s=1}^S {\mZ^{(l-1)}_{n,s}}\right)^\top\right]
%    \otimes
%    \left[\sum_{n=1}^N\left(\sum_{s=1}^S \vg^{(l)}_{n,s} \right) \left( \sum_{s=1}^S {\vg^{(l)}_{n,s}} \right)^\top \right]
%\end{align}
%This approximation coincide with the \emph{reduce} setting described in \cite{eschenhagen2023kroneckerfactored}.




%\paragraph{Empirical and non empirical Gramians}

%One way is to keep both Kronecker approximations separate, but then we need to invert a sum of two Kronecker products.
%This can be done, see \Cref{sec:inverse_kronecker_sum}, but adds twice the memory overhead compared to having a single Kronecker approximation.



% expand approximation treats the shared axis like a batch axis
\begin{comment}
\paragraph{Kronecker approximation under weight sharing}
Now consider a layer whose weight is applied onto \emph{multiple} vectors.
This concept is known as weight sharing.
This could be a linear layer with matrix-valued inputs like in attention, a convolution layer whose kernel is shared between patches of the input, or weights that are used multiple times throughout the computation graph (e.g.\, weight tying).
This means the layer will not process a single vector $\vz^{(l)}$, but a sequence of vectors $\{ \vz^{(l)}_1, \dots, \vz^{(l)}_S \}$ where $S$ denotes weight sharing number.
We can column-stack these vectors
and obtain
\begin{align}
  \begin{pmatrix}
    \vz^{(l)}_1
    &
    \cdots
    &
    \vz^{(l)}_S
  \end{pmatrix}
  &=
    %\begin{pmatrix}
      \mW^{(l)} %& \vb
    %\end{pmatrix}
    \begin{pmatrix}
      \vz^{(l-1)}_1
      &
      \cdots
      &
        \vz^{(l-1)}_S
      %\\
      %1 & 0 & 0
    \end{pmatrix}.
    \label{eq:forward-laplacian-linear-layer-compact}
\end{align}
The parameter Jacobian is consequently given by the
\begin{align}
    \jac_{\mW^{(l)}} \mZ^{(l)}
  &=
    %\begin{pmatrix}
      %\mW^{(l)} %& \vb
    %\end{pmatrix}
    \begin{pmatrix}
      \vz^{(l-1)}_1 \otimes \mI
      &
      \cdots
      &
        \vz^{(l-1)}_S\otimes \mI
      %\\
      %1 & 0 & 0
    \end{pmatrix}.
    \label{eq:forward-laplacian-linear-layer-compact}
\end{align}
Using the notation $\vg_s\coloneqq \nabla_{\vz^{(l)}_s} \vz^{L}_s$ the Gauss-Newton matrix is given by
\begin{align}
    \mF(\mW^{(l)}) = {\jac_{\mW^{(l)}} \mZ^{(L)}}^\top \jac_{\mW^{(l)}} \mZ^{(L)}
\end{align}

into a matrix $\mZ^{(l)} \in \sR^{D_{\text{in}}\times S}$, likewise for the linear layer's outputs $\mZ^{(l+1)} = \mW^{(l)} \mZ^{(l)} \in \sR^{D_{\text{out}}\times S}$ and activation gradients $\mG^{(l)} \in \sR^{D_{\text{out}} \times S}$.\todo{introduce the gradient notation}
The output-weight Jacobian of a weight-sharing layer is $\jac_{\mW^{(l)}} \mZ^{(l+1)} = {\mZ^{(l)}}^{\top} \otimes \mI$ \cite[see e.g.][]{dangel2020modular} and the Fisher does not simplify into a Kronecker product without further approximations.
%As described by \citet{eschenhagen2023kroneckerfactored}, there are two possible Kronecker approximations for this setup.
We will focus on the \emph{expand} approximation, which yields the Kronecker approximation for convolutional layers proposed by~\citet{grosse2016kroneckerfactored}.
It treats the shared axis like a batch axis,
\begin{align}
    \mF(\mW^{(l)}) = \frac1S \sum_{s=1}^S (\vz_s^{(l)} \otimes {\vg_s^{(l)}}^\top)({\vz_s^{(l)}}^{\top}\otimes \vg_s^{(l)}) \approx \frac{1}{S} \sum_{s=1}^S \vz_s \vz_s^{\top} \otimes \sum_{s=1}^S \vg_s \vg_s^{\top}. %, %\quad \text{where } \vg_s = ...%\grad{\vz_s} \ell(\vx, \hat{\vy}, \mW).
\end{align}
We can express this in matrix notation as $\mF(\mW^{(l)}) \approx \nicefrac{1}{S} \mZ^{(l)} {\mZ^{(l)}}^{\top} \otimes \mG^{(l)} {\mG^{(l)}}^{\top}$.
\end{comment}
%\subsection{Discussion and generalizations}

\subsection{KFAC for Gauss-Newton matrices involving general PDE terms} \label{sec:KFAC-general}
%\paragraph{Setting}
We consider the general %second-order
PDE
\begin{equation}
    %\Psi(u, \nabla u, \nabla^{2} u) = 0,
    \Psi(u, \partial u, \dots, \partial^{(k)} u) = 0,
\end{equation}
where $\Psi\colon \mathbb R^{K}\to\mathbb R^M,K=\binom{d+k}{d}$ is smooth.
%We write $\mathcal D u \coloneqq (u, \partial u, \dots, \partial^{(k)} u)$
%Set $v_\vtheta(x)\coloneqq (u_\vtheta(\vx), \nabla u_\vtheta(\vx), \nabla^{2} u_\vtheta(\vx))$
%and
By $r_\vtheta(\vx)\coloneqq \Psi(u(\vx), \partial u(\vx), \dots, \partial^{(k)} u(\vx))%v_\vtheta(x))
$ %we denote the residual and for suitable integration points $\vx_n$ we
we denote the residual and consider the PINN loss
\begin{equation}
    %\frac1N\sum_{n=1}^N \ell(\Psi(\mathcal Du_\vtheta(\vx_n)),
    L(\vtheta)\coloneqq \frac1N\sum_{n=1}^N \ell(r_{\vtheta}(\vx_n)),
\end{equation}
where $\ell\colon\mathbb R^K\to\mathbb R$ is a smooth convex function with definite Hessian $\nabla^2\ell\succ0$ that typically has a unique minimizer at $0$.
We consider the Gauss-Newton matrix
\begin{align}
    \mG(\vtheta) & \coloneqq \frac1N\sum_{n=1}^N \jac_\vtheta r_\vtheta(\vx_n)^\top \mLambda(r_\vtheta(\vx_n)) \jac_\vtheta r_\vtheta(\vx_n),
    %\\ & = \frac1N\sum_{n=1}^N \jac_\vtheta v_\vtheta(\vx_n)^\top \underbrace{\jac \Psi(v_\vtheta(\vx_n))^\top \mLambda(r_\vtheta(\vx_n)) \jac \Psi(v_\vtheta(\vx_n))}_{\eqqcolon \mA(\vtheta, \vx)}  \jac_\vtheta v_\vtheta(\vx_n),
\end{align}
where $\mLambda(r)\coloneqq \nabla^2 \ell(r)$ denotes the Hessian of the per-sample loss. %~\citep{eschenhagen2023kroneckerfrwtactored} yxxx
%and $v_\vtheta$ is a forward network with shared weights.

Typically, in PINNs, one chooses $\ell = \frac12\lVert \cdot \rVert_2^2$ to be the squared Euclidean distance, such that $\mLambda = \mI$.
Note, however, that in contrast to a regression problem, the residual $r_\vtheta$ involves PDE terms and not only function evaluations.
If we have a forward iteration for $r_\vtheta$, we can apply existing KFAC approximations.
We obtain this via higher-order forward mode automatic differentiation.

Writing $v_\vtheta(\vx)\coloneqq (%u_\vtheta(\vx), \nabla u_\vtheta(\vx), \nabla^{2} u_\vtheta(\vx)
u(\vx), \partial u(\vx), \dots, \partial^{(k)} u(\vx))$
we can express the block corresponding to the $l$-th layer of the Gauss-Newton matrix as
\begin{align}
    \mG(\mW^{(l)}) & = \frac1N\sum_{n=1}^N \jac_{\mW^{(l)}} v_\vtheta(\vx_n)^\top \underbrace{\jac \Psi(v_\vtheta(\vx_n))^\top \mLambda(r_\vtheta(\vx_n)) \jac \Psi(v_\vtheta(\vx_n))}_{\eqqcolon \mA_n}  \jac_{\mW^{(l)}} v_\vtheta(\vx_n),
\end{align}
and $v_\vtheta$ is a forward network with shared weights.
%The reduce approximation is then given by
%\begin{align}
%    \hat{\mG}(\mW^{(l)}) \coloneqq \left(\sum_{n=1}^N \left( \sum_{s=1}^S \vz^{(l)}_{n,s} \right) \left(\sum_{s=1}^S {\vz^{(l)}_{n,s}}^\top\right)\right)\otimes\left(\sum_{n=1}^N\left(\sum_{s=1}^S \vg^{(l)}_{n,s} \right) \mH(\vtheta, \vx_n) \left( \sum_{s=1}^S {\vg^{(l)}_{n,s}}^\top \right) \right)?
%\end{align}

Note that the forward pass in~\eqref{eq:forward_pass} extends to higher-order partial derivatives
\begin{align}
    \partial_\vx^\alpha \vz^{(l)} = \mW^{(l)}\partial_\vx^\alpha \vz^{(l-1)}
\end{align}
and hence, we can perceive the computational graph of $v_\vtheta$ as a feedforward network with weight sharing over the multi-indices $\alpha\in\mathbb N$ with $\lvert \alpha \rvert \le k$.
The generalization of%the approximation
~\eqref{eq:KFAC-PINN} is given by
\begin{tcolorbox}[colframe=kfac, title={KFAC with general PDE terms},bottom=0mm,top=0mm,middle=0mm]
\begin{align}\label{eq:KFAC-PINNs-general}
    \hat{\mG}%^{\textup{exp}}
    (\mW^{(l)})
    \coloneqq \frac{\binom{d+k}{d}}{N^2}
    \left[\sum_{n=1}^N \sum_{\lvert \alpha \rvert \le k} %\mZ^{(l-1)}_{n,\alpha}{\mZ^{(l-1)}_{n,\alpha}}^\top
    \partial_\vx^\alpha \vz_n^{(l-1)} {\partial_\vx^\alpha \vz_n^{(l-1)}}^\top\right]
    \otimes
    \left[\sum_{n=1}^N\sum_{\lvert \alpha \rvert \le k} {\mJ^{(l)}_{n,\alpha}}^\top \mA_n \mJ^{(l)}_{n,\alpha} \right],
\end{align}
\end{tcolorbox}
%where $\mZ^{(l)}_{n,\alpha} = \partial_\vx^\alpha \vz_n^{(l)}$ and
where $\mJ^{(l)}_{n,\alpha} = %\nabla
\jac_{\partial_\vx^\alpha \vz_n^{(l)}} \vv_n$ and $\vv_n = v_\vtheta(\vx_n) %= (\partial^\beta_\vx \vz_{n}^{L})_{\lvert \beta \rvert \le k}
$.
A detailed derivation can be found in \Cref{app:derivations}.
%
This approximation can be stated for a large class of weight-sharing architectures in which case it generalizes the \emph{expand approximation} by~\citet{eschenhagen2023kroneckerfactored}.
%\jm{Note that other Kronecker-factored approximations for weight-sharing architectures have been proposed, but due to the superior performance in our experiments, we focus on the expand approximation?}

%\todo[inline]{striking remark how that this can be applied for any Gramian involving PDE terms}
%There are some challenges we need to overcome to define a Kronecker-factored approximation of the Gramian from \Cref{eq:gramian}:
%\begin{itemize}
%\item The Gramian of the interior loss involves the parameter gradient of the Laplacian. To establish a Kronecker approximation, we need to know how the weight matrix of a layer enters the computation of the Laplacian. We will show that, when using the forward Laplacian framework from \cite{li2023forward}, the weight matrix enters the computation by multiplication onto another matrix. This is great, because we know how to define KFAC in such cases, thanks to the KFAC-for-weight-sharing framework developed by \citet{eschenhagen2023kroneckerfactored}.

%\item %The PINN loss consists of two terms, the interior and boundary loss.
  %We can develop a Kronecker approximation for both individually, which leaves us with the problem how to work with both terms to pre-condition a gradient.
  %One way is to keep the both Kronecker approximations separate, but then we need to invert a sum of two Kronecker products.
  %This can be done, see \Cref{sec:inverse_kronecker_sum}, but adds twice the memory overhead compared to having a single Kronecker approximation.
  %Also, the Kronecker sum's inversion requires solving a generalized eigenvalue problem, for which there is currently no API in PyTorch.
  %Hence we need to fall back to SciPy, which costs communication overhead because everything needs to be off-loaded to CPU.
  %Alternatively, we could summarize the two Kronecker approximations into a single one at the risk of losing downstream performance.
%\end{itemize}

\paragraph{KFAC for variational problems}
Our proposed KFAC approximation is not limited to PINNs and can be used for variational problems of the form
\begin{align}
    \min_u \int_\Omega \ell(u, \partial u, \dots, \partial^{(k)} u) \mathrm{d}x,
\end{align}
where $\ell\colon\mathbb R^K\to\mathbb R$ is a convex function.
%The discretization of this is given by
%\begin{align}
%    L(\vtheta) = \frac1N \sum_{n=1}^N \ell(u(\vx_n), \partial u(\vx_n), \dots, \partial^{(k)}u(\vx_n)),
%\end{align}
%which covers general PINNs but also variational problems.
%Then the Gauss-Newton matrix is given by
We can perceive this problem as a special case of the setting described above with $\Psi = \operatorname{id}$ and hence the KFAC approximation~\eqref{eq:KFAC-PINNs-general} remains meaningful %approximation of the discretized Gauss-Newton matrix
if $\mA_n$ is replaced by $\mA_n = \nabla^2\ell(u(\vx_n), \partial u(\vx_n), \dots, \partial^{(k)}u(\vx_n))$.
In particular, our approximation can be used for the \emph{deep Ritz method} and other variational approaches to solve PDEs~\citep{yu2018deep}.

\begin{comment}
    \subsection{Computational complexity}
\todo[inline]{what can we say? probably better to move to the respective approximations}
\begin{itemize}
    \item GN: $O(p^3+Np^2)$
    \item BFGS: $O(p^2+?)$
    \item L-BFGS: $O(mp+?)$?
    \item KFAC: $O(?)$
    \item
\end{itemize}
\end{comment}

\subsection{Algorithmic Details}

We provide the additional components that form our KFAC-based optimization algorithms, which are influenced by ENGD and the original KFAC algorithm.
See \Cref{app:pseudo} for pseudo-code.

\paragraph{Exponential moving averages and damping} %Given a combination of interior and condition loss, $\gL_{\Omega},\gL_{\partial\Omega}$ we
First, we approximate the interior and boundary Gramians at iteration $t$  according to~\eqref{eq:KFAC-PINN} and~\eqref{eq:KFAC-PINNs-general} and the classic KFAC approximation, respectively, giving
$\mG_{\Omega,t}^{(l)} \approx \mA_{\Omega,t}^{(l)}\otimes \mB_{\Omega,t}^{(l)}$ and $\mG_{\partial \Omega, t}^{(l)}\approx \mA_{\Omega,t}^{(l)}\otimes \mB_{\Omega,t}^{(l)}$ .
Here, we replace $\mA^{(l)}_{\bullet,t}$ with an exponential moving average $\hat{\mA}^{(l)}_{\bullet,t} = \beta \hat{\mA}^{(l)}_{\bullet,t-1} + (1 - \beta) \mA^{(l)}_{\bullet,t}$
and identically for $\hat{\mB}^{(l)}_{\bullet, t}$ for some $\beta\in[0,1)$.
Further, we add constant damping of strength $\lambda>0$ to all Kronecker factors, obtaining $\tilde{\mA}_{\bullet,t} = \hat{\mA}^{(l)}_{\bullet,t} + \lambda \mI$ and $\tilde{\mB}_{\bullet,t} = \hat{\mB}^{(l)}_{\bullet,t} + \lambda \mI$
leading to the
 block-diagonal Kronecker-factored approximation
\begin{align*}
  \mG_{\bullet, t}
  &\approx
    \blockdiag
    \left(
    \tilde{\mA}^{(1)}_{\bullet,t} \otimes \tilde{\mB}_{\bullet,t}^{(1)},
    \dots,
    \tilde{\mA}^{(L)}_{\bullet,t} \otimes \tilde{\mB}_{\bullet,t}^{(L)}
    \right)
    \qquad \bullet \in \{ \Omega, \partial\Omega\}\,.
\end{align*}
% over all previous Kronecker factors, i.e.\, as defined in ?? and identically for $\hat{\mB}^{(l)}_{\bullet, t}$.

\paragraph{Gradient pre-conditioning% and damping
}
Given the current mini-batch gradient $\vg^{(l)}$ for layer $l$, we obtain the update direction $\vDelta_t^{(l)} = -(\tilde{\mA}_{\Omega} \otimes \tilde{\mB}_{\Omega} + \tilde{\mA}_{\partial\Omega} \otimes \tilde{\mB}_{\partial\Omega})^{-1} \vg^{(l)}$ by
pre-condition with our Kronecker-factored approximate Gramian.
%First, we add constant damping of strength $\lambda>0$ to all Kronecker factors, obtaining $\tilde{\mA}_{\bullet,t} = \hat{\mA}^{(l)}_{\bullet,t} + \lambda \mI$ and $\tilde{\mB}_{\bullet,t} = \hat{\mB}^{(l)}_{\bullet,t} + \lambda \mI$.
%Then, we multiply the inverse of the sum of Kronecker products onto the gradient to obtain the update direction
This can be done without building up the full matrix utilizing a generalized eigendecomposition of the individual Kronecker factors, where we use the procedure described in \cite[Appendix I]{martens2015optimizing}.

%\paragraph{Combining PDE and boundary approximations}
%The boundary Gramian matrix $\mG_{\partial\Omega}(\mW^{(l)})$ is a classic Gauss-Newton matrix and thus we can use a classic KFAC approximation $\hat{\mG}_{\partial\Omega}(\mW^{(l)})$.
%To invert the sum $\hat{\mG}_\Omega(\mW^{(l)}) + \hat{\mG}_{\partial\Omega}(\mW^{(l)})$ of the two Kronecker approximations we use the procedure described in \cite[Appendix I]{martens2015optimizing}.

\paragraph{Learning rate and momentum}
We consider two different updates $ \vtheta_{t+1} = \vtheta_t + \vdelta_{t}$ at iteration $t$ from the pre-conditioned gradient $\vDelta_t$, which we call as `KFAC' and `KFAC*'.
KFAC uses momentum %buffer of all previous updates,
$\hat{\vdelta}_t = \mu \vdelta_{t-1} + \vDelta_t$ %(similar to momentum in SGD).
%The
where the parameter $\mu$ can be chosen by the practitioner.
Like in ENGD, we use a logarithmic grid line search along
%$\hat{\vdelta}_t$, i.e.\,
%choosing
leading to the update
$\vdelta_t = \alpha_{\star} \hat{\vdelta}_t$ where
$\alpha_{\star} = \argmin_{\alpha} \gL(\vtheta_t + \alpha \hat{\vdelta}_t)$ where $\alpha \in \{2^{-30}, \dots, 2^0\}$.
KFAC* uses the learning rate and momentum heuristic proposed for the original KFAC optimizer~\citep{martens2015optimizing}, see also~\Cref{sec:experimental_details}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
