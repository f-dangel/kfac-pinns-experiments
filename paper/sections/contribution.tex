There are some challenges we need to overcome to define a Kronecker-factored approximation of the Gramian from \Cref{eq:gramian}:
\begin{itemize}
\item The Gramian of the interior loss involves the parameter gradient of the Laplacian. To establish a Kronecker approximation, we need to know how the weight matrix of a layer enters the computation of the Laplacian. We will show that, when using the forward Laplacian framework from \cite{li2023forward}, the weight matrix enters the computation by multiplication onto another matrix. This is great, because we know how to define KFAC in such cases, thanks to the KFAC-for-weight-sharing framework developed by \citet{eschenhagen2023kroneckerfactored}.

\item The PINN loss consists of two terms, the interior and boundary loss.
  We can develop a Kronecker approximation for both individually, which leaves us with the problem how to work with both terms to pre-condition a gradient.
  One way is to keep the both Kronecker approximations separate, but then we need to invert a sum of two Kronecker products.
  This can be done, see \Cref{sec:inverse_kronecker_sum}, but adds twice the memory overhead compared to having a single Kronecker approximation.
  Also, the Kronecker sum's inversion requires solving a generalized eigenvalue problem, for which there is currently no API in PyTorch.
  Hence we need to fall back to SciPy, which costs communication overhead because everything needs to be off-loaded to CPU.
  Alternatively, we could summarize the two Kronecker approximations into a single one at the risk of losing downstream performance.
\end{itemize}

Here, we introduce a Kronecker-factored approximation of the Gauß-Newton algorithm for PINNs. 
To this end we consider a general PDE of the general form 
\begin{equation}
    \Psi(u, \partial u, \dots, \partial^{(k)} u) = 0,
\end{equation}
where we assume $\Psi\colon \mathbb R^K\to\mathbb R^M$ to be a smooth function.
%We write $\mathcal D u \coloneqq (u, \partial u, \dots, \partial^{(k)} u)$
By $r_\theta\coloneqq \Psi(u_\vtheta, \dots, \partial^{(k)} u_\vtheta)$ we denote the residual and for suitable integration points $\vx_n$ we consider the PINN loss
\begin{equation}
    %\frac1N\sum_{n=1}^N \ell(\Psi(\mathcal Du_\vtheta(\vx_n)),
    L(\vtheta)\coloneqq \frac1N\sum_{n=1}^N \ell(r_{\vtheta}(\vx_n)),
\end{equation}
where we assume $\ell\colon\mathbb R^M\to\mathbb R$ is a convex function with definite Hessian $\nabla^2\ell\succ0$ and unique minimizer at $0$. 
We consider the Gauß-Newton matrix
\begin{equation}
    \mG(\vtheta) \coloneqq \frac1N\sum_{n=1}^N \jac_\vtheta r_\vtheta(\vx_n)^\top \mLambda(r_\vtheta(\vx_n)) \jac_\vtheta r_\vtheta(\vx_n),
\end{equation}
where $\mLambda(r)\coloneqq \nabla^2 \ell(r)$ is the Hessian matrix, see~\cite{eschenhagen2023kroneckerfactored}. 
Typically, in PINNs, one chooses $\ell = \frac12\lVert \cdot \rVert_2^2$ to be the squared Euclidean distance, such that $\mLambda = I$. 
Note, however, that in contrast to a regression problem, the residual $r_\vtheta$ involves PDE terms and not only function evaluations. 
If we have a forward iteration for $r_\vtheta$, we can apply existing KFAC approximations. 
We obtain this via higher-order forward mode automatic differentiation. 

\subsection{Higher-order forward mode automatic differentiation}
\label{sec:taylor-mode-AD}

Here, we review higher-order forward mode differentiation, also known as \emph{Taylor-mode automatic differentiation}~\citep{griewank1996algorithm, griewank2008evaluating, bettencourt2019taylor}. 
Many PDEs only incorporate first and second partial derivatives and we focus our discussion here on second-order automatic differentiation for the sake of simplicity of the presentation. 
However, a completely analogous approach can be taken for higher-order PDEs. 

How to compute the function values through a forward pass is well known. 
The computation of the gradient through forward mode differentiation uses the chain rule $\jac (f\circ g) = \jac f (g) \jac g$. 
If we want to compute the Hessian in a forward-mode style, we use the \emph{Hessian chain rule}, which is given by 
\begin{align} 
       %\nabla^2 (f\circ g) & = (\jac g)^\top\cdot \nabla^2 f(g) \cdot \jac g + \sum_{k} \partial_{k} f(g) \cdot \nabla^2 g_k \\ 
       \partial_i \partial_j (f \circ g)(\vz) & = \partial_i g(\vz)^\top D^2f(g(\vz))\partial_j g(\vz) + Df(g(\vz)) \partial_i \partial_j g(\vz). 
\end{align}
If $f_{\vtheta^{(l)}}(\vz) = \mW^{(l)} \vz + \vb^{(l)}$ is a linear layer, then the second order forward pass is given by 
\begin{subequations}
    \begin{align}
    \vz^{(l)} & = \mW^{(l)} \vz^{(l-1)} + \vb^{(l)} \\ 
    \partial_{i} \vz^{(l)} & = \mW^{(l)} \partial_i \vz^{(l-1)} \\ 
    \partial_i\partial_j \vz^{(l)} & = \mW^{(l)} \partial_i\partial_j \vz^{(l-1)} \label{subeq:secondOrderForward-LinearLayer}. 
\end{align}
\end{subequations}
The second-order forward pass through a nonlinear layer $\vz\mapsto \sigma(\vz)$ is given by  
\begin{subequations}
    \begin{align}
    \vz^{(l)} & = \sigma(\vz^{(l-1)}) \\ 
    \partial_{i} \vz^{(l)} & = \sigma'(\vz^{(l-1)}) \odot \partial_i \vz^{(l-1)} \\ 
    \partial_i\partial_j \vz^{(l)} & = \partial_i \vz^{(l-1)} \odot \sigma''(\vz^{(l-1)}) \odot \partial_j \vz^{(l-1)} + \sigma'(\vz^{(l-1)}) \odot \partial_i \partial_j \vz^{(l-1)} \label{subeq:secondOrderForward-nonlinearLayer}. 
\end{align}
\end{subequations}


\paragraph{Forward Laplacian} 
To compute not the full Hessian but only the Laplacian, we can simplify the forward pass to only propagate the Laplacian which is known as \emph{forward Laplacian}~\citep{li2023forward}. 
We obtain the Laplacian forward pass by summing~\eqref{subeq:secondOrderForward-LinearLayer} and~\eqref{subeq:secondOrderForward-nonlinearLayer} over $i=j$ which yields 
\begin{subequations}
    \begin{align}
        \Delta \vz^{(l)} & = \mW^{(l)} \Delta \vz^{(l-1)} \quad \text{and } \\ 
        \Delta \vz^{(l)} & = \sum_i \sigma''(\vz^{(l-1)}) \odot (\partial_i \vz^{(l-1)})^{\odot 2} + \sigma'(\vz^{(l-1)}) \odot \Delta \vz^{(l-1)}
    \end{align}
\end{subequations}
for linear and nonlinear layers, respectively. 
This greatly reduces the computational complexity of the forward pass, but can only be used for PDEs that only involve second-order derivatives via the Laplacian, i.e., for PDEs of the form $\Psi(u, \partial u, \Delta u)=0$. 

\subsection{Kronecker-factored approximate Gauß-Newton}

In matrix form: 
\begin{align}
  \nonumber
  \underbrace{
  \begin{pmatrix}
    \vz^{(l)}
    &
    \partial_{i} \vz^{(l)}
    &
    \partial_i\partial_j \vz^{(l)}
  \end{pmatrix}
  }_{\coloneq \mT^f \in \sR^{D_{\text{out}} \times (D+2)}}
  &=
    \begin{pmatrix}
      \mW & \vb
    \end{pmatrix}
    \underbrace{
    \begin{pmatrix}
      \vz^{(l)}
      &
      \partial_{i} \vz^{(l)}
      &
        \partial_i\partial_j \vz^{(l)}
      \\
      1 & 0 & 0
    \end{pmatrix}
    }_{\coloneq \mT^h \in \sR^{(D_{\text{in}} +1) \times (D+2)}}\,,
    %\shortintertext{or, in compact form,}
    %\mT^f
  %&=
    %\tilde{\mW}
    %\mT^h\,.
    \label{eq:forward-laplacian-linear-layer-compact}
\end{align}
or $\mT^f=\tilde{\mW}\mT^h$. 

\paragraph{KFAC with weight sharing}

\paragraph{Combining PDE and boundary Gramian}

\paragraph{Higher-order PDEs}

\paragraph{Input-based Kronecker approximation:} One downside of our approach is that we need to store the full computation graph of the forward Laplacian framework to be able to compute the grad-output-based Kronecker factors.
Some works find that if setting this factor to $\mI$, the resulting input-based KFAC still yields good performance~\cite{benzing2022gradient,petersen2023isaac}

\paragraph{Possible extensions:} Our KFAC implementation requires matrix (eigen-)decompositions and uses dense pre-conditioner matrices.
This can cause numerical instabilities and large memory consumption.
Both issues can be addressed by using an inverse-free KFAC update~\cite{lin2023simplifying} and structured Kronecker factors~\cite{lin2023structured}.
We leave this additions to future work.
One could also merge the backward pass for each Gramian with that of its loss into a single backward traversal rather than two sequential ones, e.g.\,as done in \cite{dangel2020backpack}.
However, then one needs to manually implement the additional backpropagation (through both the normal forward pass, but also through the forward Laplacian pass).

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
