\subsection{Hyper-Parameter Tuning Protocol}\label{sec:tuning-protocol}

In all our experiments, we tune the following optimizer hyper-parameters and otherwise use the PyTorch default values:
\begin{itemize}
\item \textbf{SGD:} learning rate, momentum
\item \textbf{Adam:} learning rate
\item \textbf{Hessian-free:} type of curvature matrix (Hessian or GGN), damping, whether to adapt damping over time (yes or no), maximum number of CG iterations
\item \textbf{LBFGS:} learning rate, history size
\item \textbf{ENGD:} damping, factor of the exponential moving average applied to the Gramian, initialization of the Gramian (zero or identity matrix)
\item \textbf{KFAC:} factor of the exponential moving average applied to the Kronecker factors, damping, momentum, initialization of the Kronecker factors (zero or identity matrix)
\item \textbf{KFAC*:} factor of the exponential moving average applied to the Kronecker factors, damping, initialization of the Kronecker factors (zero or identity matrix)
\end{itemize}

Depending on the optimizer and experiment we use grid, random, or Bayesian search from Weights \& Biases.
Each individual run is executed in double precision and allowed to run for a given time budget, and the best run within a search is given by that with lowest final $L_2$ error on a fixed evaluation data set.
All runs are executed on RTX 6000 GPUs with 24\,GiB of RAM to be comparable.
For grid and random searches, we use a round-based approach.
First, we choose a relatively wide search space and limit to approximately 50 runs.
In a second, round, we narrow down the hyper-parameter space based on the first round, then re-run for another approximately 50 runs.
We will release the details of all hyper-parameter search spaces, as well as the hyper-parameters for the best runs in our implementation.

\subsection{2d Poisson Equation}

\paragraph{Setup} We consider a two-dimensional Poisson equation $-\Delta u(x, y) = 2 \pi^2 \sin(\pi x) \sin(\pi y)$ on the unit square $(x,y) \in [0, 1]^2$ with sine product right hand side and zero boundary conditions $u(x, y) = 0$ for $(x,y) \in \partial [0,1]^2$.
We choose a single set of training points with $N_{\Omega} = 900, N_{\partial\Omega} = 120$.
The $L_2$ error is evaluated on a separate set of $\num{9000}$ data points using the known solution $u_{\star}(x, y) = \sin(x) \sin(y)$.
Each run is limited to 1000\,s.
We compare three MLP architectures of increasing size, each of whose linear layers are Tanh-activated except for the final one: a shallow $2\to 64\to 1$ MLP with $D=257$, a five layer $2 \to 64 \to 64 \to 48 \to 48 \to 1$ MLP with $D=\num{9873}$, and a five layer $2 \to 256 \to 256\to 128 \to 128 \to 1$ MLP with $D=\num{116097}$.
For the biggest architecture, full and per-layer ENGD lead to out-of-memory errors.
\Cref{fig:poisson2d-appendix} visualizes the results.

\begin{figure}[!h]
  \centering
  \def\pathToFigs{../kfac_pinns_exp/exp17_groupplot_poisson2d}
  \begin{subfigure}[t]{1.0\linewidth}
    \caption{}\label{subfig:poisson2d-time}
    % trim legend, xlabel and xticklabels
    % [trim={left bottom right top},clip]
    \includegraphics[trim={0 1.3cm 0 0},clip]{\pathToFigs/l2_error_over_time.pdf}
    % trim the legend and titles
    % [trim={left bottom right top},clip]
    \includegraphics[trim={0 0.5cm 0 0.3cm},clip]{\pathToFigs/loss_over_time.pdf}
  \end{subfigure}
  \begin{subfigure}[t]{1.0\linewidth}
    \caption{}\label{subfig:poisson2d-step}
    % trim the legend, xlabel and xticklabels
    % [trim={left bottom right top},clip]
    \includegraphics[trim={0 1.3cm 0 0.3cm},clip]{\pathToFigs/l2_error_over_step.pdf}
    % trim the titles
    \includegraphics[trim={0 0 0 0.3cm},clip]{\pathToFigs/loss_over_step.pdf}
  \end{subfigure}
  \caption{ Training loss and evaluation $L_2$ error for learning the solution to a 2d Poisson equation over (\subref{subfig:poisson2d-time}) time and (\subref{subfig:poisson2d-step}) steps.
    Columns are different neural networks.}\label{fig:poisson2d-appendix}
\end{figure}

\paragraph{Best run details}
The runs shown in \Cref{fig:poisson2d-appendix} correspond to the following hyper-parameters:
\begin{itemize}
\item $2\to 64\to 1$ MLP with $D=257$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp09_reproduce_poisson2d/tex}
  \item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
  \item \textbf{ENGD (full):} \input{\pathToRuns/best_ENGD_full.tex}
  \item \textbf{ENGD (layer-wise):} \input{\pathToRuns/best_ENGD_per_layer.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
  \end{itemize}

\item $2 \to 64 \to 64 \to 48 \to 48 \to 1$ MLP with $D=\num{9873}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp15_poisson2d_deepwide/tex}
  \item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
  \item \textbf{ENGD (full):} \input{\pathToRuns/best_ENGD_full.tex}
  \item \textbf{ENGD (layer-wise):} \input{\pathToRuns/best_ENGD_per_layer.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
  \end{itemize}

\item $2 \to 256 \to 256\to 128 \to 128 \to 1$ MLP with $D=\num{116097}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp20_poisson2d_mlp_tanh_256/tex}
  \item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
  \end{itemize}
\end{itemize}

\paragraph{Search space details} The runs shown in \Cref{fig:poisson2d-appendix} were determined to be the best via a search with approximately 50 runs on the following search spaces which were obtained by refining an initially wider search ($\mathcal{C}$ denotes a categorical, $\mathcal{U}$ a uniform, and $\mathcal{LU}$ a log-uniform distribution):
\begin{itemize}
\item $2\to 64\to 1$ MLP with $D=257$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp09_reproduce_poisson2d/tex}
  \item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
  \item \textbf{ENGD (full):} \input{\pathToRuns/sweep_ENGD_full.tex}
  \item \textbf{ENGD (layer-wise):} \input{\pathToRuns/sweep_ENGD_per_layer.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
  \end{itemize}

\item $2 \to 64 \to 64 \to 48 \to 48 \to 1$ MLP with $D=\num{9873}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp15_poisson2d_deepwide/tex}
  \item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
  \item \textbf{ENGD (full):} \input{\pathToRuns/sweep_ENGD_full.tex}
  \item \textbf{ENGD (layer-wise):} \input{\pathToRuns/sweep_ENGD_per_layer.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
  \end{itemize}

\item $2 \to 256 \to 256\to 128 \to 128 \to 1$ MLP with $D=\num{116097}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp20_poisson2d_mlp_tanh_256/tex}
  \item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
  \end{itemize}
\end{itemize}

\subsection{5d Poisson Equation}

\paragraph{Setup} We consider a five-dimensional Poisson equation $-\Delta u(\vx) = \pi^2 \sum_{i=1}^5 \cos(\pi \evx_i)$ on the five-dimensional unit square $\vx \in [0, 1]^5$ with cosine sum right hand side and boundary conditions $u(\vx) = \sum_{i=1}^5 \cos(\pi \evx_i)$ for $\vx \in \partial [0,1]^5$.
We sample training batches of size $N_{\Omega} = \num{3000}, N_{\partial\Omega} = 500$ and evaluate the $L_2$ error on a separate set of $\num{30000}$ data points using the known solution $u_{\star}(\vx) = \sum_{i=1}^5 \cos(\pi \evx_i)$.
All optimizers except for KFAC sample a new training batch each iteration.
KFAC only re-samples every 100 iterations because we noticed it works significantly better with multiple iterations on a fixed batch.
To make sure that this does not lead to an unfair advantage of KFAC, we conduct an additional experiment where we also tune the batch sampling frequency, as well as other hyper-parameters; see \Cref{sec:high-dimensional-poissons-app}.
The results presented in this section are consistent with this additional experiment (compare the rightmost column of \Cref{fig:poisson5d-appendix} and the leftmost column of \Cref{fig:poisson-bayes-appendix}).
Each run is limited to 3000\,s.
We compare three MLP architectures of increasing size, each of whose linear layers are Tanh-activated except for the final one: a shallow $5\to 64\to 1$ MLP with $D=449$, a five layer $5 \to 64 \to 64 \to 48 \to 48 \to 1$ MLP with $D=\num{10065}$, and a five layer $5 \to 256 \to 256\to 128 \to 128 \to 1$ MLP with $D=\num{116864}$.
For the biggest architecture, full and layer-wise ENGD lead to out-of-memory errors.
\Cref{fig:poisson5d-appendix} visualizes the results.

\begin{figure}[!h]
  \centering
  \def\pathToFigs{../kfac_pinns_exp/exp18_groupplot_poisson5d}
  \begin{subfigure}[t]{1.0\linewidth}
    \caption{}\label{subfig:poisson5d-time}
    % trim legend, xlabel and xticklabels
    % [trim={left bottom right top},clip]
    \includegraphics[trim={0 1.3cm 0 0},clip]{\pathToFigs/l2_error_over_time.pdf}
    % trim the legend and titles
    % [trim={left bottom right top},clip]
    \includegraphics[trim={0 0.5cm 0 0.3cm},clip]{\pathToFigs/loss_over_time.pdf}
  \end{subfigure}
  \begin{subfigure}[t]{1.0\linewidth}
    \caption{}\label{subfig:poisson5d-step}
    % trim the legend, xlabel and xticklabels
    % [trim={left bottom right top},clip]
    \includegraphics[trim={0 1.3cm 0 0.3cm},clip]{\pathToFigs/l2_error_over_step.pdf}
    % trim the titles
    \includegraphics[trim={0 0 0 0.3cm},clip]{\pathToFigs/loss_over_step.pdf}
  \end{subfigure}
  \caption{Training loss and evaluation $L_2$ error for learning the solution to a 5d Poisson equation over (\subref{subfig:poisson5d-time}) time and (\subref{subfig:poisson5d-step}) steps.
    Columns are different neural networks.}\label{fig:poisson5d-appendix}
\end{figure}

\paragraph{Best run details}
The runs shown in \Cref{fig:poisson5d-appendix} correspond to the following hyper-parameters:
\begin{itemize}
\item $5\to 64\to 1$ MLP with $D=449$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp10_reproduce_poisson5d/tex}
  \item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
  \item \textbf{ENGD (full):} \input{\pathToRuns/best_ENGD_full.tex}
  \item \textbf{ENGD (layer-wise):} \input{\pathToRuns/best_ENGD_per_layer.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
  \end{itemize}

\item $5 \to 64 \to 64 \to 48 \to 48 \to 1$ MLP with $D=\num{10065}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp16_poisson5d_deepwide/tex}
  \item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
  \item \textbf{ENGD (full):} \input{\pathToRuns/best_ENGD_full.tex}
  \item \textbf{ENGD (layer-wise):} \input{\pathToRuns/best_ENGD_per_layer.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
  \end{itemize}

\item $5 \to 256 \to 256\to 128 \to 128 \to 1$ MLP with $D=\num{116865}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp19_poisson5d_mlp_tanh_256/tex}
  \item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
  \end{itemize}
\end{itemize}

\paragraph{Search space details} The runs shown in \Cref{fig:poisson5d-appendix} were determined to be the best via a search with approximately 50 runs on the following search spaces which were obtained by refining an initially wider search ($\mathcal{C}$ denotes a categorical, $\mathcal{U}$ a uniform, and $\mathcal{LU}$ a log-uniform distribution):
\begin{itemize}
\item $5\to 64\to 1$ MLP with $D=449$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp10_reproduce_poisson5d/tex}
  \item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
  \item \textbf{ENGD (full):} \input{\pathToRuns/sweep_ENGD_full.tex}
  \item \textbf{ENGD (layer-wise):} \input{\pathToRuns/sweep_ENGD_per_layer.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
  \end{itemize}

\item $5 \to 64 \to 64 \to 48 \to 48 \to 1$ MLP with $D=\num{10065}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp16_poisson5d_deepwide/tex}
  \item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
  \item \textbf{ENGD (full):} \input{\pathToRuns/sweep_ENGD_full.tex}
  \item \textbf{ENGD (layer-wise):} \input{\pathToRuns/sweep_ENGD_per_layer.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
  \end{itemize}

\item $5 \to 256 \to 256\to 128 \to 128 \to 1$ MLP with $D=\num{116865}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp19_poisson5d_mlp_tanh_256/tex}
  \item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
  \end{itemize}
\end{itemize}

\subsection{5/10/100-d Poisson Equations with Bayesian Search}\label{sec:high-dimensional-poissons-app}

\paragraph{Setup} Here, we consider three Poisson equations $- \Delta u(\vx) = f(\vx)$ with different right hand sides and boundary conditions on the unit square $\vx \in [0, 1]^d$:
\begin{itemize}
\item $d=5$ with cosine sum right hand side $f(\vx) = \pi^2 \sum_{i=1}^d \cos(\pi \evx_i)$, boundary conditions $u(\vx) = \sum_{i=1}^d \cos(\pi \evx_i)$ for $\vx \in \partial [0,1]^d$, and known solution $u_{\star}(\vx) = \sum_{i=1}^d \cos(\pi \evx_i)$.
  We assign each run a budget of $\num{3000}\,\text{s}$.

\item $d=10$ with zero right hand side $f(\vx) = 0$, harmonic mixed second order polynomial boundary conditions $u(\vx) = \sum_{i=1}^{\nicefrac{d}{2}} \evx_{2i-1} \evx_{2i}$ for $\vx \in \partial [0,1]^d$, and known solution $u_{\star}(\vx) =  \sum_{i=1}^{\nicefrac{d}{2}} \evx_{2i-1} \evx_{2i}$.
  We assign each run a budget of $\num{6000}\,\text{s}$.

\item $d=100$ with constant non-zero right hand side $f(\vx) = -2 d$, square norm boundary conditions $u(\vx) = \left\lVert \vx \right\rVert_2^2$ for $\vx \in \partial [0,1]^d$, and known solution $u_{\star}(\vx) =  \left\lVert \vx \right\rVert_2^2$.
We assign each run a budget of $\num{10000}\,\text{s}$.
\end{itemize}
We tune the optimizer-hyperparameters described in \Cref{sec:tuning-protocol}, as well as the batch sizes $N_{\Omega}, N_{\partial \Omega}$, and their associated re-sampling frequencies using Bayesian search.
We use five layer MLP architectures with varying widths whose layers are Tanh-activated except for the final layer.
These architectures are too large to be optimized by ENGD.
\Cref{fig:poisson-bayes-appendix} visualizes the results.

\begin{figure}[!h]
  \centering
  \def\pathToFigs{../kfac_pinns_exp/exp33_poisson_bayes_groupplot}
  \begin{subfigure}[t]{1.0\linewidth}
    \caption{}\label{subfig:poisson-bayes-time}
    % trim legend, xlabel and xticklabels
    % [trim={left bottom right top},clip]
    \includegraphics[trim={0 1.3cm 0 0},clip]{\pathToFigs/l2_error_over_time.pdf}
    % trim the legend and titles
    % [trim={left bottom right top},clip]
    \includegraphics[trim={0 0.5cm 0 0.3cm},clip]{\pathToFigs/loss_over_time.pdf}
  \end{subfigure}
  \begin{subfigure}[t]{1.0\linewidth}
    \caption{}\label{subfig:poisson-bayes-step}
    % trim the legend, xlabel and xticklabels
    % [trim={left bottom right top},clip]
    \includegraphics[trim={0 1.3cm 0 0.3cm},clip]{\pathToFigs/l2_error_over_step.pdf}
    % trim the titles
    \includegraphics[trim={0 0 0 0.3cm},clip]{\pathToFigs/loss_over_step.pdf}
  \end{subfigure}
  \caption{Training loss and evaluation $L_2$ error for learning the solution to high-dimensional Poisson equations over (\subref{subfig:poisson-bayes-time}) time and (\subref{subfig:poisson-bayes-step}) steps using Bayesian search.}\label{fig:poisson-bayes-appendix}
\end{figure}

\paragraph{Best run details} The runs shown in \Cref{fig:poisson-bayes-appendix} correspond to the following hyper-parameters:

\begin{itemize}

\item 5d Poisson equation, $5 \to 256 \to 256 \to 128 \to 128 \to 1$ MLP with $D=\num{116865}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp26_poisson5d_mlp_tanh_256_bayes/tex}
  \item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
  \end{itemize}

\item 10d Poisson equation, $10 \to 256 \to 256 \to 128 \to 128 \to 1$ MLP with $D=\num{118145}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp32_poisson10d_mlp_tanh_256_bayes/tex}
  \item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
  \end{itemize}

\item 100d Poisson equation, $100 \to 768 \to 768 \to 512 \to 512 \to 1$ MLP with $D=\num{1325057}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp14_poisson_100d_weinan/tex}
  \item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
  \end{itemize}
\end{itemize}

\paragraph{Search space details} The runs shown in \Cref{fig:poisson-bayes-appendix} were determined to be the best via a Bayesian search on the following search spaces which each optimizer given approximately the same total computational time ($\mathcal{C}$ denotes a categorical, $\mathcal{U}$ a uniform, and $\mathcal{LU}$ a log-uniform distribution):
\begin{itemize}

\item 5d Poisson equation, $5 \to 256 \to 256 \to 128 \to 128 \to 1$ MLP with $D=\num{116865}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp26_poisson5d_mlp_tanh_256_bayes/tex}
  \item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
  \end{itemize}

\item 10d Poisson equation, $10 \to 256 \to 256 \to 128 \to 128 \to 1$ MLP with $D=\num{118145}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp32_poisson10d_mlp_tanh_256_bayes/tex}
  \item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
  \end{itemize}

\item 100d Poisson equation, $100 \to 768 \to 768 \to 512 \to 512 \to 1$ MLP with $D=\num{1325057}$
  \begin{itemize}
    \def\pathToRuns{../kfac_pinns_exp/exp14_poisson_100d_weinan/tex}
  \item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
  \item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
  \item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
  \item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
  \item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
  \item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
  \end{itemize}
\end{itemize}


\subsection{1+1d Heat Equation}

\paragraph{Setup} TODO

\paragraph{Best run details} TODO

\paragraph{Search space details} TODO

\subsection{4+1d Heat Equation}

\paragraph{Setup} TODO

\paragraph{Best run details} TODO

\paragraph{Search space details} TODO

\subsection{4+1d Heat Equation with Bayesian Search}

\paragraph{Setup} TODO

\paragraph{Best run details}
The runs shown in \Cref{XXX} correspond to the following hyper-parameters:
\begin{itemize}
  \def\pathToRuns{../kfac_pinns_exp/exp31_heat4d_mlp_tanh_256_bayes/tex/}
\item \textbf{SGD:} \input{\pathToRuns/best_SGD.tex}
\item \textbf{Adam:} \input{\pathToRuns/best_Adam.tex}
\item \textbf{Hessian-free:} \input{\pathToRuns/best_HessianFree.tex}
\item \textbf{LBFGS:} \input{\pathToRuns/best_LBFGS.tex}
\item \textbf{KFAC:} \input{\pathToRuns/best_KFAC.tex}
\item \textbf{KFAC*:} \input{\pathToRuns/best_KFAC_auto.tex}
\end{itemize}

\paragraph{Search space details} TODO

\begin{itemize}
  \def\pathToRuns{../kfac_pinns_exp/exp31_heat4d_mlp_tanh_256_bayes/tex/}
\item \textbf{SGD:} \input{\pathToRuns/sweep_SGD.tex}
\item \textbf{Adam:} \input{\pathToRuns/sweep_Adam.tex}
\item \textbf{Hessian-free:} \input{\pathToRuns/sweep_HessianFree.tex}
\item \textbf{LBFGS:} \input{\pathToRuns/sweep_LBFGS.tex}
\item \textbf{KFAC:} \input{\pathToRuns/sweep_KFAC.tex}
\item \textbf{KFAC*:} \input{\pathToRuns/sweep_KFAC_auto.tex}
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
