\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

\usepackage[preprint]{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2023}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2023}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2023}

\input{preamble/custom_early.tex}
\input{preamble/neurips_2023.tex}
\input{preamble/goodfellow.tex} % follow DL notation from the Goodfellow book
\input{preamble/custom.tex}
\input{preamble/metadata.tex}

\begin{document}

\maketitle

\begin{abstract}
  PINNs are hard to train with first-order methods.
  To train PINNs efficiently, we need to take into account the geometry implied by the PDE operator.
  Existing methods that consider this geometry compute and invert the full Gramian.
  However, these ENGD-based methods do not scale well to architectures with many parameters due to the quadratic memory and cubic time complexity of storing and inverting the Gramian.
  The challenge to develop approximations to the Gramian is that it requires taking the parameter derivative of the PDE operator, which itself contains higher-order derivative.
  Here, we propose a Kronecker factored approximation for the Gramian, which scales more favourably than existing approaches in terms of both time and memory, while showing similar performance downstream.
\end{abstract}

\section{Introduction}

\input{sections/introduction.tex}

\section{Background}

\input{sections/background_pinns.tex}
\input{sections/background_engd.tex}
\input{sections/background_kfac.tex}

\section{KFAC for PINNs}

We restrict ourselves to feed-forward sequential NNs whose parameters are all in linear layers.
To derive a Kronecker-factored approximation of the Gramian, we first describe how a layer's parameter enters the Laplacian's computation in \Cref{sec:laplacian-computation-graph}.
This allows for expressing the exact Gramian as a sum over Kronecker-structured terms stemming from the parameter's direct children in the compute graph (\Cref{sec:kronecker-structure-gramian}).

\paragraph{Sequential NNs} Consider a sequential neural network $u_{\vtheta}$ with depth $L$ that consists of layers $f^{(i)}_{\vtheta^{(i)}}$ with trainable parameters $\vtheta^{(i)} \in \sR^{d^{(i)}}, i=1,\dots, L,$ that transform an input $\vx \in \sR^M$ into a prediction $u_{\vtheta}(\vx)\in \sR^C$ via intermediate representations $\vz^{(i)} \in \sR^{h^{(i)}}, i= 0, \dots, L$,
\begin{align}
  \begin{split}
    u_{\vtheta}
    &=
      f^{(L)}_{\vtheta^{(L)}} \circ f^{(L-1)}_{\vtheta^{(L-1)}} \circ \ldots \circ f^{(1)}_{\vtheta^{(1)}}
    \\
    f^{(i)}_{\vtheta^{(i)}}\colon \sR^{h^{(i-1)}}
    &\to
      \sR^{h^{(i)}}\,,
    \\
    \vz^{(i-1)}
    &\mapsto
      \vz^{(i)} = f^{(i)}_{\vtheta^{(i)}}(\vz^{(i-1)})
  \end{split}
\end{align}
where $\vz^{(0)} \coloneqq \vx$, $\vz^{(L)} \coloneqq \vu$, and $\vtheta = ({\vtheta^{(1)}}^{\top}, \dots, {\vtheta^{(L)}}^{\top})^{\top}$ is the concatenation of parameters over layers.
A parameter might be empty, e.g.\,if the layer is an activation layer.

\paragraph{Flattening} Above, we assumed all quantities ($\vz^{(i)}, \vtheta^{(i)}$) to be vectors.
In case of tensor-valued quantities, we can first flatten them into vectors to reduce to the vector case.
Our index convention to vectorize will be first-varies-fastest, which means column-stacking for a matrix (row index varies first, column index varies second).
We denote the flattening operation by $\flatten(\cdot)$.
Very useful is the so called \emph{vec-trick} stating that
\begin{equation}\label{eq:vecTrick}
  \flatten(AXB) = (B^\top\otimes A)\flatten{X}
\end{equation}
for matrices $A, X, B$. In particular, this shows that $B^\top\otimes A$ is the  matrix representing the linear mapping $X\mapsto AXB$ and hence $J_X(AXB) = B^\top\otimes A$.

\paragraph{Jacobian \& Hessian} The flattening notation allows to reduce derivatives of matrix/tensor-valued objects back to the matrix case.
Consider the Jacobian $\jac_{\va}\vb$ of a vector $\vb$ w.r.t.\,a vector $\va$.
It collects all partial derivatives as $[\mJ_{\va}\vb]_{i,j} = \nicefrac{\partial [\vb]_i}{\partial [\va]_j}$.
For the Jacobian $\jac_{\mA}\mB$ of a matrix $\mB$ w.r.t.\,a matrix $\mA$, we simply have $\jac_{\mA} \mB = \jac_{\flatten( \mA )}\flatten(\mB)$.
Likewise, the Hessian $\gradsquared{\va}b$ of a scalar $b$ w.r.t.\,a vector $\va$ collects the second-order partial derivatives according to $[\gradsquared{\va}b]_{i,j} = \nicefrac{\partial^2 b}{\partial [\va]_i \partial [\va]_j}$.
For the Hessian $\gradsquared{\mA} b$ of a scalar $b$ w.r.t.\,a matrix $\mA$, we simply have $\gradsquared{\mA} b = \gradsquared{\flatten(\mA)}b$.
We also have $\grad{\mA} b = \grad{\flatten(\mA)} b$ for the gradient of a scalar w.r.t.\,a matrix.

\subsection{The Laplacian's Computation Graph}\label{sec:laplacian-computation-graph}
\input{sections/laplacian_feedforward_nn.tex}

\subsection{Kronecker Structure of the Gramian}\label{sec:kronecker-structure-gramian}

This subsection describes the Jacobians of the operations with the weight matrix in
a linear layer during the computation of the Laplacian and ends by providing an
equation for the Gramian block.

\subsection{Kroneckerfactored Approximate Gramian (KFAG)}

This subsection describes the Kronecker approximation we propose for the per-layer Gramian.
It should also justify that the approximation is similar to that in other works.

\toodoo{F.D.
  Warning: The below write-up is a first attempt to come up with a Kronecker approximation.
  I still need to find more intuitive explanations, ideally with ideas from other KFAC papers.}

\paragraph{Single datum case} As a first step, let's write down only the diagonal terms of the Gramian, i.e.\,the terms caused by the same children and try to simplify each term into a Kronecker product of same dimension:
\begin{align}
  \begin{split}
    \mF^{(i)}
    &\approx
      \underbrace{
      \left(
      {\vz^{(i-1)}}^\top\otimes \mI
      \right)^{\top}
      \grad{\vz^{(i)}}\Delta u
      \left(
      \left(
      {\vz^{(i-1)}}^\top\otimes \mI
      \right)^{\top}
      \grad{\vz^{(i)}}\Delta u
      \right)^{\top}
      }_{(1, 1)}
    \\
    &\phantom{=}+
      \underbrace{
      \left(
      \mI \otimes \grad{\vz^{(i)}}u
      \right)^{\top}
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \left(
      \left(
      \mI \otimes \grad{\vz^{(i)}}u
      \right)^{\top}
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
      }_{(2, 2)}
    \\
    &\phantom{=}+
      \underbrace{
      2
      \left(
      \mI \otimes
      \left[
      \left( \gradsquared{\vz^{(i)}}u \right) \mW^{(i)}
      \right]
      \right)
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \left(
      2
      \left(
      \mI \otimes
      \left[
      \left( \gradsquared{\vz^{(i)}}u \right) \mW^{(i)}
      \right]
      \right)
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
      }_{(3, 3)}
      \,.
      \shortintertext{Without approximations, we can re-write this as}
    &=
      \vz^{(i-1)} {\vz^{(i-1)}}^\top
      \otimes
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      \left(
      \grad{\vz^{(i)}}u
      \right)
      \left(
      \grad{\vz^{(i)}}u
      \right)^{\top}
      \otimes
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      4
      \left(
      \mI \otimes
      \left[
      \left( \gradsquared{\vz^{(i)}}u \right) \mW^{(i)}
      \right]
      \right)
      \left[
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
      \right]
      \left(
      \mI \otimes
      \left[
      {\mW^{(i)}}^{\top}
      \left( \gradsquared{\vz^{(i)}}u \right)
      \right]
      \right)
      \,.
      \intertext{The leading two terms have the same Kronecker structure, $h^{(i-1)} \times h^{(i-1)} \otimes h^{(i)} \times h^{(i)}$.
      The third term does not simplify further, because the non-identity term in the Jacobians (outer terms) is a matrix, not a vector.
      One way to obtain the same Kronecker structure is to approximate the central term $ \left(\grad{\gradsquared{\vz^{(i-1)}}u}\Delta u \right) \left(\grad{\gradsquared{\vz^{(i-1)}}u}\Delta u \right)^{\top} \approx \mU \otimes \mI$ where $\mU \in \sR^{h^{(i-1)}\times h^{(i-1)}}$ (remember that $\grad{\gradsquared{\vz^{(i-1)}}}u \in \sR^{{h^{(i-1)}}^2}$.
      In the following, let $g := \grad{\gradsquared{\vz^{(i-1)}}}u$ for brevity.
      The `optimal' $\mU$ that minimizes the Frobenius norm $\left\lVert\mU \otimes \mI - \vg \vg^{\top} \right\rVert_{\text{F}}^2$ is the averaged block diagonal $\mU = \nicefrac{1}{h^{(i-1)}} \mG \mG^{\top}$ where $\mG = \flatten^{-1}(\vg) \in \sR^{h^{(i-1)} \times h^{(i-1)}}$.
      With this additional approximation, we can also express the third term as a Kronecker product:}
    &\approx
      \vz^{(i-1)} {\vz^{(i-1)}}^\top
      \otimes
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      \left(
      \grad{\vz^{(i)}}u
      \right)
      \left(
      \grad{\vz^{(i)}}u
      \right)^{\top}
      \otimes
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      \frac{4}{h^{(i-1)}}
      \left(
      \flatten^{-1}
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)
      \right)
      \left(
      \flatten^{-1}
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)
      \right)^{\top}
      \otimes
      \left( \gradsquared{\vz^{(i)}}u \right)
      \mW^{(i)}
      {\mW^{(i)}}^{\top}
      \left( \gradsquared{\vz^{(i)}}u \right)
      \,.
      \shortintertext{Let's make the final approximation from three Kronecker products into a single Kronecker product.
      In KFAC-style, we use a Kronecker-of-sums to approximate a sum-of-Kroneckers:}
    &\approx
      \mA^{(i)} \otimes \mB^{(i)}
  \end{split}
\end{align}
with
\begin{subequations}
  \label{eq:gram-kronecker-approximations-unbatched}
  \begin{align}
    \begin{split}
      \mA^{(i)}
      &=
        \vz^{(i-1)} {\vz^{(i-1)}}^\top
        +
        \left(
        \grad{\vz^{(i)}}u
        \right)
        \left(
        \grad{\vz^{(i)}}u
        \right)^{\top}
      \\
      &\phantom{=}+
        \frac{4}{h^{(i-1)}}
        \left(
        \flatten^{-1}
        \left(
        \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
        \right)
        \right)
        \left(
        \flatten^{-1}
        \left(
        \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
        \right)
        \right)^{\top}\,,
    \end{split}
    \\
    \begin{split}
      \mB^{(i)}
      &=
        \left(
        \grad{\vz^{(i)}}\Delta u
        \right)
        \left(
        \grad{\vz^{(i)}}\Delta u
        \right)^{\top}
        +
        \left(
        \grad{\grad{\vz^{(i-1)}}u}\Delta u
        \right)
        \left(
        \grad{\grad{\vz^{(i-1)}}u}\Delta u
        \right)^{\top}
      \\
      &\phantom{=}+
        \left( \gradsquared{\vz^{(i)}}u \right)
        \mW^{(i)}
        {\mW^{(i)}}^{\top}
        \left( \gradsquared{\vz^{(i)}}u \right)\,.
    \end{split}
  \end{align}
\end{subequations}

\paragraph{With batching} In the presence of multiple data points, we need to approximate the Gramian $\mF^{(i)} \approx \nicefrac{1}{N} \sum_{n=1}^N \mA_n^{(i)} \otimes \mB_n^{(i)}$ further (the subscripts $_n$ denote the computation on datum $n$).
We can just do that in the same way as the original KFAC paper, which gives us
\begin{align}\label{eq:gram-kronecker-approximations-batched}
  \mF^{(i)}
  \approx
  \left(
  \frac{1}{N}\sum_{n=1}^N \mA_n^{(i)}
  \right)
  \otimes
  \left(
  \sum_{n=1}^N \mB_n^{(i)}
  \right)
\end{align}
\Cref{eq:gram-kronecker-approximations-unbatched,eq:gram-kronecker-approximations-batched} are our proposed approximation for the Gramian.

\toodoo{--- F.D.
  Below is an alternative write-up which starts from the perspective of weight sharing.
  I haven't quite figured out the exact connection yet.}

We will Kronecker-approximate the Gramian using the recently introduced generalization of KFAC to weight-sharing layers in~\citet{eschenhagen2023kroneckerfactored}.

\paragraph{One datum, no weight sharing} Let's start with maximum likelihood estimation with a single data point $(\vx, \vy)$.
Consider a linear layer inside a neural network which maps some vector-valued hidden feature of $\vx$, $\va \in \sR^{D_{\text{in}}}$ to a vector-valued output $\vz \in \sR^{D_{\text{out}}}$ via $\vz = \mW \va$.
$\vz$ is then further processed and used to compute the negative log-likelihood loss $\ell(\vx, \vy, \mW) = - \log p(\vy \mid \vx, \mW)$.
For this single-usage layer, the weigh matrix's Fisher is exactly Kroneckerfactored, $\mF(\mW) = \va \va^{\top} \otimes \E_{\hat{\vy} \sim p(\dot \mid \vx, \mW)}\left[ \vg \vg^{\top} \right]$ where $\vg = \grad{\vz} \ell(\vx, \hat{\vy}, \mW)$.
In practise, we will use one sample from the model's likelihood to estimate the second expectation.
This yields $\mF(\mW) \approx \va \va^{\top} \otimes \vg \vg^{\top}$

\paragraph{One datum, weight sharing} Now consider a layer which is used
multiple times in the computation graph. This means the layer will not process a
single vector $\va$, but a sequence of vectors $\left\{ \va_1, \dots, \va_S
\right\}$ where $S$ denotes weight sharing number. We can column-stack these
vectors into a matrix $\mA \in \sR^{D_{\text{in}}\times S}$, likewise for the
linear layer's outputs $\mZ \in \sR^{D_{\text{out}}\times S}$ and activation
gradients $\mG \in \sR^{D_{\text{out}} \times S}$.

As described in \citet{eschenhagen2023kroneckerfactored}, there are two possible Kronecker approximations for this setup.

The first one is the \emph{expand} approximation, which takes the covariance over the shared vectors, $\mF(\mW) \approx \Cov_s[\va] \otimes S \Cov_s[\vg] = \nicefrac{1}{S} \sum_{s=1}^S \va_s \va_s^{\top} \otimes \sum_{s=1}^S \vg_s \vg_s^{\top} \eqqcolon \mF^{(\text{expand})}(\mW)$ where $\vg_s = \grad{\vz_s} \ell(\vx, \hat{\vy}, \mW)$.
We can express this in matrix notation as $\mF^{(\text{expand})}(\mW) = \nicefrac{1}{S} \mA \mA^{\top} \otimes \mG \mG^{\top}$.

A more aggressive, though cheaper, approximation is the \emph{reduce} approximation, which uses the mean outer product, $\mF(\mW) \approx \E_s[\va] \E_s[\va]^{\top} \otimes S \E_s[\vg] S \E_s[\vg]^{\top} = \left(\nicefrac{1}{S} \sum_{s=1}^S \va_s \right) \left( \nicefrac{1}{S} \sum_{s=1}^S \va_s^{\top}\right) \otimes \left(\sum_{s=1}^S \vg_s \right) \left(\sum_{s=1}^S \vg_s^{\top} \right) \eqqcolon \mF^{(\text{reduce})}(\mW)$.
We can express this in matrix form as $\mF^{(\text{reduce})}(\mW) = \left( \nicefrac{1}{S} \mA \vone \right) \left( \nicefrac{1}{S} \mA \vone \right)^{\top} \otimes \left(\mG \vone \right) \left( \mG \vone \right)^{\top}$.

\section{Generalization to other PDEs}

\input{sections/generalization.tex}

\section{Experiments}

We want to show the following things:
\begin{itemize}
\item We can safely discard the Gramian's off-diagonal blocks without harming
  training performance. This reduces the Gramian's size, but still imposes
  strong constraints on scalability.

\item Our proposed Kronecker approximation works roughly as well as the
  full/block diagonal Gramian, while being much cheaper to compute, store, and
  invert.

\item Thanks to the Kronecker approximation of the Gramian, we can scale to larger neural networks where the other methods either do not work (storing the Gramian is prohibitibely expensive) or become quite slow (matrix-free linear system solve via Gramian-vector products).
\end{itemize}

Todos:
\begin{itemize}
\item concrete example ground truth: 2d Poisson on unit square with sine target
\end{itemize}

Ideas:
\begin{itemize}
\item try out different approximations
  \begin{itemize}
  \item Ground truth
  \item Block diagonal exact
  \item Diagonal
  \item Block diagonal with different approximations
  \end{itemize}
\end{itemize}

\section{Conclusion}

\paragraph{Limitations:}
\begin{itemize}
\item Only works for sequential networks
\item Hessian backpropagation requires memory quadratic in the intermediate features size and therefore becomes impractical for large intermediates like in CNNs.
\item Our implementation is likely manual and relatively hard to automate with the current graph inspection tools offered by ML frameworks.
\end{itemize}

\begin{ack} % automatically suppressed in anonymized submission
  The authors thank Runa Eschenhagen for insightful discussions on the relation to KFAC for weight sharing layers.
\end{ack}

\bibliography{references}
\bibliographystyle{apalike}

\appendix

\input{sections/appendix.tex}
\input{sections/laplacian_shallow_net.tex}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
