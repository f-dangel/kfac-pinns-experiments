\documentclass[11pt]{article}
\usepackage{arxiv}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{pxfonts}
\usepackage{mathtools}
\usepackage{tocloft}
\usepackage{dsfont}
\usepackage{todonotes}%loads xcolor with []
\usepackage{extarrows}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{comment}

\usepackage{wrapfig}
\usepackage{caption}


\usepackage{hyperref}
\usepackage{subcaption}

% Definitions of handy macros can go here
\newcommand{\toodoo}[1]{{\tt\color{red} #1}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[]
\newtheorem{example}[definition]{Example}
\newtheorem{remark}[definition]{Remark}
\newtheorem{setting}[definition]{Setting}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{hypothesis}[definition]{Hypothesis}
\theoremstyle{plain}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}

\renewcommand{\labelenumi}{\textup{(\textit{\roman{enumi}})}}
\setlength\cftparskip{0pt}
\setlength\cftbeforesecskip{2pt}

\newcommand{\N}{\mathbb{N}}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\Rn}{\R^n}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\rami}[1]{\textcolor{blue}{#1}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\input{preamble/goodfellow.tex} % follow DL notation from the Goodfellow book
\input{preamble/custom.tex}

\title{Kfac PINNs }
\date{November 2022}

\author{Felix Dangel, Johannes M\"uller \& Marius Zeinhofer}

\begin{document}

\maketitle

\begin{abstract}
  Kfac for PINNs.
\end{abstract}
\section{Introduction}
PINNs are difficult to optimize.

\section{The Gramian/Fischer for the PINN Formulation of the Poisson Equation}
$\bullet$ I think most of the complexity can already be understood just from this example.
I illustrate, using the Poisson equation how to write the Gramian/Fisher as the product of a Jacobian and its transpose, see equation \eqref{eq:Jacobian_Fischer}.
Is this what you were after Felix?
I think it differs from the standard setting in the sense that suddenly the Laplacian shows up.
It can also be used to derive a matrix-free version of the energy natural gradient descent, like done in Hessian free optimization.
$\bullet$ We consider the Poisson equation
\begin{align*}
  -\Delta u & = f \quad \text{in }\Omega \\
  u & = g \quad \text{on }\partial\Omega
\end{align*}
where $f\in L^2(\Omega)$ and $g\in H^{3/2}(\partial\Omega)$ are some given rhs and boundary values.

$\bullet$ The function space energy is given by
\[ E(u) = \frac{1}{2} \int_\Omega (\Delta u + f) \mathrm dx + \frac12 \int_{\partial\Omega} (u-g)^2 \mathrm ds. \]

$\bullet$ The PINN loss, with discretized integrals, is
\begin{align*}
  L(\theta)
  &=
    \frac{1}{2N_\Omega} L_\Omega(\theta) + \frac{1}{2N_{\partial\Omega}}L_{\partial\Omega}(\theta)
  \\
  &=
    \frac{1}{2N_\Omega} \sum_{i=1}^{N_\Omega} (\Delta u_\theta(x_i) + f(x_i))^2 + \frac{1}{2N_{\partial\Omega}}\sum_{i=1}^{N_{\partial\Omega}} ( u_\theta(x^b_i) - g(x^b_i))^2
\end{align*}
where we denote by $(x_i)$, $i=1,\dots,N_\Omega$ the points in the interior of $\Omega$ and by $(x^b_i)$, $i=1,\dots,N_{\partial\Omega}$ the points on the boundary.
We need quite a lot of these points in practice, it can be in the thousands.
And we need them in one batch, too.

$\bullet$ The energy natural gradient is for this example to use the Fischer/Gramian of the form
\begin{equation*}
  F(\theta) = F_\Omega(\theta) + F_{\partial\Omega}(\theta)
\end{equation*}
where
\begin{equation}
  F_\Omega(\theta)_{ij} = \frac1{{N_\Omega}} \sum_{k=1}^{N_\Omega} \partial_{\theta_i} \Delta u_\theta(x_k) \partial_{\theta_j} \Delta u_\theta(x_k)
  % = \frac1{{N_\Omega}} \sum_{i=1}^{N_\Omega} (\partial_{\theta_i} f_\theta) (\partial_{\theta_j} f_\theta ),
\end{equation}
% where $f_\theta = \Delta u_\theta$.
and
\begin{equation}
  F_{\partial\Omega}(\theta)_{ij} = \frac1{{N_\Omega}} \sum_{k=1}^{N_{\partial\Omega}} \partial_{\theta_i} \Delta u_\theta(x_k^b) \partial_{\theta_j} \Delta u_\theta (x_k^b).
  % = \frac1{{N_\Omega}} \sum_{i=1}^{N_\Omega} (\partial_{\theta_i} f_\theta) (\partial_{\theta_j} f_\theta ),
\end{equation}
In more compact form and written as Jacobian products, this can be expressed as
\begin{equation}\label{eq:Jacobian_Fischer}
  F_\Omega(\theta) = Dr_\Omega(\theta)^T \cdot Dr_\Omega(\theta)
  \quad \text{and} \quad
  F_{\partial\Omega}(\theta) = Dr_{\partial\Omega}(\theta)^T \cdot Dr_{\partial\Omega}(\theta).
\end{equation}
Here, $Dr_\Omega(\theta)$ and $Dr_{\partial\Omega}(\theta)$  denote the \emph{Jacobians} of the maps
\begin{equation*}
  r_{\Omega}: \Theta \to \mathbb{R}^{N_\Omega}, \quad r_{\Omega}(\theta) = \frac{1}{\sqrt{N_{\Omega}}}(\Delta u_\theta(x_1), \dots, \Delta u_\theta(x_{N_{\Omega}}))
\end{equation*}
and
\begin{equation*}
  r_{\partial\Omega}: \Theta \to \mathbb{R}^{N_\partial\Omega}, \quad r_{\partial\Omega}(\theta) = \frac{1}{\sqrt{N_{\partial\Omega}}}(u_\theta(x^b_1), \dots, u_\theta(x^b_{N_{\partial\Omega}}))
\end{equation*}

\section{K-FAC for a shallow network}
\begin{comment}
    For the one-dimensional shallow case we have
\[ u_\theta(x) = \sum_{i=1}^m a_i\sigma(b_i x + c_i) + d. \]
Then 
\[ u_\theta'(x) = \sum_{i=1}^m a_ib_i\sigma'(b_i x + c_i) \]
and 
\[ u_\theta''(x) = \sum_{i=1}^m a_ib_i^2\sigma''(b_i x + c_i). \]
The parameter derivative of the Laplacian (i.e., second-order derivative)
\begin{align*}
     \partial_{a_i}u_\theta''(x) & = b_i^2\sigma''(b_i x + c_i) \\
     \partial_{b_i}u_\theta''(x) & = 2a_ib_i\sigma''(b_i x + c_i) + a_i b_i^2x\sigma^{(3)}(b_i x + c_i) \\
     \partial_{c_i}u_\theta''(x) & = a_i b_i^2\sigma^{(3)}(b_i x + c_i) \\
     \partial_{d}u_\theta''(x) & = 0.
\end{align*}
The parameter derivative of the Laplacian (i.e., second-order derivative)
\begin{align*}
     \partial_{a_i}u_\theta''(x) & = b_i^2\sigma''(b_i x + c_i) \\
     \partial_{b_i}u_\theta''(x) & = 2a_ib_i\sigma''(b_i x + c_i) + a_i b_i^2x\sigma^{(3)}(b_i x + c_i) \\
     \partial_{c_i}u_\theta''(x) & = a_i b_i^2\sigma^{(3)}(b_i x + c_i) \\
     \partial_{d}u_\theta''(x) & = 0.
\end{align*}
\end{comment}

First, we do the Hessian calculation as explicitly as possible. For this, we consider the easy case of a shallow network with one-dimensional output
\[ u_\theta(x) = \sum_{i=1}^m a_i\sigma(b_i^\top x + c_i) + d. \]
Then 
\[ \nabla_x u_\theta(x) = \sum_{i=1}^m a_ib_i\sigma'(b_i x + c_i) \]
or 
\[ \partial_{x_j} u_\theta(x) = \sum_{i=1}^m a_i(b_i)_j\sigma'(b_i x + c_i). \]
Consequently, we compute 
\[ \partial_{x_k}\partial_{x_j} u_\theta(x) = \sum_{i=1}^m a_i(b_i)_j(b_i)_k\sigma'(b_i x + c_i) \]
which yields the Hessian
\[ \nabla^2_x u_\theta(x) = \sum_{i=1}^m a_i\sigma''(b_i x + c_i) \cdot b_ib_i^\top = \sum_{i=1}^m a_i\sigma''(b_i x + c_i) \cdot  b_i\otimes b_i \]
and the Laplacian is given by 
\[ \Delta_x u_\theta(x) = \sum_{i=1}^m a_i\sigma''(b_i x + c_i) \operatorname{tr}(b_ib_i^\top) = \sum_{i=1}^m a_i\sigma''(b_i x + c_i)  \operatorname{tr}(b_i\otimes b_i). \]
The parameter derivatives of the Hessian are given 
\begin{align*}
     \partial_{a_i}\nabla_x^2 u_\theta(x) & = \sigma''(b_i x + c_i) b_ib_i^\top \\
     \nabla_{b_i}\nabla_x^2 u_\theta(x) & = a_i\sigma''(b_i x + c_i) (b_i\otimes I+I\otimes b_i) + a_i \sigma^{(3)}(b_i x + c_i) \cdot  (I\otimes x^\top)(b_i\otimes b_i) \quad ?? \\
     %+ a_i x\sigma^{(3)}(b_i x + c_i) \cdot b_ib_i^\top \\
     \partial_{c_i}\nabla_x^2 u_\theta(x) & = a_i b_i^2\sigma^{(3)}(b_i x + c_i) \\
     \partial_{d}\nabla_x^2 u_\theta(x) & = 0.
\end{align*}
The parameter derivatives of the Laplacian are given 
\begin{align*}
     \partial_{a_i}\Delta_x u_\theta(x) & = \sigma''(b_i x + c_i) \operatorname{tr}(b_ib_i^\top) \\
     \nabla_{b_i}\Delta_x u_\theta(x) & = 2a_i\sigma''(b_i x + c_i) + a_i \sigma^{(3)}(b_i x + c_i) \operatorname{tr}(b_ib_i^\top) \cdot x \\
     \partial_{c_i}\Delta_x u_\theta(x) & = a_i \sigma^{(3)}(b_i x + c_i)\operatorname{tr}(b_ib_i^\top) \\
     \partial_{d}\Delta_x u_\theta(x) & = 0.
\end{align*}


\paragraph{Writing as linear algebra}

Consider a shallow network
\[ u_\theta(x) = W_2\sigma(W_1 x), \]
where $x\in\mathbb R^d$, $W_1\in\mathbb R^{m\times d}, W_2\in\mathbb R^{1\times m}$.

\paragraph{Task: }
Understand the structure, in particular the computational graph of $f_\theta = \Delta u_\theta$.

\paragraph{One dimensional case: }
Assume first that $d=1$.
Then
\[ D_x u_\theta(x) = W_2 \operatorname{diag}(\sigma'(W_1 x)) W_1 \]
and
\[ D^2_x u_\theta(x) = W_2%(W_1^\top\otimes W_2)
  \operatorname{diag}(\operatorname{diag}(\sigma''(W_1 x)) W_1)W_1. % = W_2 \operatorname{diag}\sigma''(W_1 x) W_1^2 = W_2\odot W_1^2 \sigma''(W_1x),
\]
In another form
\[D^2 u_\theta(x) = W_2 \operatorname{diag}(\operatorname{diag}\sigma''(W_1 x) W_1) W_1 = W_2 \operatorname{diag}\sigma''(W_1 x) W_1^2 = W_2\odot W_1^2 \sigma''(W_1x),
\]
where $(W_2\odot W_1^2)_k = (W_2)_k \cdot (W_1)_k^2$.
% where $(W_2\odot W_1^2)_k = (W_2)_k \cdot (W_1)_k^2$.
% The Laplacian is then given as
% \[ f_\theta(x) = \Delta u_\theta(x) = \operatorname{tr}(D_x^2u_\theta(x)) =\operatorname{tr}(W_2 \operatorname{diag}(\operatorname{diag}(\sigma''(W_1 x)) W_1) W_1).  \]
% Differentiation with respect to $W_2$ yields
% \[ \frac{\partial f_\theta(x)}{\partial W_2} = \frac{\partial \Delta u_\theta(x)}{ \partial W_2} =  \]

One can also rewrite the diagonalization operator via a Hadamard product as $\operatorname{diag}(x) = (x\mathds{1}^\top)\odot I$, where $\mathds{1}$ denotes the all one vector, $I$ the identity and $\odot$ the Hadamard (i.e., entrywise) product of two matrices.
Not sure whether this helps though...

\paragraph{Multi-dimensional case}

% \paragraph{One dimensional case: }

\section{Generalization to other PDEs}
Essential to the formula above is that the discrete bilinear form (not explicitly stated above) can be written in the way
\begin{equation}\label{eq:gramian_asssumption}
  a^h(u,v)
  =
  \langle T(u), T(v) \rangle_{\mathbb R^N}.
\end{equation}
This implies
\begin{equation*}
  F(\theta) = D(T\circ P)^T \cdot D(T \circ P).
\end{equation*}
If you compute the entries of $F$ this yields
\begin{equation*}
  F(\theta)_{ij} = \langle T(\partial_{\theta_i}u_\theta), T(\partial_{\theta_j}u_\theta) \rangle_{\mathbb R^N}.
\end{equation*}
In the case of the Laplacian this $G$ will be one of the summands, so for example $F_\Omega$.
The map $T$ needs to be linear and is a combination of PDE operators and point evaluation, in the case of the Laplacian
\begin{equation*}
  T:C^2(\Omega) \to \mathbb R^{N_\Omega}, \quad T(u) = \frac{1}{\sqrt{N_{\Omega}}}(\Delta u(x_1), \dots, \Delta u(x_{N_{\Omega}})).
\end{equation*}
The map $P$ is the parametrization, i.e., $\theta\mapsto u_\theta$.
I think the form \eqref{eq:gramian_asssumption} is true for all of the PDEs we have in mind...but Johannes should double check that.

\section{Laplacian of a Feed-forward Neural Network}
\input{sections/laplacian_feedforward_nn.tex}

\bibliography{references}
\bibliographystyle{apalike}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
