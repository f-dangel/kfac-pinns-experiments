\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

\usepackage[preprint]{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2023}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2023}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2023}

\input{preamble/custom_early.tex}
\input{preamble/neurips_2023.tex}
\input{preamble/goodfellow.tex} % follow DL notation from the Goodfellow book
\input{preamble/custom.tex}
\input{preamble/metadata.tex}

\begin{document}

\maketitle

\begin{abstract}
  PINNs are hard to train with first-order methods.
  To train PINNs efficiently, we need to take into account the geometry implied by the PDE operator.
  Existing methods that consider this geometry compute and invert the full Gramian.
  However, these ENGD-based methods do not scale well to architectures with many parameters due to the quadratic memory and cubic time complexity of storing and inverting the Gramian.
  The challenge to develop approximations to the Gramian is that it requires taking the parameter derivative of the PDE operator, which itself contains higher-order derivative.
  Here, we propose a Kronecker factored approximation for the Gramian, which scales more favourably than existing approaches in terms of both time and memory, while showing similar performance downstream.
\end{abstract}

\section{Introduction}

\input{sections/introduction.tex}

\section{Background}

\input{sections/background_pinns.tex}
\input{sections/background_engd.tex}
\input{sections/background_kfac.tex}

\section{KFAC for PINNs}

\subsection{The Laplacian's Computation Graph}
We restrict the discussion to feed-forward sequential neural networks.
\input{sections/laplacian_feedforward_nn.tex}

\subsection{Kronecker Structure of the Gramian}

This subsection describes the Jacobians of the operations with the weight matrix in
a linear layer during the computation of the Laplacian and ends by providing an
equation for the Gramian block.

\subsection{Kroneckerfactored Approximate Gramian (KFAG)}

This subsection describes the Kronecker approximation we propose for the per-layer Gramian.
It should also justify that the approximation is similar to that in other works.

\section{Generalization to other PDEs}

\input{sections/generalization.tex}

\section{Experiments}

We want to show the following things:
\begin{itemize}
\item We can safely discard the Gramian's off-diagonal blocks without harming
  training performance. This reduces the Gramian's size, but still imposes
  strong constraints on scalability.

\item Our proposed Kronecker approximation works roughly as well as the
  full/block diagonal Gramian, while being much cheaper to compute, store, and
  invert.

\item Thanks to the Kronecker approximation of the Gramian, we can scale to larger neural networks where the other methods either do not work (storing the Gramian is prohibitibely expensive) or become quite slow (matrix-free linear system solve via Gramian-vector products).
\end{itemize}

Todos:
\begin{itemize}
\item concrete example ground truth: 2d Poisson on unit square with sine target
\end{itemize}

Ideas:
\begin{itemize}
\item try out different approximations
  \begin{itemize}
  \item Ground truth
  \item Block diagonal exact
  \item Diagonal
  \item Block diagonal with different approximations
  \end{itemize}
\end{itemize}

\section{Conclusion}

\begin{ack} % automatically suppressed in anonymized submission
  Acknowledgments go here.
\end{ack}

\bibliography{references}
\bibliographystyle{apalike}

\appendix

\input{sections/appendix.tex}
\input{sections/laplacian_shallow_net.tex}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
